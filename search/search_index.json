{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Machine Learning Theory Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Machine Learning Theory"},{"location":"#machine-learning-theory","text":"","title":"Machine Learning Theory"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"SVM/","text":"Support Vector Machine Perceptrons and Margin We know that in the Perceptron Algorithm the number of mistakes is given by, \\(\\text{#mistakes} \\leq \\frac{R^2}{\\gamma^2}\\) , we also said that data is linearly separable with margin \\(\\gamma\\) , which means that there exists some \\(w^*\\) such that \\(w^{*^T} x_i \\geq \\gamma \\quad \\forall i , \\gamma>0\\) Question If we were given a dataset like in the above image , which of the 2 \\(w^*\\) would be a better choice? We can see that \\(w_1^*\\) has a greater margin \\(\\gamma_1\\) when compared to \\(w_2^*\\) which has a smaller margin \\(\\gamma_2\\) . If we want to decrease the number if mistakes our algorithm makes, we can increase the value of \\(\\gamma\\) because \\(\\text{#mistakes} \\leq \\frac{R^2}{\\gamma^2}\\) , in other words number of mistakes is inversely proportional to \\(\\gamma^2\\) . The greater the \\(\\gamma\\) the lower will be the number of mistakes. Therefore , the better of the 2 \\(w^*\\) here is \\(w_1^*\\) . Maximum Margin: Formulation Now that we know that \\(w^*\\) which have larger \\(\\gamma\\) margin are better than other \\(w^*\\) , our goal now is to come up with a formulation that maximizes \\(\\gamma\\) margin. \\[\\label{max-margin-formulation} \\tag{1} \\underset{w,\\gamma}{\\max} \\quad \\gamma \\quad \\text{such that } (w^T x_i)y_i \\geq \\gamma \\;\\; \\forall i\\] We want to maximize \\(\\gamma\\) over \\(w\\) , but we arent choosing any random \\(w\\) , the \\(w\\) should satisfy the condition of \\((w^T x_i)y_i \\geq \\gamma\\) . Problem With Above Formulation Lets say for some \\(w\\) the \\(\\gamma\\) margin is , \\(\\{ x:w^T x = 5 \\}\\) , it can be argued that there exists another \\(w\\) such that \\((2 w)^T x = 2 \\times 5 = 10\\) . So for this \\(w\\) , there exist infinitely many \"scaled versions\". To solve the above stated problem , we will \"ground\" the value of \\(w\\) such that \\(||w||^2 = 1\\) . Therefore our new formulation will be, \\[\\begin{split} \\underset{w , \\gamma}{\\max} \\gamma \\\\ \\\\ \\text{such that } \\\\ (w^T x_i)y_i \\geq \\gamma \\quad \\forall i \\\\ \\text{and } ||w||^2 = 1 \\end{split}\\] Simplified Formulation The above maximization can be reformulated into an expression with just one variable ( \\(w\\) ). \\[\\label{abcd} \\tag{1} \\begin{split} \\underset{w}{\\max} \\;\\; \\text{width}(w) \\\\ \\\\ \\text{such that} \\\\ (w^T x_i)y_i \\geq 1 \\quad \\forall i \\\\ \\end{split}\\] The equation here basically means that , we are allowing any \\(w\\) which linearly separates our dataset with \\(\\gamma\\) margin and the norm square of that \\(w\\) must be 1 and instead of maximizing \\(\\gamma\\) , we are maximizing the width. The width here is the distance between to parallel lines of \\(\\gamma\\) margins. What is width(w)? For some \\(w\\) which classifies a dataset , \\(x_1\\) and \\(x_2\\) be the points lying on the \\(\\gamma\\) margins. The distance between these points (width(w)) can be given by, \\[\\begin{equation*} \\begin{split} x_1 ^T w - x_2 ^T w &= 2 \\\\ (x_1 - x_2)^T w &= 2 \\\\ ||x_1 - x_2||_2 \\; ||w||_2 \\; \\cos(\\theta) &= 2 \\quad \\quad (\\theta = 0\u00b0) \\\\ \\therefore || x_1 - x_2 ||_2 &= \\frac{2}{||w||_2} \\end{split} \\end{equation*}\\] Re-Simplified Formulation Now that we know what width(w) is , we can reformulate our maximization equation as , \\[\\begin{split} \\underset{w}{\\max} \\;\\; \\frac{2}{||w||^2} \\\\ \\\\ \\text{such that} \\\\ (w^T x_i)y_i \\geq 1 \\quad \\forall i \\\\ \\end{split}\\] Equivalently , the above experession can be turned into a minimization equation, \\[\\begin{split} \\boxed{\\underset{w}{\\min} \\;\\; \\frac{1}{2} ||w||^2} \\\\ \\\\ \\text{such that} \\\\ (w^T x_i)y_i \\geq 1 \\quad \\forall i \\\\ \\end{split}\\] Constrained Optimization Our goal now is to formulate a \"dual problem\" for the above minimization (\"primal\") problem. But for now we will look at, \\[\\begin{split} \\underset{w}{\\min} f(w) \\\\ \\\\ \\text{such that } \\\\ g(w) \\leq 0 \\\\ \\end{split}\\] To solve for the above minimization problem we will form a lagrangian function, \\[ \\mathcal{L}(w , \\alpha) = f(w) + \\alpha g(w) \\] For a fixed value of \\(w\\) , consider , \\[ \\underset{\\alpha \\geq 0}{\\max} \\mathcal{L}(w , \\alpha) = \\underset{\\alpha \\geq 0}{\\max} f(w) + \\alpha g(w) \\] Case 1 Now that we have fixed a value of \\(w\\) , lets assume that \\(g(w) > 0\\) . If \\(g(w) > 0\\) and \\(f(w) \\in \\{-\\infty , \\infty \\}\\) , to maximize the above function, we can keep increasing the value \\(\\alpha\\) . When the value of alpha is increased , the value of \\(g(w)\\) will also increase. Hence , when \\(g(w) > 0\\) the solution if \\(\\infty\\) Example1 f(w) > 0 Lets say \\(f(w) = 10\\) and \\(g(w) = 20\\) , if the value of \\(\\alpha\\) is increased, the value of overall function will also increase. We can keep on increasing the value of \\(\\alpha\\) and the value of overall function will keep on increasing. Hence , we say that the solution in this subcase is \\(\\infty\\) Example2 f(w) < 0 Lets say \\(f(w) = -100\\) and \\(g(w) = 20\\) , if the value of \\(\\alpha\\) is increased, just like above , the value of overall function will also increase. When \\(\\alpha = 5\\) ,the function will evalute to zero. When \\(\\alpha = 100\\) , the function will evalute to 1900. Hence , we can say that the solution in this subcase also is \\(\\infty\\) Case 2 In Case 2 we will assume that \\(g(w) \\leq 0\\) . If \\(g(w) < 0\\) and \\(f(w) \\in \\{-\\infty , \\infty \\}\\) , the only way to maximize the above function is to put \\(\\alpha = 0\\) . When \\(\\alpha =0\\) , \\(g(w)\\) will also become zero. Hence , when \\(g(w) < 0\\) the solution is always \\(f(w)\\) . Example1 f(w)>0 Lets say \\(f(w) = 10\\) and \\(g(w) = -20\\) , no matter what value (except 0) of \\(\\alpha\\) we use here the overall function value will always decrease. The only way to maintain the value of the overall function is to put \\(\\alpha = 0\\) . Hence, we say that the solution in this subcase is always \\(f(w)\\) . Example2 f(w)<0 Lets say \\(f(w) = -10\\) and \\(g(w) = -20\\) , no matter what value (except 0) of \\(\\alpha\\) we use here the overall value of the function (just like above) will decrease. When \\(\\alpha = 5\\) the function will evaluate to -110. When \\(\\alpha = 100\\) the function will evaluate to -2010. It can be seen that if the value of \\(\\alpha\\) is increased , the overall value of the function decreases. Hence , we can say that the solution in this subcase is always \\(f(w)\\) . Therefore the solutions for the above lagrangian maximization are, \\[ \\underset{\\alpha \\geq 0}{\\max} \\mathcal{L}(w , \\alpha) = \\begin{cases} \\infty & g(w)>0 \\\\ f(w) & g(w) \\leq 0 \\\\ \\end{cases}\\] Langrangian Function Maximization Inside the shaded region , the function evaluates to \\(f(w)\\) , while outside the shaded region the function evaluates to \\(\\infty\\) . From the above diagram we can see that for multiple values of \\(w\\) the function evalutes to \\(f(w)\\) . We want to find the minimum \\(w\\) at which the function evaluates to \\(f(w)\\) , \\[\\underset{w}{\\min} \\left[ \\underset{\\alpha \\geq 0}{\\max} f(w) + g(w) \\right] \\] Note Note that this expression is same as the original minimization problem we started with, \\[\\begin{split} \\underset{w}{\\min} f(w) \\\\ \\\\ \\text{such that } \\\\ g(w) \\leq 0 \\\\ \\end{split}\\] To gain more insight over our newly derived min-max problem we will try to turn it into a max-min problem. The max-min expression will be, \\[\\underset{\\alpha \\geq 0}{\\max} \\left[ \\underset{w}{\\min} f(w) + g(w) \\right] \\] Note that we can turn it into a max-min problem because both \\(f\\) and \\(g\\) are convex functions. Multiple Constraints If there are problems which require multiple constraints , they can be formulated as, \\[\\begin{split} \\underset{w}{\\min} f(w) \\\\ \\\\ \\text{such that } \\\\ g_i(w) \\leq 0 \\;\\; \\forall i \\\\ \\end{split}\\] Equivalently this can also be written as, \\[\\underset{w}{\\min} \\left[ \\underset{\\underset{\\geq 0}{\\alpha_1} , \\underset{\\geq 0}{\\alpha_2} , ... \\underset{\\geq 0}{\\alpha_k}}{\\max} f(w) + \\alpha_1 g_1(w) + \\alpha_2 g_2(w) \\cdots + \\alpha_k g_k(w) \\right] \\] Note that \\(i\\) here represents the number constraints and there are total \\(k\\) constrains. Formulating the Dual Problem We started off with maximizing \\(\\gamma\\) so that the number of mistakes made by our algoritm are less/reduced. We then turned it into a expression with only one variable \\(w\\) After that , we resimplified the maximization expression for \\(||w||\\) instead of width(w). Then we took a detour and solved for a constrained opitmization problem . We then modified the constrained optimization problem and ended with lagrangian maximization expression. But where does this all lead to? This leads us to getting back to the re-simplified problem and change it in such a way that it matches with the constrained optimization problem. Our Re-Simplified Expression was, \\[\\begin{split} \\underset{w}{\\min} \\;\\; \\frac{1}{2} ||w||^2 \\\\ \\\\ \\text{such that} \\\\ (w^T x_i)y_i \\geq 1 \\quad \\forall i \\\\ \\end{split}\\] To treat this expression as a constrained optimization problem that we did above , we have to convert this to standard form. Here the constraint is \\((w^T x_i)y_i \\geq 1\\) but in our constrained optimization problem the constraint was \\(g(w) \\leq 0\\) . Standardized Form The standard form will be, \\[\\label{efgh} \\tag{2} \\begin{split} \\underset{w}{\\min} \\;\\; \\frac{1}{2} ||w||^2 \\\\ \\\\ \\text{such that} \\\\ 1 - (w^T x_i)y_i \\leq 0 \\quad \\forall i \\\\ \\end{split}\\] Now the langrangian function for this standardized form will be, \\[\\mathcal{L}(w , \\alpha) = \\frac{1}{2} ||w||^2 + \\sum_{i=1}^{n} \\alpha_i (1 - (w^T x_i)y_i)\\] We know that a langrangian function can be written as an min-max expression, \\[\\underset{w}{\\min} \\underset{\\alpha \\geq 0}{\\max} \\left[ \\frac{1}{2} ||w||^2 + \\sum_{i=1}^n \\alpha_i \\left( 1 - (w^T x_i)y_i \\right) \\right]\\] Similarly , this can also be written as max-min problem, \\[\\underset{\\alpha \\geq 0}{\\max} \\underset{w}{\\min} \\left[ \\frac{1}{2} ||w||^2 + \\sum_{i=1}^n \\alpha_i \\left( 1 - (w^T x_i)y_i \\right) \\right]\\] Note that \\(\\alpha \\geq 0\\) means that \\(\\alpha\\) is a column matrix with all the \\(\\alpha_i\\) to be \\(\\geq 0\\) . We will now work with this max-min problem to further deepen our understanding about our original \\(||w||\\) maximization problem. Solution for Langrangian Max-Min Problem For some \\(\\alpha \\geq 0\\) ,the inner minimization problem becomes an unconstrained optimization problem. We will try to find its solution using gradients, \\[\\underset{w}{\\min} \\quad \\frac{1}{2} ||w||^2 + \\sum_{i=1}^{n} \\alpha_i (1 - (w^T x_i)y_i )\\] The gradient of above expression is , \\[\\begin{split} w^*_{\\alpha} + \\sum_{i=1}^n \\alpha_i (-x_i y_i) = 0 \\\\ w^*_{\\alpha} = \\sum_{i=1}^n \\alpha_i ( x_i y_i) \\\\ \\end{split}\\] From this we can conclude that for a fixed value of \\(\\alpha\\) our best \\(w\\) would be a linear combination of \\(x_i,y_i,\\alpha_i\\) If we substitute this value of \\(w^*_\\alpha\\) we can find the minimizer of the min-max expression above. On simplification after substitution, \\[ \\alpha^T I - \\frac{1}{2} (XY \\alpha)^T (XY \\alpha) \\] Note that here \\(w^*_\\alpha\\) here is in matrix notation form ( \\(w^*_\\alpha = XY \\alpha\\) ). Therefore the \"dual\" problem to the \"primal\" problem will be, \\[\\underset{w \\geq 0}{\\max} \\alpha^T I - \\frac{1}{2} \\alpha^T Y^T X^TX Y \\alpha \\] So what have we gained after finally arriving at this dual problem? Dual Variable is in \\(\\mathbb{R}^n\\) dimension , while the primal problem is in \\(\\mathbb{R}^d\\) space. If \\(d >> n\\) , its better to solve the dual problem. The objective in dual problem depends on \\(X^TX\\) , which can be kernalized. Recap Flowchart Support Vector Machine Now that we know that \\(w^*_\\alpha\\) depends on \\(\\alpha_i\\) , where importance of a datapoint is given by \\(\\alpha_i\\) , we want to find out the points where \\(\\alpha_i > 0\\) . We will take a small detour and get back to this question. \\[\\begin{equation*} \\begin{split} \\underset{\\mathbf{w^*} \\text{ is the primal solution}}{\\overset{\\textbf{Primal}} {\\underset{w}{\\min} \\left[ \\underset{\\alpha \\geq 0}{\\max} f(w) + \\alpha g(w) \\right]}} &= \\ \\underset{\\pmb{\\alpha^*} \\text{ is the dual solution}}{\\overset{\\textbf{Dual}} { \\underset{\\alpha \\geq 0}{\\max} \\left[ \\underset{w}{\\min} f(w) + \\alpha g(w) \\right]}} \\\\ \\end{split} \\end{equation*}\\] Now we will input the solutions of the dual and primal problems back into their equations, \\[ \\underset{\\alpha \\geq 0}{\\max} f(w^*) + \\alpha g(w^*) \\quad \\quad \\quad \\quad \\underset{w}{\\min} f(w) + \\alpha^* g(w) \\] The function on the left (primal problem) will evaluate to \\(f(w^*)\\) $$$$ \\[\\begin{equation*} \\begin{split} f(w^*) = \\underset{w}{\\min} f(w) + \\alpha^* g(w) &\\leq f(w^*) + \\alpha^* g(w^*) \\\\ f(w^*) &\\leq f(w^*) + \\alpha^* g(w^*) \\\\ \\alpha^* g(w^*) &\\geq 0 \\\\ \\end{split} \\end{equation*}\\] But we already know \\(\\alpha^* \\geq 0\\) and \\(g(w^*) \\leq 0\\) . The only point where both our equations are true is, Complementary Slackness \\[\\alpha^* g(w^*) = 0\\] Similarly , for multiple constraints \\[\\alpha_i^* g_i(w^*) = 0 \\quad \\forall i\\] Also , \\(g(w^*) = 1 - (w^T x_i)y_i\\) , this means the above equation can also be written as, $$ \\alpha_i ( 1 - (w^T x_i)y_i) = 0 \\quad \\forall i $$ Now, according to Complementary Slackness , if \\(\\alpha_i > 0\\) , then \\(1 - (w^T x_i)y_i = 0\\) , \\[ \\boxed{(w^T x_i)y_i = 1} \\] From the above equation we can conclude that, points which have \\(\\alpha_i > 0\\) lie on some line denoted by \\((w^T x_i)y_i = 1\\) . This means that the points which contribute to the best \\(w^*\\) only lie on the \\((w^T x_i)y_i = 1\\) line. Rest of the datapoints which dont lie on this line do not matter for formulation of \\(w^*\\) . Only the points that are on the \"Supporting\" hyperplane ( \\((w^T x_i)y_i = 1\\) ) contribute to \\(w^*\\) . These special points are called supoort vectors. Hence , this algorithm is called \"Support Vector Machine (SVM)\". \\(w^*\\) is a sparse linear combination of the datapoints. Kernalization of SVM Given a point \\(x_\\text{test}\\) the prediction for that point is , \\[\\begin{equation*} \\begin{split} (w^*)^T x_\\text{test} &= \\left( \\sum_{i=1}^n \\alpha_i \\; x_i y_i \\right)^T x_\\text{test} \\\\ &= \\sum_{i=1}^n \\alpha_i \\; y_i (x_i^T x_\\text{test}) \\\\ (w^*)^T \\phi(x_\\text{test}) &= \\sum_{i=1}^n \\alpha_i \\; y_i K(x_i , x_\\text{test}) \\end{split} \\end{equation*}\\]","title":"Support Vector Machine"},{"location":"SVM/#support-vector-machine","text":"","title":"Support Vector Machine"},{"location":"SVM/#perceptrons-and-margin","text":"We know that in the Perceptron Algorithm the number of mistakes is given by, \\(\\text{#mistakes} \\leq \\frac{R^2}{\\gamma^2}\\) , we also said that data is linearly separable with margin \\(\\gamma\\) , which means that there exists some \\(w^*\\) such that \\(w^{*^T} x_i \\geq \\gamma \\quad \\forall i , \\gamma>0\\) Question If we were given a dataset like in the above image , which of the 2 \\(w^*\\) would be a better choice? We can see that \\(w_1^*\\) has a greater margin \\(\\gamma_1\\) when compared to \\(w_2^*\\) which has a smaller margin \\(\\gamma_2\\) . If we want to decrease the number if mistakes our algorithm makes, we can increase the value of \\(\\gamma\\) because \\(\\text{#mistakes} \\leq \\frac{R^2}{\\gamma^2}\\) , in other words number of mistakes is inversely proportional to \\(\\gamma^2\\) . The greater the \\(\\gamma\\) the lower will be the number of mistakes. Therefore , the better of the 2 \\(w^*\\) here is \\(w_1^*\\) .","title":"Perceptrons and Margin"},{"location":"SVM/#maximum-margin-formulation","text":"Now that we know that \\(w^*\\) which have larger \\(\\gamma\\) margin are better than other \\(w^*\\) , our goal now is to come up with a formulation that maximizes \\(\\gamma\\) margin. \\[\\label{max-margin-formulation} \\tag{1} \\underset{w,\\gamma}{\\max} \\quad \\gamma \\quad \\text{such that } (w^T x_i)y_i \\geq \\gamma \\;\\; \\forall i\\] We want to maximize \\(\\gamma\\) over \\(w\\) , but we arent choosing any random \\(w\\) , the \\(w\\) should satisfy the condition of \\((w^T x_i)y_i \\geq \\gamma\\) . Problem With Above Formulation Lets say for some \\(w\\) the \\(\\gamma\\) margin is , \\(\\{ x:w^T x = 5 \\}\\) , it can be argued that there exists another \\(w\\) such that \\((2 w)^T x = 2 \\times 5 = 10\\) . So for this \\(w\\) , there exist infinitely many \"scaled versions\". To solve the above stated problem , we will \"ground\" the value of \\(w\\) such that \\(||w||^2 = 1\\) . Therefore our new formulation will be, \\[\\begin{split} \\underset{w , \\gamma}{\\max} \\gamma \\\\ \\\\ \\text{such that } \\\\ (w^T x_i)y_i \\geq \\gamma \\quad \\forall i \\\\ \\text{and } ||w||^2 = 1 \\end{split}\\]","title":"Maximum Margin: Formulation"},{"location":"SVM/#simplified-formulation","text":"The above maximization can be reformulated into an expression with just one variable ( \\(w\\) ). \\[\\label{abcd} \\tag{1} \\begin{split} \\underset{w}{\\max} \\;\\; \\text{width}(w) \\\\ \\\\ \\text{such that} \\\\ (w^T x_i)y_i \\geq 1 \\quad \\forall i \\\\ \\end{split}\\] The equation here basically means that , we are allowing any \\(w\\) which linearly separates our dataset with \\(\\gamma\\) margin and the norm square of that \\(w\\) must be 1 and instead of maximizing \\(\\gamma\\) , we are maximizing the width. The width here is the distance between to parallel lines of \\(\\gamma\\) margins. What is width(w)? For some \\(w\\) which classifies a dataset , \\(x_1\\) and \\(x_2\\) be the points lying on the \\(\\gamma\\) margins. The distance between these points (width(w)) can be given by, \\[\\begin{equation*} \\begin{split} x_1 ^T w - x_2 ^T w &= 2 \\\\ (x_1 - x_2)^T w &= 2 \\\\ ||x_1 - x_2||_2 \\; ||w||_2 \\; \\cos(\\theta) &= 2 \\quad \\quad (\\theta = 0\u00b0) \\\\ \\therefore || x_1 - x_2 ||_2 &= \\frac{2}{||w||_2} \\end{split} \\end{equation*}\\]","title":"Simplified Formulation"},{"location":"SVM/#re-simplified-formulation","text":"Now that we know what width(w) is , we can reformulate our maximization equation as , \\[\\begin{split} \\underset{w}{\\max} \\;\\; \\frac{2}{||w||^2} \\\\ \\\\ \\text{such that} \\\\ (w^T x_i)y_i \\geq 1 \\quad \\forall i \\\\ \\end{split}\\] Equivalently , the above experession can be turned into a minimization equation, \\[\\begin{split} \\boxed{\\underset{w}{\\min} \\;\\; \\frac{1}{2} ||w||^2} \\\\ \\\\ \\text{such that} \\\\ (w^T x_i)y_i \\geq 1 \\quad \\forall i \\\\ \\end{split}\\]","title":"Re-Simplified Formulation"},{"location":"SVM/#constrained-optimization","text":"Our goal now is to formulate a \"dual problem\" for the above minimization (\"primal\") problem. But for now we will look at, \\[\\begin{split} \\underset{w}{\\min} f(w) \\\\ \\\\ \\text{such that } \\\\ g(w) \\leq 0 \\\\ \\end{split}\\] To solve for the above minimization problem we will form a lagrangian function, \\[ \\mathcal{L}(w , \\alpha) = f(w) + \\alpha g(w) \\] For a fixed value of \\(w\\) , consider , \\[ \\underset{\\alpha \\geq 0}{\\max} \\mathcal{L}(w , \\alpha) = \\underset{\\alpha \\geq 0}{\\max} f(w) + \\alpha g(w) \\]","title":"Constrained Optimization"},{"location":"SVM/#case-1","text":"Now that we have fixed a value of \\(w\\) , lets assume that \\(g(w) > 0\\) . If \\(g(w) > 0\\) and \\(f(w) \\in \\{-\\infty , \\infty \\}\\) , to maximize the above function, we can keep increasing the value \\(\\alpha\\) . When the value of alpha is increased , the value of \\(g(w)\\) will also increase. Hence , when \\(g(w) > 0\\) the solution if \\(\\infty\\) Example1 f(w) > 0 Lets say \\(f(w) = 10\\) and \\(g(w) = 20\\) , if the value of \\(\\alpha\\) is increased, the value of overall function will also increase. We can keep on increasing the value of \\(\\alpha\\) and the value of overall function will keep on increasing. Hence , we say that the solution in this subcase is \\(\\infty\\) Example2 f(w) < 0 Lets say \\(f(w) = -100\\) and \\(g(w) = 20\\) , if the value of \\(\\alpha\\) is increased, just like above , the value of overall function will also increase. When \\(\\alpha = 5\\) ,the function will evalute to zero. When \\(\\alpha = 100\\) , the function will evalute to 1900. Hence , we can say that the solution in this subcase also is \\(\\infty\\)","title":"Case 1"},{"location":"SVM/#case-2","text":"In Case 2 we will assume that \\(g(w) \\leq 0\\) . If \\(g(w) < 0\\) and \\(f(w) \\in \\{-\\infty , \\infty \\}\\) , the only way to maximize the above function is to put \\(\\alpha = 0\\) . When \\(\\alpha =0\\) , \\(g(w)\\) will also become zero. Hence , when \\(g(w) < 0\\) the solution is always \\(f(w)\\) . Example1 f(w)>0 Lets say \\(f(w) = 10\\) and \\(g(w) = -20\\) , no matter what value (except 0) of \\(\\alpha\\) we use here the overall function value will always decrease. The only way to maintain the value of the overall function is to put \\(\\alpha = 0\\) . Hence, we say that the solution in this subcase is always \\(f(w)\\) . Example2 f(w)<0 Lets say \\(f(w) = -10\\) and \\(g(w) = -20\\) , no matter what value (except 0) of \\(\\alpha\\) we use here the overall value of the function (just like above) will decrease. When \\(\\alpha = 5\\) the function will evaluate to -110. When \\(\\alpha = 100\\) the function will evaluate to -2010. It can be seen that if the value of \\(\\alpha\\) is increased , the overall value of the function decreases. Hence , we can say that the solution in this subcase is always \\(f(w)\\) . Therefore the solutions for the above lagrangian maximization are, \\[ \\underset{\\alpha \\geq 0}{\\max} \\mathcal{L}(w , \\alpha) = \\begin{cases} \\infty & g(w)>0 \\\\ f(w) & g(w) \\leq 0 \\\\ \\end{cases}\\]","title":"Case 2"},{"location":"SVM/#langrangian-function-maximization","text":"Inside the shaded region , the function evaluates to \\(f(w)\\) , while outside the shaded region the function evaluates to \\(\\infty\\) . From the above diagram we can see that for multiple values of \\(w\\) the function evalutes to \\(f(w)\\) . We want to find the minimum \\(w\\) at which the function evaluates to \\(f(w)\\) , \\[\\underset{w}{\\min} \\left[ \\underset{\\alpha \\geq 0}{\\max} f(w) + g(w) \\right] \\] Note Note that this expression is same as the original minimization problem we started with, \\[\\begin{split} \\underset{w}{\\min} f(w) \\\\ \\\\ \\text{such that } \\\\ g(w) \\leq 0 \\\\ \\end{split}\\] To gain more insight over our newly derived min-max problem we will try to turn it into a max-min problem. The max-min expression will be, \\[\\underset{\\alpha \\geq 0}{\\max} \\left[ \\underset{w}{\\min} f(w) + g(w) \\right] \\] Note that we can turn it into a max-min problem because both \\(f\\) and \\(g\\) are convex functions.","title":"Langrangian Function Maximization"},{"location":"SVM/#multiple-constraints","text":"If there are problems which require multiple constraints , they can be formulated as, \\[\\begin{split} \\underset{w}{\\min} f(w) \\\\ \\\\ \\text{such that } \\\\ g_i(w) \\leq 0 \\;\\; \\forall i \\\\ \\end{split}\\] Equivalently this can also be written as, \\[\\underset{w}{\\min} \\left[ \\underset{\\underset{\\geq 0}{\\alpha_1} , \\underset{\\geq 0}{\\alpha_2} , ... \\underset{\\geq 0}{\\alpha_k}}{\\max} f(w) + \\alpha_1 g_1(w) + \\alpha_2 g_2(w) \\cdots + \\alpha_k g_k(w) \\right] \\] Note that \\(i\\) here represents the number constraints and there are total \\(k\\) constrains.","title":"Multiple Constraints"},{"location":"SVM/#formulating-the-dual-problem","text":"We started off with maximizing \\(\\gamma\\) so that the number of mistakes made by our algoritm are less/reduced. We then turned it into a expression with only one variable \\(w\\) After that , we resimplified the maximization expression for \\(||w||\\) instead of width(w). Then we took a detour and solved for a constrained opitmization problem . We then modified the constrained optimization problem and ended with lagrangian maximization expression. But where does this all lead to? This leads us to getting back to the re-simplified problem and change it in such a way that it matches with the constrained optimization problem. Our Re-Simplified Expression was, \\[\\begin{split} \\underset{w}{\\min} \\;\\; \\frac{1}{2} ||w||^2 \\\\ \\\\ \\text{such that} \\\\ (w^T x_i)y_i \\geq 1 \\quad \\forall i \\\\ \\end{split}\\] To treat this expression as a constrained optimization problem that we did above , we have to convert this to standard form. Here the constraint is \\((w^T x_i)y_i \\geq 1\\) but in our constrained optimization problem the constraint was \\(g(w) \\leq 0\\) .","title":"Formulating the Dual Problem"},{"location":"SVM/#standardized-form","text":"The standard form will be, \\[\\label{efgh} \\tag{2} \\begin{split} \\underset{w}{\\min} \\;\\; \\frac{1}{2} ||w||^2 \\\\ \\\\ \\text{such that} \\\\ 1 - (w^T x_i)y_i \\leq 0 \\quad \\forall i \\\\ \\end{split}\\] Now the langrangian function for this standardized form will be, \\[\\mathcal{L}(w , \\alpha) = \\frac{1}{2} ||w||^2 + \\sum_{i=1}^{n} \\alpha_i (1 - (w^T x_i)y_i)\\] We know that a langrangian function can be written as an min-max expression, \\[\\underset{w}{\\min} \\underset{\\alpha \\geq 0}{\\max} \\left[ \\frac{1}{2} ||w||^2 + \\sum_{i=1}^n \\alpha_i \\left( 1 - (w^T x_i)y_i \\right) \\right]\\] Similarly , this can also be written as max-min problem, \\[\\underset{\\alpha \\geq 0}{\\max} \\underset{w}{\\min} \\left[ \\frac{1}{2} ||w||^2 + \\sum_{i=1}^n \\alpha_i \\left( 1 - (w^T x_i)y_i \\right) \\right]\\] Note that \\(\\alpha \\geq 0\\) means that \\(\\alpha\\) is a column matrix with all the \\(\\alpha_i\\) to be \\(\\geq 0\\) . We will now work with this max-min problem to further deepen our understanding about our original \\(||w||\\) maximization problem.","title":"Standardized Form"},{"location":"SVM/#solution-for-langrangian-max-min-problem","text":"For some \\(\\alpha \\geq 0\\) ,the inner minimization problem becomes an unconstrained optimization problem. We will try to find its solution using gradients, \\[\\underset{w}{\\min} \\quad \\frac{1}{2} ||w||^2 + \\sum_{i=1}^{n} \\alpha_i (1 - (w^T x_i)y_i )\\] The gradient of above expression is , \\[\\begin{split} w^*_{\\alpha} + \\sum_{i=1}^n \\alpha_i (-x_i y_i) = 0 \\\\ w^*_{\\alpha} = \\sum_{i=1}^n \\alpha_i ( x_i y_i) \\\\ \\end{split}\\] From this we can conclude that for a fixed value of \\(\\alpha\\) our best \\(w\\) would be a linear combination of \\(x_i,y_i,\\alpha_i\\) If we substitute this value of \\(w^*_\\alpha\\) we can find the minimizer of the min-max expression above. On simplification after substitution, \\[ \\alpha^T I - \\frac{1}{2} (XY \\alpha)^T (XY \\alpha) \\] Note that here \\(w^*_\\alpha\\) here is in matrix notation form ( \\(w^*_\\alpha = XY \\alpha\\) ). Therefore the \"dual\" problem to the \"primal\" problem will be, \\[\\underset{w \\geq 0}{\\max} \\alpha^T I - \\frac{1}{2} \\alpha^T Y^T X^TX Y \\alpha \\] So what have we gained after finally arriving at this dual problem? Dual Variable is in \\(\\mathbb{R}^n\\) dimension , while the primal problem is in \\(\\mathbb{R}^d\\) space. If \\(d >> n\\) , its better to solve the dual problem. The objective in dual problem depends on \\(X^TX\\) , which can be kernalized.","title":"Solution for Langrangian Max-Min Problem"},{"location":"SVM/#recap-flowchart","text":"","title":"Recap Flowchart"},{"location":"SVM/#support-vector-machine_1","text":"Now that we know that \\(w^*_\\alpha\\) depends on \\(\\alpha_i\\) , where importance of a datapoint is given by \\(\\alpha_i\\) , we want to find out the points where \\(\\alpha_i > 0\\) . We will take a small detour and get back to this question. \\[\\begin{equation*} \\begin{split} \\underset{\\mathbf{w^*} \\text{ is the primal solution}}{\\overset{\\textbf{Primal}} {\\underset{w}{\\min} \\left[ \\underset{\\alpha \\geq 0}{\\max} f(w) + \\alpha g(w) \\right]}} &= \\ \\underset{\\pmb{\\alpha^*} \\text{ is the dual solution}}{\\overset{\\textbf{Dual}} { \\underset{\\alpha \\geq 0}{\\max} \\left[ \\underset{w}{\\min} f(w) + \\alpha g(w) \\right]}} \\\\ \\end{split} \\end{equation*}\\] Now we will input the solutions of the dual and primal problems back into their equations, \\[ \\underset{\\alpha \\geq 0}{\\max} f(w^*) + \\alpha g(w^*) \\quad \\quad \\quad \\quad \\underset{w}{\\min} f(w) + \\alpha^* g(w) \\] The function on the left (primal problem) will evaluate to \\(f(w^*)\\) $$$$ \\[\\begin{equation*} \\begin{split} f(w^*) = \\underset{w}{\\min} f(w) + \\alpha^* g(w) &\\leq f(w^*) + \\alpha^* g(w^*) \\\\ f(w^*) &\\leq f(w^*) + \\alpha^* g(w^*) \\\\ \\alpha^* g(w^*) &\\geq 0 \\\\ \\end{split} \\end{equation*}\\] But we already know \\(\\alpha^* \\geq 0\\) and \\(g(w^*) \\leq 0\\) . The only point where both our equations are true is,","title":"Support Vector Machine"},{"location":"SVM/#complementary-slackness","text":"\\[\\alpha^* g(w^*) = 0\\] Similarly , for multiple constraints \\[\\alpha_i^* g_i(w^*) = 0 \\quad \\forall i\\] Also , \\(g(w^*) = 1 - (w^T x_i)y_i\\) , this means the above equation can also be written as, $$ \\alpha_i ( 1 - (w^T x_i)y_i) = 0 \\quad \\forall i $$ Now, according to Complementary Slackness , if \\(\\alpha_i > 0\\) , then \\(1 - (w^T x_i)y_i = 0\\) , \\[ \\boxed{(w^T x_i)y_i = 1} \\] From the above equation we can conclude that, points which have \\(\\alpha_i > 0\\) lie on some line denoted by \\((w^T x_i)y_i = 1\\) . This means that the points which contribute to the best \\(w^*\\) only lie on the \\((w^T x_i)y_i = 1\\) line. Rest of the datapoints which dont lie on this line do not matter for formulation of \\(w^*\\) . Only the points that are on the \"Supporting\" hyperplane ( \\((w^T x_i)y_i = 1\\) ) contribute to \\(w^*\\) . These special points are called supoort vectors. Hence , this algorithm is called \"Support Vector Machine (SVM)\". \\(w^*\\) is a sparse linear combination of the datapoints.","title":"Complementary Slackness"},{"location":"SVM/#kernalization-of-svm","text":"Given a point \\(x_\\text{test}\\) the prediction for that point is , \\[\\begin{equation*} \\begin{split} (w^*)^T x_\\text{test} &= \\left( \\sum_{i=1}^n \\alpha_i \\; x_i y_i \\right)^T x_\\text{test} \\\\ &= \\sum_{i=1}^n \\alpha_i \\; y_i (x_i^T x_\\text{test}) \\\\ (w^*)^T \\phi(x_\\text{test}) &= \\sum_{i=1}^n \\alpha_i \\; y_i K(x_i , x_\\text{test}) \\end{split} \\end{equation*}\\]","title":"Kernalization of SVM"},{"location":"SVM2/","text":"Dual Formation for Soft Margin SVM We know that SVM works well on a linearly separable dataset, but our goal was to accommodate outliers/noise in the dataset and hence we created a new formulation from SVM called \"Soft Margin SVM\" , given by the formula, \\[\\begin{split} \\underset{w \\in \\mathbb{R}^d}{\\min} \\frac{1}{2}||w||^2 + C \\sum_{i=1}^n \\xi_i \\\\ \\text{such that } (w^T x_i)y_i + \\xi_i \\geq 1 \\;\\; \\forall i \\\\ \\xi_i \\geq 0 \\;\\; \\forall i \\\\ \\end{split}\\] We want to kernalize this equation so that it is able to perform even on \"non-linear\" datasets. Example This example depicts how we want our \"Kernalized Soft Margin SVM\" to be. It should be able to work on a non-linearly separable dataset. It should also be able to identify \"non-linear\" structures within the dataset. The first step towards our goal would be to form a \"Dual Problem\" for the \"Soft Margin SVM\" equation. To formulate the \"Dual Problem\" we will first make a langrangian function, \\[ \\mathcal{L}(w, \\xi , \\alpha , \\beta) = \\frac{1}{2}||w||^2 + C \\sum_{i=1}^{n}\\xi_i + \\sum_{i=1}^n \\alpha_i(1 - w^T x_i y_i - \\xi_i) + \\sum_{i=1}^n \\beta_i (- \\xi_i)\\] Note \\(\\alpha\\) corresponds to the first constraint ( \\((w^T x_i)y_i + \\xi_i \\geq 1\\) ). \\(\\beta\\) corresponds to the second constraint ( \\(\\xi_i \\geq 0\\) ). The constraints are written in standard form , in the above equations. Therefore , the \"Dual Problem\" will be, \\[ \\underset{w , \\xi}{\\min} \\left[ \\underset{\\substack{\\alpha \\geq 0 \\\\ \\beta \\geq 0}}{\\max} \\frac{1}{2} ||w||^2 + C \\sum_{i=1}^n \\xi_i + \\sum_{i=1}^n \\alpha_i (1 - (w^T x_i)y_i - \\xi_i)) + \\sum_{i=1}^n \\beta_i (- \\xi_i) \\right] \\] For a fixed value \\(\\alpha\\) and \\(\\beta\\) , The derivative of \"Dual Problem\" with respect to \\(w\\) will be, \\[ w^*_{\\alpha , \\beta} = \\sum_{i=1}^n \\alpha_i x_i y_i \\label{w-star} \\tag{1} \\] The derivative of \"Dual Problem\" with respect to \\(\\xi\\) will be, \\[\\begin{equation*} \\tag{2} \\begin{split} C + \\alpha_i(-1) + \\beta_i(-1) &= 0 \\\\ \\alpha_i + \\beta_i &= C \\\\ \\end{split} \\end{equation*}\\] From Equation 1 and Equation 2 , Our new dual problem will be, \\[\\boxed{\\underset{\\substack{\\alpha \\geq 0 \\\\ \\beta \\geq 0 \\\\ \\alpha + \\beta = C}}{\\max} \\alpha^T I - \\alpha^T Y^T X^TX Y \\alpha}\\] Note that this is same as the dual problem we had in \"SVM/Hard SVM\". \\(\\alpha \\geq 0\\) and \\(\\alpha + \\beta = C\\) , which means \\(\\beta\\) is restricting the range of \\(\\alpha\\) , in other words , \\(\\alpha\\) can be at most \\(C\\) . We can equivalently say that, \\[\\boxed{\\underset{0 \\leq \\alpha \\leq C}{\\max} \\alpha^T I - \\alpha^T Y^T X^TX Y \\alpha}\\] Cases of Different Value of \\(C\\) When \\(C = 0\\) If \\(C = 0\\) , this means that \\(\\alpha = 0\\) , also from Equation \\(\\eqref{w-star}\\) , \\(w^*_{\\alpha , \\beta} = 0\\) . When \\(C = \\infty\\) If \\(C = \\infty\\) , that means there is no upper bound on \\(\\alpha\\) and the above \"Dual Problem\" becomes exactly the same as Hard-Margin SVM. Complementary Slackness Conditions for Soft-Margin SVM Let ( \\(w^* , \\xi^*\\) ) be the \"Primal\" optimal solutions. Let ( \\(\\alpha^* , \\beta^*\\) ) be the \"Dual\" optimal solutions. Complementary Slackness Conditions At optimality , \\[\\forall i \\quad \\alpha_i^* (1 - (w^{*^T} x_i)y_i - \\xi_i^*) = 0 \\label{CS1} \\tag{1}\\] \\[\\forall i \\quad \\beta_i^* (- \\xi_i^*) = 0 \\label{CS2} \\tag{2}\\] Cases for Complementary Slackness When \\(\\alpha_i^* = 0\\) \\(\\implies \\beta_i^* = C\\) as \\(\\alpha_i^* + \\beta_i^* = C\\) . \\(\\implies \\xi_i^* = 0\\) as \\(\\beta_i^* = C\\) (From \\(\\eqref{CS2}\\) ) We know that , \\(w^{*^T} x_i y_i + \\xi_i^* \\geq 1\\) \\(\\implies w^{*^T} x_i y_i \\geq 1\\) as \\(\\xi_i = 0\\) \\(w^*\\) classifies \\((x_i ,y_i)\\) correctly. When \\(\\alpha*_i \\in (0,C)\\) \\(\\implies \\beta_i^* \\in (0,C)\\) \\(\\implies \\xi_i^* = 0\\) (From \\(\\eqref{CS2}\\) ) \\(\\implies 1 - (w^{*^T}x_i)y_i - \\xi_i = 0\\) (From \\(\\eqref{CS1}\\) ) \\(\\implies 1 - (w^{*^T}x_i)y_i = 0\\) as \\(\\xi_i^* = 0\\) \\(\\implies (w^{*^T}x_i)y_i = 1\\) When \\(\\alpha^*_i = C\\) \\(\\implies \\beta_i^* = 0\\) as \\(\\alpha_i^* + \\beta_i^* = C\\) \\(\\xi_i^* \\geq 0\\) as \\(\\beta_i^* = 0\\) \\(1 - (w^{*^T}x_i)y_i + \\xi_i = 0\\) (From \\(\\eqref{CS1}\\) ) \\(\\implies \\xi_i^* = 1 - (w^{*^T}x_i)y_i\\) \\(\\implies 0 \\leq 1 - (w^{*^T}x_i)y_i\\) as \\(\\xi_i^* \\geq 0\\) \\(\\implies (w^{*^T}x_i)y_i \\leq 1\\) \\(w^*\\) will either classify the points incorrectly or \\(w^*\\) will correctly classify the points but not with enough margin. Summary for Soft-Margin SVM Lets see things from the \"Primal\" point of view, \\((w^{*^T}x_i)y_i < 1\\) We know that, \\((w^{*^T}x_i)y_i + \\xi_i^* \\geq 1\\) \\(\\implies \\xi_i^* \\geq 1 - (w^{*^T}x_i)y_i\\) \\(\\beta_i^* = 0\\) as \\(\\xi_i^* > 0\\) (From \\(\\eqref{CS2}\\) ) \\(\\alpha_i^* = C\\) \\((w^{*^T}x_i)y_i = 1\\) We know that , \\((w^{*^T}x_i)y_i + \\xi_i^* \\geq 1\\) \\(\\implies \\xi_i^* \\geq 1 - (w^{*^T}x_i)y_i\\) \\(\\implies \\xi_i^* \\geq 0\\) as \\((w^{*^T}x_i)y_i = 1\\) \\(\\alpha_i^* \\in [0,C]\\) \\((w^{*^T}x_i)y_i > 1\\) \\(\\implies \\underbrace{1 - (w^{*^T}x_i)y_i}_{<0} - \\underbrace{\\xi_i^*}_{\\leq 0} < 0\\) \\(\\implies \\alpha_i^* = 0\\) (From \\(\\eqref{CS1}\\) ) Points which are strictly greater than 1 , do not constribute to \\(w^*\\) . Summary Black Points have either \\(\\gamma\\) (margin) greater than 1 or less than 1. These points do not constribute to \\(w^*\\) Blue Points are classified correctly but not with enough margin. From Orange Points there is nothing much to conclude except the fact that they will lie on the hyperplane. The only points which contribute to our \\(w^*\\) are either on the supporting hyperplane (Orange Points) or they are on the wrong side of supporting hyperplane (Blue Points). Our assumption is that these (Blue and Orange) points will be much lesser than the other (Black) points. Hence, we will get a sparse solution. Overfitting and Underfitting We will take a look at meta/ensemble classifiers. These type of classifying techniques help in transforming \"Weak Learners\" into \"Strong Learners\". By \"Weak Learners\" we mean , algorithms which are better than random performance but dont have high accuracy. To broaden our understanding about \"Weak Learners\" we will first take a look at Overfitting and Underfitting, Note The relationship between input and output is assumed to be some structure + noise. \\[ \\text{Input} = \\underbrace{\\text{Structure + Noise}}_{\\text{Output}} \\] Error is given by, \\[\\text{Error} = \\text{Bias} + \\text{Variance}\\] Example Overfitting Overfitting happens when our algorithm \"fits noise\" as a part of the structure. Overfit models change a lot when theres a change in variance of the dataset.These models suffer from high Variance. Example In the case of a Decision Tree , we can keep increasing the depth of the Decision Tree to an arbitrary amount to get the least possible error on the training dataset. The problem with this training method is that it may produce great results on the training dataset but its accuracy will reduce significantly when on testing dataset. In the Overfit Dataset , the training error is zero. Even though the actual structure is sinusoidal the overfit dataset \"fits noise\" thinking that the noise is part of the structure. Underfitting Underfitting is the opposite of Overfitting. It happens when our algorithm assumes some part of the structure to be noise. Underfit models dont change much even if the variance of the dataset increases a lot. These type of models suffer from high Bias. Example In the Underfit Dataset , even though the Actual Dataset is sinusoidal , the structure is assumed to be a linear structure. Our algorithm thinks that some of the input is noise not structure. Bagging (Boostrap Aggregation) Lets say some points \\(\\{x_1 , x_2 , \\cdots x_n \\} \\in \\mathcal{N}(\\mu , 1)\\) with some mean \\(\\mu\\) and variance 1. The estimator of unknown mean \\(\\mu\\) can be, \\[\\begin{equation*}\\begin{split} \\hat{\\mu_1} &= x_1 \\\\ \\hat{\\mu_2} &= x_2 \\\\ & \\vdots \\\\ \\hat{\\mu_n} &= x_n \\\\ \\end{split}\\end{equation*}\\] Though the best way to estimate \\(\\mu\\) would be to use maximum likelihood estimator , \\(\\hat{\\mu_\\text{ML}} = \\frac{1}{n} \\sum_{i=1}^n x_i\\) But why is the Maximum Likelihood the best estimator? When take an average over a huge dataset for mean \\(\\mu\\) the , averaging, reduces the variance/fluctuation of the mean \\(\\mu\\) . We can use this technique of averaging to counter the \"High Variance Issue\" in Overfitted Datasets. Bagging Lets say there are \\(D_1 , D_2 , \\cdots , D_m\\) datasets , each with \\(n\\) datapoints. We will now make \"Overfit\" Decision Trees for each of these datasets. Yes , we are specifically making overfit decision trees. Where each Decision Tree is represented as \\(DT_1 , DT_2 , \\cdots , DT_m\\) . Each Decision Tree outputs a Classifier \\(h\\) , which is represented as, \\(h_1 , h_2 , \\cdots , h_m\\) . Moreover , each \\(h_i:\\mathbb{R} \\to [0,1]\\) Our assumption is that each classifier ( \\(h_i\\) ) is trained independently. As we are making the Decision Trees \"Overfit\" on the training dataset, these models will suffer from high variance. To reduce their variance , we can average the \\(h_i\\) classifiers such that, \\[\\begin{split} h^*(x) = \\text{sign} \\left(\\frac{1}{m} \\sum_{i=1}^n h_i(x) \\right) \\\\ \\text{where,} \\\\ h(z) = \\begin{cases} +1 & \\text{if } z \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases} \\end{split}\\] This aggregate classifier ( \\(h^*(x)\\) ) will have lower variance than the original classifiers. The problem with this approach is that , we only get a single dataset not \\(m\\) different datasets. In general , we only get a single dataset , where \\(D = \\{(x_1,y_1) , (x_2,y_2) , \\cdots (x_n,y_n)\\}\\) and \\(x_i \\in \\mathbb{R}^d\\) and \\(y \\in [0,1]\\) . To apply this method of averaging (shown above) , our input needs to be \\(m\\) different datasets. A simple approach to divide the single dataset into \\(m\\) different datasets would be to just make \\(m\\) datasets each having \\(n/m\\) , datapoints , where \\(n\\) is equal to total number of datapoints in the single dataset. The issue with this type of approach is that , each of these \\(m\\) datasets will not have full \"information\" about the overall dataset. To divide the dataset into \\(m\\) different datasets , we will make it so that each dataset ( \\(D_i\\) ) has repeated points in them. This procedure of creating different datasets with repeated points in called Bootstrapping. We will create \\(m\\) different \"Bags\" and draw datapoints at random with replacement from the original (single) dataset. Each \"Bag\" will have the same number of datapoints as in the original (single) dataset. This makes it so that some \"Bags\" have may have datapoints common and even have repeated datapoints within them. At the end when we have \\(m\\) \"Bags\" , we can run an algorithm (Example : Decision Tree) over the \"Bags\" and reduce their variance by the \"Bagging (Bootstrap Aggregation) Method\". Probability of Repetition of Points Now that we know that , datapoints are repeated inside the Bags , we want to get a general Probability for the amount of repetition that happens. A point appears with the chance of \\(1 / n\\) in a bag. Probability that it does not appear in the bag is \\((1 - (\\frac{1}{n}))^n\\) Probability that it actually appears in the bag \\(1 - (1 - (\\frac{1}{n}))^n\\) For large enough \\(n\\) , this probability approximates to around 67%. Overfit datasets have low Bias and high Variance , but after applying the \"Bagging Method\" the Variance also reduces (in most cases). Boosting (Adaptive Boosting) The Adaptive Boosting Algorithm can be used to convert \"Weak Learners\" into \"Strong Learners\" , underfit models are one such example, as they suffer from High Bias and Low Variance. The input for Adaptive Boosting is our usual \\(D = \\{(x_1 , y_1) , (x_2 , y_2) \\cdots , (x_2 , y_2) \\}\\) where \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in [0,1]\\) . The first step is to initialize the weights for Boosting. \\(D_0(i) = \\frac{1}{n}\\) , \\(D_0\\) represents the weights at initilization (iteration 0). We will now create a \"Bag\" and sample points with replacement from \\(D_t\\) . Then we will input our dataset and distribution/weights ( \\(S,D_t\\) ) into a weak learner to get \\(h_t\\) . If the weak learner/algorithm is not able to handle both the dataset and its distribution/weights then we create a \"Bag\" and use the distribution/weights as probabilities for sampling points with replacement. \\(h_t\\) is the classifier , \\(h_t : \\mathbb{R}^d \\to [0,1]\\) After performing step 1, we get the weights for a dataset \\(S\\) . Lets say at the end of step 2 , we get a decision tree ( \\(h_t\\) ) which classifies 600 points correctly and 400 points incorrectly. We can change the weights such that , those 400 incorrectly classified datapoints they are classified correctly in the next round , but how? , what changes should we make? The third step would be to increase or decrease the weights of the points for the next iteration. \\[ \\hat{D_{t+1}}(i) = \\begin{cases} D_t(i) \\cdot e^{\\alpha_t} & \\text{if } h_t(x_i) \\neq y_i \\\\ D_t(i) \\cdot e^{-\\alpha_t} & \\text{if } h_t(x_i) = y_i \\\\ \\end{cases} \\] We are going to decrease the weights of the points which are classified correctly and Increase the weights of the points classified incorrectly. Failure The problem with current formulation of distribution/weights is that, when we increase or decrease the weights , they no longer sum upto 1. At the end we will normalize the weights, \\[ D_{t+1}(i) = \\frac{\\hat{D_{t+1}(i)}}{\\sum_{i=1}^n \\hat{D_{t+1}}(j)} \\] Repeat step3 unless training error is zero. At the end of all these steps we are left with \\(t\\) classifiers \\((h_1 , h_2 , \\cdots h_t)\\) for \\(t\\) iterations. To combine these classifiers somehow into one single classifier, \\[ h^*(x) = \\text{sign}\\left(\\sum_{i=1}^n \\alpha_t h_t(x) \\right) \\] Note \\[ \\alpha_t = \\ln \\sqrt{\\frac{1 - \\text{err}(h_t)}{\\text{err}(h_t)}} \\] Also, one can prove that \\[ \\text{If } T \\geq \\frac{1}{2 \\gamma^2} \\ln(2n) \\] then , Training error becomes 0. Here \\(T\\) is total number of iterations and \\(\\gamma\\) is the value which determines \"By how much is the weak learner better than random classifier\". If the random classifier has the accuracy of 55% and Weak Learner has accuracy of 60% then \\(\\gamma = 60\\% - 55\\% = 5\\% = 0.05\\) 111","title":"SVM2"},{"location":"SVM2/#dual-formation-for-soft-margin-svm","text":"We know that SVM works well on a linearly separable dataset, but our goal was to accommodate outliers/noise in the dataset and hence we created a new formulation from SVM called \"Soft Margin SVM\" , given by the formula, \\[\\begin{split} \\underset{w \\in \\mathbb{R}^d}{\\min} \\frac{1}{2}||w||^2 + C \\sum_{i=1}^n \\xi_i \\\\ \\text{such that } (w^T x_i)y_i + \\xi_i \\geq 1 \\;\\; \\forall i \\\\ \\xi_i \\geq 0 \\;\\; \\forall i \\\\ \\end{split}\\] We want to kernalize this equation so that it is able to perform even on \"non-linear\" datasets. Example This example depicts how we want our \"Kernalized Soft Margin SVM\" to be. It should be able to work on a non-linearly separable dataset. It should also be able to identify \"non-linear\" structures within the dataset. The first step towards our goal would be to form a \"Dual Problem\" for the \"Soft Margin SVM\" equation. To formulate the \"Dual Problem\" we will first make a langrangian function, \\[ \\mathcal{L}(w, \\xi , \\alpha , \\beta) = \\frac{1}{2}||w||^2 + C \\sum_{i=1}^{n}\\xi_i + \\sum_{i=1}^n \\alpha_i(1 - w^T x_i y_i - \\xi_i) + \\sum_{i=1}^n \\beta_i (- \\xi_i)\\] Note \\(\\alpha\\) corresponds to the first constraint ( \\((w^T x_i)y_i + \\xi_i \\geq 1\\) ). \\(\\beta\\) corresponds to the second constraint ( \\(\\xi_i \\geq 0\\) ). The constraints are written in standard form , in the above equations. Therefore , the \"Dual Problem\" will be, \\[ \\underset{w , \\xi}{\\min} \\left[ \\underset{\\substack{\\alpha \\geq 0 \\\\ \\beta \\geq 0}}{\\max} \\frac{1}{2} ||w||^2 + C \\sum_{i=1}^n \\xi_i + \\sum_{i=1}^n \\alpha_i (1 - (w^T x_i)y_i - \\xi_i)) + \\sum_{i=1}^n \\beta_i (- \\xi_i) \\right] \\] For a fixed value \\(\\alpha\\) and \\(\\beta\\) , The derivative of \"Dual Problem\" with respect to \\(w\\) will be, \\[ w^*_{\\alpha , \\beta} = \\sum_{i=1}^n \\alpha_i x_i y_i \\label{w-star} \\tag{1} \\] The derivative of \"Dual Problem\" with respect to \\(\\xi\\) will be, \\[\\begin{equation*} \\tag{2} \\begin{split} C + \\alpha_i(-1) + \\beta_i(-1) &= 0 \\\\ \\alpha_i + \\beta_i &= C \\\\ \\end{split} \\end{equation*}\\] From Equation 1 and Equation 2 , Our new dual problem will be, \\[\\boxed{\\underset{\\substack{\\alpha \\geq 0 \\\\ \\beta \\geq 0 \\\\ \\alpha + \\beta = C}}{\\max} \\alpha^T I - \\alpha^T Y^T X^TX Y \\alpha}\\] Note that this is same as the dual problem we had in \"SVM/Hard SVM\". \\(\\alpha \\geq 0\\) and \\(\\alpha + \\beta = C\\) , which means \\(\\beta\\) is restricting the range of \\(\\alpha\\) , in other words , \\(\\alpha\\) can be at most \\(C\\) . We can equivalently say that, \\[\\boxed{\\underset{0 \\leq \\alpha \\leq C}{\\max} \\alpha^T I - \\alpha^T Y^T X^TX Y \\alpha}\\]","title":"Dual Formation for Soft Margin SVM"},{"location":"SVM2/#cases-of-different-value-of-c","text":"When \\(C = 0\\) If \\(C = 0\\) , this means that \\(\\alpha = 0\\) , also from Equation \\(\\eqref{w-star}\\) , \\(w^*_{\\alpha , \\beta} = 0\\) . When \\(C = \\infty\\) If \\(C = \\infty\\) , that means there is no upper bound on \\(\\alpha\\) and the above \"Dual Problem\" becomes exactly the same as Hard-Margin SVM.","title":"Cases of Different Value of \\(C\\)"},{"location":"SVM2/#complementary-slackness-conditions-for-soft-margin-svm","text":"Let ( \\(w^* , \\xi^*\\) ) be the \"Primal\" optimal solutions. Let ( \\(\\alpha^* , \\beta^*\\) ) be the \"Dual\" optimal solutions.","title":"Complementary Slackness Conditions for Soft-Margin SVM"},{"location":"SVM2/#complementary-slackness-conditions","text":"At optimality , \\[\\forall i \\quad \\alpha_i^* (1 - (w^{*^T} x_i)y_i - \\xi_i^*) = 0 \\label{CS1} \\tag{1}\\] \\[\\forall i \\quad \\beta_i^* (- \\xi_i^*) = 0 \\label{CS2} \\tag{2}\\]","title":"Complementary Slackness Conditions"},{"location":"SVM2/#cases-for-complementary-slackness","text":"When \\(\\alpha_i^* = 0\\) \\(\\implies \\beta_i^* = C\\) as \\(\\alpha_i^* + \\beta_i^* = C\\) . \\(\\implies \\xi_i^* = 0\\) as \\(\\beta_i^* = C\\) (From \\(\\eqref{CS2}\\) ) We know that , \\(w^{*^T} x_i y_i + \\xi_i^* \\geq 1\\) \\(\\implies w^{*^T} x_i y_i \\geq 1\\) as \\(\\xi_i = 0\\) \\(w^*\\) classifies \\((x_i ,y_i)\\) correctly. When \\(\\alpha*_i \\in (0,C)\\) \\(\\implies \\beta_i^* \\in (0,C)\\) \\(\\implies \\xi_i^* = 0\\) (From \\(\\eqref{CS2}\\) ) \\(\\implies 1 - (w^{*^T}x_i)y_i - \\xi_i = 0\\) (From \\(\\eqref{CS1}\\) ) \\(\\implies 1 - (w^{*^T}x_i)y_i = 0\\) as \\(\\xi_i^* = 0\\) \\(\\implies (w^{*^T}x_i)y_i = 1\\) When \\(\\alpha^*_i = C\\) \\(\\implies \\beta_i^* = 0\\) as \\(\\alpha_i^* + \\beta_i^* = C\\) \\(\\xi_i^* \\geq 0\\) as \\(\\beta_i^* = 0\\) \\(1 - (w^{*^T}x_i)y_i + \\xi_i = 0\\) (From \\(\\eqref{CS1}\\) ) \\(\\implies \\xi_i^* = 1 - (w^{*^T}x_i)y_i\\) \\(\\implies 0 \\leq 1 - (w^{*^T}x_i)y_i\\) as \\(\\xi_i^* \\geq 0\\) \\(\\implies (w^{*^T}x_i)y_i \\leq 1\\) \\(w^*\\) will either classify the points incorrectly or \\(w^*\\) will correctly classify the points but not with enough margin.","title":"Cases for Complementary Slackness"},{"location":"SVM2/#summary-for-soft-margin-svm","text":"Lets see things from the \"Primal\" point of view, \\((w^{*^T}x_i)y_i < 1\\) We know that, \\((w^{*^T}x_i)y_i + \\xi_i^* \\geq 1\\) \\(\\implies \\xi_i^* \\geq 1 - (w^{*^T}x_i)y_i\\) \\(\\beta_i^* = 0\\) as \\(\\xi_i^* > 0\\) (From \\(\\eqref{CS2}\\) ) \\(\\alpha_i^* = C\\) \\((w^{*^T}x_i)y_i = 1\\) We know that , \\((w^{*^T}x_i)y_i + \\xi_i^* \\geq 1\\) \\(\\implies \\xi_i^* \\geq 1 - (w^{*^T}x_i)y_i\\) \\(\\implies \\xi_i^* \\geq 0\\) as \\((w^{*^T}x_i)y_i = 1\\) \\(\\alpha_i^* \\in [0,C]\\) \\((w^{*^T}x_i)y_i > 1\\) \\(\\implies \\underbrace{1 - (w^{*^T}x_i)y_i}_{<0} - \\underbrace{\\xi_i^*}_{\\leq 0} < 0\\) \\(\\implies \\alpha_i^* = 0\\) (From \\(\\eqref{CS1}\\) ) Points which are strictly greater than 1 , do not constribute to \\(w^*\\) .","title":"Summary for Soft-Margin SVM"},{"location":"SVM2/#summary","text":"Black Points have either \\(\\gamma\\) (margin) greater than 1 or less than 1. These points do not constribute to \\(w^*\\) Blue Points are classified correctly but not with enough margin. From Orange Points there is nothing much to conclude except the fact that they will lie on the hyperplane. The only points which contribute to our \\(w^*\\) are either on the supporting hyperplane (Orange Points) or they are on the wrong side of supporting hyperplane (Blue Points). Our assumption is that these (Blue and Orange) points will be much lesser than the other (Black) points. Hence, we will get a sparse solution.","title":"Summary"},{"location":"SVM2/#overfitting-and-underfitting","text":"We will take a look at meta/ensemble classifiers. These type of classifying techniques help in transforming \"Weak Learners\" into \"Strong Learners\". By \"Weak Learners\" we mean , algorithms which are better than random performance but dont have high accuracy. To broaden our understanding about \"Weak Learners\" we will first take a look at Overfitting and Underfitting, Note The relationship between input and output is assumed to be some structure + noise. \\[ \\text{Input} = \\underbrace{\\text{Structure + Noise}}_{\\text{Output}} \\] Error is given by, \\[\\text{Error} = \\text{Bias} + \\text{Variance}\\] Example","title":"Overfitting and Underfitting"},{"location":"SVM2/#overfitting","text":"Overfitting happens when our algorithm \"fits noise\" as a part of the structure. Overfit models change a lot when theres a change in variance of the dataset.These models suffer from high Variance. Example In the case of a Decision Tree , we can keep increasing the depth of the Decision Tree to an arbitrary amount to get the least possible error on the training dataset. The problem with this training method is that it may produce great results on the training dataset but its accuracy will reduce significantly when on testing dataset. In the Overfit Dataset , the training error is zero. Even though the actual structure is sinusoidal the overfit dataset \"fits noise\" thinking that the noise is part of the structure.","title":"Overfitting"},{"location":"SVM2/#underfitting","text":"Underfitting is the opposite of Overfitting. It happens when our algorithm assumes some part of the structure to be noise. Underfit models dont change much even if the variance of the dataset increases a lot. These type of models suffer from high Bias. Example In the Underfit Dataset , even though the Actual Dataset is sinusoidal , the structure is assumed to be a linear structure. Our algorithm thinks that some of the input is noise not structure.","title":"Underfitting"},{"location":"SVM2/#bagging-boostrap-aggregation","text":"Lets say some points \\(\\{x_1 , x_2 , \\cdots x_n \\} \\in \\mathcal{N}(\\mu , 1)\\) with some mean \\(\\mu\\) and variance 1. The estimator of unknown mean \\(\\mu\\) can be, \\[\\begin{equation*}\\begin{split} \\hat{\\mu_1} &= x_1 \\\\ \\hat{\\mu_2} &= x_2 \\\\ & \\vdots \\\\ \\hat{\\mu_n} &= x_n \\\\ \\end{split}\\end{equation*}\\] Though the best way to estimate \\(\\mu\\) would be to use maximum likelihood estimator , \\(\\hat{\\mu_\\text{ML}} = \\frac{1}{n} \\sum_{i=1}^n x_i\\) But why is the Maximum Likelihood the best estimator? When take an average over a huge dataset for mean \\(\\mu\\) the , averaging, reduces the variance/fluctuation of the mean \\(\\mu\\) . We can use this technique of averaging to counter the \"High Variance Issue\" in Overfitted Datasets.","title":"Bagging (Boostrap Aggregation)"},{"location":"SVM2/#bagging","text":"Lets say there are \\(D_1 , D_2 , \\cdots , D_m\\) datasets , each with \\(n\\) datapoints. We will now make \"Overfit\" Decision Trees for each of these datasets. Yes , we are specifically making overfit decision trees. Where each Decision Tree is represented as \\(DT_1 , DT_2 , \\cdots , DT_m\\) . Each Decision Tree outputs a Classifier \\(h\\) , which is represented as, \\(h_1 , h_2 , \\cdots , h_m\\) . Moreover , each \\(h_i:\\mathbb{R} \\to [0,1]\\) Our assumption is that each classifier ( \\(h_i\\) ) is trained independently. As we are making the Decision Trees \"Overfit\" on the training dataset, these models will suffer from high variance. To reduce their variance , we can average the \\(h_i\\) classifiers such that, \\[\\begin{split} h^*(x) = \\text{sign} \\left(\\frac{1}{m} \\sum_{i=1}^n h_i(x) \\right) \\\\ \\text{where,} \\\\ h(z) = \\begin{cases} +1 & \\text{if } z \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases} \\end{split}\\] This aggregate classifier ( \\(h^*(x)\\) ) will have lower variance than the original classifiers. The problem with this approach is that , we only get a single dataset not \\(m\\) different datasets. In general , we only get a single dataset , where \\(D = \\{(x_1,y_1) , (x_2,y_2) , \\cdots (x_n,y_n)\\}\\) and \\(x_i \\in \\mathbb{R}^d\\) and \\(y \\in [0,1]\\) . To apply this method of averaging (shown above) , our input needs to be \\(m\\) different datasets. A simple approach to divide the single dataset into \\(m\\) different datasets would be to just make \\(m\\) datasets each having \\(n/m\\) , datapoints , where \\(n\\) is equal to total number of datapoints in the single dataset. The issue with this type of approach is that , each of these \\(m\\) datasets will not have full \"information\" about the overall dataset. To divide the dataset into \\(m\\) different datasets , we will make it so that each dataset ( \\(D_i\\) ) has repeated points in them. This procedure of creating different datasets with repeated points in called Bootstrapping. We will create \\(m\\) different \"Bags\" and draw datapoints at random with replacement from the original (single) dataset. Each \"Bag\" will have the same number of datapoints as in the original (single) dataset. This makes it so that some \"Bags\" have may have datapoints common and even have repeated datapoints within them. At the end when we have \\(m\\) \"Bags\" , we can run an algorithm (Example : Decision Tree) over the \"Bags\" and reduce their variance by the \"Bagging (Bootstrap Aggregation) Method\".","title":"Bagging"},{"location":"SVM2/#probability-of-repetition-of-points","text":"Now that we know that , datapoints are repeated inside the Bags , we want to get a general Probability for the amount of repetition that happens. A point appears with the chance of \\(1 / n\\) in a bag. Probability that it does not appear in the bag is \\((1 - (\\frac{1}{n}))^n\\) Probability that it actually appears in the bag \\(1 - (1 - (\\frac{1}{n}))^n\\) For large enough \\(n\\) , this probability approximates to around 67%. Overfit datasets have low Bias and high Variance , but after applying the \"Bagging Method\" the Variance also reduces (in most cases).","title":"Probability of Repetition of Points"},{"location":"SVM2/#boosting-adaptive-boosting","text":"The Adaptive Boosting Algorithm can be used to convert \"Weak Learners\" into \"Strong Learners\" , underfit models are one such example, as they suffer from High Bias and Low Variance. The input for Adaptive Boosting is our usual \\(D = \\{(x_1 , y_1) , (x_2 , y_2) \\cdots , (x_2 , y_2) \\}\\) where \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in [0,1]\\) . The first step is to initialize the weights for Boosting. \\(D_0(i) = \\frac{1}{n}\\) , \\(D_0\\) represents the weights at initilization (iteration 0). We will now create a \"Bag\" and sample points with replacement from \\(D_t\\) . Then we will input our dataset and distribution/weights ( \\(S,D_t\\) ) into a weak learner to get \\(h_t\\) . If the weak learner/algorithm is not able to handle both the dataset and its distribution/weights then we create a \"Bag\" and use the distribution/weights as probabilities for sampling points with replacement. \\(h_t\\) is the classifier , \\(h_t : \\mathbb{R}^d \\to [0,1]\\) After performing step 1, we get the weights for a dataset \\(S\\) . Lets say at the end of step 2 , we get a decision tree ( \\(h_t\\) ) which classifies 600 points correctly and 400 points incorrectly. We can change the weights such that , those 400 incorrectly classified datapoints they are classified correctly in the next round , but how? , what changes should we make? The third step would be to increase or decrease the weights of the points for the next iteration. \\[ \\hat{D_{t+1}}(i) = \\begin{cases} D_t(i) \\cdot e^{\\alpha_t} & \\text{if } h_t(x_i) \\neq y_i \\\\ D_t(i) \\cdot e^{-\\alpha_t} & \\text{if } h_t(x_i) = y_i \\\\ \\end{cases} \\] We are going to decrease the weights of the points which are classified correctly and Increase the weights of the points classified incorrectly. Failure The problem with current formulation of distribution/weights is that, when we increase or decrease the weights , they no longer sum upto 1. At the end we will normalize the weights, \\[ D_{t+1}(i) = \\frac{\\hat{D_{t+1}(i)}}{\\sum_{i=1}^n \\hat{D_{t+1}}(j)} \\] Repeat step3 unless training error is zero. At the end of all these steps we are left with \\(t\\) classifiers \\((h_1 , h_2 , \\cdots h_t)\\) for \\(t\\) iterations. To combine these classifiers somehow into one single classifier, \\[ h^*(x) = \\text{sign}\\left(\\sum_{i=1}^n \\alpha_t h_t(x) \\right) \\] Note \\[ \\alpha_t = \\ln \\sqrt{\\frac{1 - \\text{err}(h_t)}{\\text{err}(h_t)}} \\] Also, one can prove that \\[ \\text{If } T \\geq \\frac{1}{2 \\gamma^2} \\ln(2n) \\] then , Training error becomes 0. Here \\(T\\) is total number of iterations and \\(\\gamma\\) is the value which determines \"By how much is the weak learner better than random classifier\". If the random classifier has the accuracy of 55% and Weak Learner has accuracy of 60% then \\(\\gamma = 60\\% - 55\\% = 5\\% = 0.05\\) 111","title":"Boosting (Adaptive Boosting)"},{"location":"Types%20of%20Models/","text":"Types of Models Generative Model-Based Algorithm For a dataset \\(D= \\{ (x_1,y_1), (x_2,y_2), ... (x_n,y_n) \\}\\) , let \\(x\\) and \\(y\\) both be binary features such that \\(x_i \\in \\{ 0,1 \\}^d\\) and \\(y_i \\in \\{ 0,1 \\}\\) Can features be binary? Does it even have an real life uses? The features can be binary and usually represent Yes/No type question. One of the examples is of spam classification of mail which we will be doing now. Spam Classifcication For the spam classification of emails we will have to make \\(d\\) length featuers for each email , even when the lengths and words of emails are different. One of the ways we could do this is to make a very very long vector/list which stores the poisition of each word in a dictionary in ascending order. Example Lets say (for simplicity) our dictionary has 4 words, [\"Welcome\", \"to\" , \"earth\" , \"aliens\"]. Although, the words arent ordered here, our feature vectors/lists will be ordered. Now lets say that our mail has the following 2 words, \"Welcome Aliens\". We will encode it in such a way that each feature represents the poisition and occurence of the word in dictionary. Feature vector of \\(d\\) length for \"Welcome Aliens\" will be, [1 , 0 , 0 , 1]. To do Generative Modeling for our \\(d\\) length feature vectors, our next step would be assign probabilities to each word for being in the spam and non-spam category. In other words each feature ( \\(x\\) ) has 2 possibilities of being spam or non-spam ( \\(y\\) ). Our task at hand is to assign probabilities to all the features with all the possible combinations (2). \\[P(x,y) = \\underbrace{P(x)}_{P (\\text{email})} \\times \\underbrace{ P(y|x)}_{ P( \\text{spam|email})} = \\underbrace{ P(y)}_{ P( \\text{spam})} \\times \\underbrace{ P(x|y)}_{ P( \\text{email|spam})}\\] There are several advantages to writing \\(P(x,y)\\) in such a way, though we only care about at the end of the day is \\(P(y|x)\\) , that is , given a feature vector what is probability it is spam or not spam. If we already know the structure of \\(P(y|x)\\) we can ignore the probability of \\(P(x)\\) , as it doesnt help us much with the predicition of a new test datapoint. So in some sense, we can just do a discriminative model for \\(P(y|x)\\) , instead of modeling \\(P(y|x)\\) and \\(P(x)\\) . In the second equation ( \\(P(x|y) \\times P(y)\\) ) , we arent assuming a specific structure for \\(y\\) given \\(x\\) , instead we assume how labels are generated which is \\(P(y)\\) . Basically , we are assuming how to the structure/email will be given that we know the email is spam or not spam. This is a more meaningful assumption as we have some way of understanding for how the email will look like given that it is spam or not spam. So now we will be modeling for \\(P(y)\\) and \\(P(x|y)\\) . Generative Story for Email Given Label To make a generative model we will now make generative story to train our model. Our generative story will have 2 steps, Step 1 : Decide the label of the email by tossing a coin. \\(P(y_i = 1) = p\\) Step 2 : Decide the features of the email using the label in previous step. \\(P(x_i | y_i)\\) In Step1 we are basically tossing a coin and if (lets say) comes out to be heads , we say that this email is spam with some probability \\(P\\) . The Step1 is deciding on which type of email (spam or non-spam) are we trying to generate. In Step2 we are basically generating the email itself for a set of words with different probabilities (which sum upto 1) given we know which kind of email it is from Step1. We follow the Step1 and Step2 from our algorithm above, and generate an email of features with \\(d\\) length. Now lets say our dictionary only had \\(3\\) words, so the total number of possible feature vectors will be \\(2^{d=3} = 8\\) , each feature can take 2 values (0,1). For our spam category, the words like \"lottery\" , \"win\" will have a higher probability as features than the non-spam category. Note that the probability of each feature in a category represents its chance of occurence within that specific category. Even if the feature vectors on both the categories are the same , the sets of probabilities are different for both of them. Some of the probabilities might even be zero for for some features in (lets say) spam category when compared to the non-spam category. Now if we want to learn a model from the above algorithm , we need to consider how many parameters are there for this coin and 2 dice. The coin has probability \\(p\\) , so thats 1 of the parameters. The spam dice has \\(d\\) probabilities , each probability coressponding to the combination of the features . Each feature has 2 possibilities (0,1), for \\(d\\) number of features we will have \\(2^d\\) parameters. We know that all the probabilities sum upto 1 , which means we can always find out the \\(d^{\\text{th}}\\) probability using \\(2^d-1\\) probabilities. Therefore at the end we are left with \\(2^d -1\\) probabilities. The non-spam dice is the same as spam dice just the probabilities of the features is different , hence we will also have \\(2^d-1\\) parameters for non-spam dice. Our total number of parameters will be \\(1 + (2^d -1) + (2^d -1) = 2^{d+1} - 1\\) . We can see that as we increase our number of features , the number of parameters increase exponentially. This is simply too many parameters to handle/compute. If our dictionary had 10000 words , our parameters to compute would be \\(2^10000\\) , to put that into perpective , this number is greater than the number of atoms in the entire observable universe.","title":"Types of Models"},{"location":"Types%20of%20Models/#types-of-models","text":"","title":"Types of Models"},{"location":"Types%20of%20Models/#generative-model-based-algorithm","text":"For a dataset \\(D= \\{ (x_1,y_1), (x_2,y_2), ... (x_n,y_n) \\}\\) , let \\(x\\) and \\(y\\) both be binary features such that \\(x_i \\in \\{ 0,1 \\}^d\\) and \\(y_i \\in \\{ 0,1 \\}\\) Can features be binary? Does it even have an real life uses? The features can be binary and usually represent Yes/No type question. One of the examples is of spam classification of mail which we will be doing now.","title":"Generative Model-Based Algorithm"},{"location":"Types%20of%20Models/#spam-classifcication","text":"For the spam classification of emails we will have to make \\(d\\) length featuers for each email , even when the lengths and words of emails are different. One of the ways we could do this is to make a very very long vector/list which stores the poisition of each word in a dictionary in ascending order. Example Lets say (for simplicity) our dictionary has 4 words, [\"Welcome\", \"to\" , \"earth\" , \"aliens\"]. Although, the words arent ordered here, our feature vectors/lists will be ordered. Now lets say that our mail has the following 2 words, \"Welcome Aliens\". We will encode it in such a way that each feature represents the poisition and occurence of the word in dictionary. Feature vector of \\(d\\) length for \"Welcome Aliens\" will be, [1 , 0 , 0 , 1]. To do Generative Modeling for our \\(d\\) length feature vectors, our next step would be assign probabilities to each word for being in the spam and non-spam category. In other words each feature ( \\(x\\) ) has 2 possibilities of being spam or non-spam ( \\(y\\) ). Our task at hand is to assign probabilities to all the features with all the possible combinations (2). \\[P(x,y) = \\underbrace{P(x)}_{P (\\text{email})} \\times \\underbrace{ P(y|x)}_{ P( \\text{spam|email})} = \\underbrace{ P(y)}_{ P( \\text{spam})} \\times \\underbrace{ P(x|y)}_{ P( \\text{email|spam})}\\] There are several advantages to writing \\(P(x,y)\\) in such a way, though we only care about at the end of the day is \\(P(y|x)\\) , that is , given a feature vector what is probability it is spam or not spam. If we already know the structure of \\(P(y|x)\\) we can ignore the probability of \\(P(x)\\) , as it doesnt help us much with the predicition of a new test datapoint. So in some sense, we can just do a discriminative model for \\(P(y|x)\\) , instead of modeling \\(P(y|x)\\) and \\(P(x)\\) . In the second equation ( \\(P(x|y) \\times P(y)\\) ) , we arent assuming a specific structure for \\(y\\) given \\(x\\) , instead we assume how labels are generated which is \\(P(y)\\) . Basically , we are assuming how to the structure/email will be given that we know the email is spam or not spam. This is a more meaningful assumption as we have some way of understanding for how the email will look like given that it is spam or not spam. So now we will be modeling for \\(P(y)\\) and \\(P(x|y)\\) .","title":"Spam Classifcication"},{"location":"Types%20of%20Models/#generative-story-for-email-given-label","text":"To make a generative model we will now make generative story to train our model. Our generative story will have 2 steps, Step 1 : Decide the label of the email by tossing a coin. \\(P(y_i = 1) = p\\) Step 2 : Decide the features of the email using the label in previous step. \\(P(x_i | y_i)\\) In Step1 we are basically tossing a coin and if (lets say) comes out to be heads , we say that this email is spam with some probability \\(P\\) . The Step1 is deciding on which type of email (spam or non-spam) are we trying to generate. In Step2 we are basically generating the email itself for a set of words with different probabilities (which sum upto 1) given we know which kind of email it is from Step1. We follow the Step1 and Step2 from our algorithm above, and generate an email of features with \\(d\\) length. Now lets say our dictionary only had \\(3\\) words, so the total number of possible feature vectors will be \\(2^{d=3} = 8\\) , each feature can take 2 values (0,1). For our spam category, the words like \"lottery\" , \"win\" will have a higher probability as features than the non-spam category. Note that the probability of each feature in a category represents its chance of occurence within that specific category. Even if the feature vectors on both the categories are the same , the sets of probabilities are different for both of them. Some of the probabilities might even be zero for for some features in (lets say) spam category when compared to the non-spam category. Now if we want to learn a model from the above algorithm , we need to consider how many parameters are there for this coin and 2 dice. The coin has probability \\(p\\) , so thats 1 of the parameters. The spam dice has \\(d\\) probabilities , each probability coressponding to the combination of the features . Each feature has 2 possibilities (0,1), for \\(d\\) number of features we will have \\(2^d\\) parameters. We know that all the probabilities sum upto 1 , which means we can always find out the \\(d^{\\text{th}}\\) probability using \\(2^d-1\\) probabilities. Therefore at the end we are left with \\(2^d -1\\) probabilities. The non-spam dice is the same as spam dice just the probabilities of the features is different , hence we will also have \\(2^d-1\\) parameters for non-spam dice. Our total number of parameters will be \\(1 + (2^d -1) + (2^d -1) = 2^{d+1} - 1\\) . We can see that as we increase our number of features , the number of parameters increase exponentially. This is simply too many parameters to handle/compute. If our dictionary had 10000 words , our parameters to compute would be \\(2^10000\\) , to put that into perpective , this number is greater than the number of atoms in the entire observable universe.","title":"Generative Story for Email Given Label"},{"location":"classification/","text":"Note Throughout the diagrams I have used red color to indicate a +1 or positive label and blue color for -1 or a negative label Classification Introduction to Binary Classification For a set of datapoints \\(\\{ x_1 , x_2 , x_3 ... x_n \\}\\) where \\(x \\in \\mathbb{R}^d\\) and labels \\(\\{ y_1 , y_2 , y_3 ... y_n \\}\\) where \\(y \\in \\{ 0,1 \\} / \\{ -1,1 \\}\\) . Our goal is to get a function \\(h\\) which maps the datapoints to the classes ( \\(h:\\mathbb{R}^d \\to \\{ 0,1 \\}\\) ). Loss Function \\[\\text{Loss}(h) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{1}(h(x_i) \\neq y_i) \\quad \\quad \\quad \\mathbb{1}(z) = \\begin{cases} 1 & \\text{if true} \\\\ 0 & \\text{otherwise} \\end{cases}\\] Like in the case of linear regression , where we restricted the space for the loss function , to a linear space ; We can do something similar to that here too \\[\\begin{equation*} \\begin{split} \\underset{h \\in \\mathcal{H}_\\text{linear}}{\\min} \\sum_{i=1}^n \\mathbb{1}(h(x_i) \\neq y_i) \\\\ \\\\ \\text{where,}\\\\ \\\\ \\mathcal{H}_\\text{linear} = \\{ h_w : h_w(x) = \\text{sign}(w^T x) \\} \\\\ \\text{sign}(z) = \\begin{cases} 1 & \\text{if z>0} \\\\ 0 & \\text{otherwise} \\end{cases} \\end{split} \\end{equation*}\\] The above problem is an NP-Hard Problem in general. In regression we used to take the gradient , set it to zero and find the \\(w\\) , that doesnt apply here because the loss here can only take 2 discrete values (0 or 1) which makes this loss function non - differentiable an non - continuous. Can we somehow use linear regression to solve for this classification problem? For a dataset \\(\\{(x_1 , y_1) , (x_2,y_2) , ... (x_n , y_n) \\}\\) , then we input this dataset to a linear regression model which gives out a \\(w \\in \\mathbb{R}^d\\) . At the end we use this \\(w\\) to get \\(h_w\\) by solving for \\(w^Tx =0\\) ( \\(h_w : \\mathbb{R}^d \\to \\{ 0,1 \\}\\) ). Is this a good idea? Does this help us? Now if we add more positive label datapoints , We can see that the \\(w\\) gets skewed which in turn changes \\(h_w\\) . The above algorithm depends on a linear regression model which solves for \\(w\\) by taking into account all the datapoints. From a linear classification point of view , the classifying line \\(h_w\\) shouldnt be changed when new datapoints are introduced on either side but from a regression point of view these datapoints which are far apart from the classifying line ,we are trying to minimize over all the datapoints , because of which our \\(w\\) gets tilted back and forth. This will give us lines which dont actually classify the datapoints properly. Conclusion Regression is sensitive to location of the datapoints and not just the \"side\" on which the data lies with respect to the separator. K-Nearest Neighbours (KNN) For some set of datapoints with corresponding labels , if a new datapoint's (for which we predict the label) belongs to the training dataset , then we already have an answer for its (new datapoint's) label, but what to do when it doesnt belong to the training dataset? Given a test points \\(x_\\text{test} \\in \\mathbb{R}^d\\) , find the closest point \\(x^*\\) to \\(x_\\text{test}\\) in the training set. Predict \\(y_\\text{test} = y^*\\) Is this a good algorithm? Issues with this algorithm This algorithm can get affected by outliers. Lets say our nearest point to \\(x_\\text{test}\\) happens to be an outlier, then our algorithm will predict the same value as \\(y^*\\) , which may label the point wrongly. Simple Fix for Issue Above Given \\(x_\\text{test}\\) , find the \\(k\\) closest points in the training dataset \\((x_1^* , x_2^* , ... x_k^*)\\) . Predict \\(y_\\text{test} = \\text{Majority}(y_1^* , y_2^* , ... y_k^*)\\) We have to supply the parameter \\(k\\) for the above fix , which the data does not tell us, so how many neighbours we should look for? Decision Boundry Case 1 (k = 1) Lets look at the case when \\(k=1\\) , Note that these datapoints already have specified labels which comes from the dataset. Because \\(k=1\\) we get \"holes\" in certain regions for the decision boundry, looking at it objectively the blue point in the red region and vis a vis are certainly outliers and should be ignored , but our classification algorithm is sensitive to outliers. Also on the extreme right end we can see that there are some red points , which makes the whole area on the right region as red , ideally that should have been blue , as those extreme right red points are outliers. Now we know that taking \\(k=1\\) is a bad idea. ;C Animation The nearest datapoint to \\(x_\\text{test}\\) (White Dot) is the Positively Labelled (Red) Dot. Case 2 (k=n) Now lets look at the case when \\(k=n\\) Note that these datapoints already have specified labels which comes from the dataset. Our algorithm here considers all the datapoints as closest , because \\(k=n\\) , hence we take the majority of the labels and make the decision boundry. In our case the majority of the labels are blue (negative) and hence the whole region is considered to be blue (negative). In other words if we do a prediciton for a label of some point , our answer will always be negative (blue) label. Problems Encountered Asking too few ( \\(k=1\\) ) neighbours gives us an outlier issue. Asking too many ( \\(k=n\\) ) neighbours gives us the \"majority label\". Animation The Majority Label in the dataset is +1 (Red Dot) hence, \\(x_\\text{test}\\) (White Dot) is Positively Labelled. What to do then? We must find such a \\(k^*\\) that it ignores the outliers and yet it maintains a resonable decision boundry. Animation Here we took k=25. It can be seen that \\(x_\\text{test}\\) (White Dot) lies near a cluster of Positively Labelled (Red) Dots. How do find the right number of neighbours ( \\(k^*\\) )? Chossing k* We know that we can treat \\(k\\) as an hyperparameter , because its not a part of the algorithm but rather the input which goes into the algorithm. We also know that , smaller the \\(k\\) the more complicated the decision boundry is. To solve this , we choose different values of \\(k\\) and cross validate for them respectively. Issues with KNN Algorithm Choosing distance function (to identify the closest \\(k\\) neighbours) might become an issue. When predicting a new datapoint \\(x_\\text{test}\\) we measure the distance of \\(x_\\text{test}\\) from all the datapoints and then identify the \\(k\\) nearest neighbours. Now if we want to predict the label for another datapoint , we have to repeat the whole procedure again. This shows that we dont actually learn a \"model\" and our algorithm solely relies on the datapoints. We cannot throw away the datapoints after learning a \"model\" , because in the first place , there is no model for KNN . This is the biggest issue with KNN Algorithm. Introduction to Decision Trees The input for decision tree algorithm is the usual dataset \\(\\{ (x_1,y_1) , (x_2,y_2) , ... (x_n,y_n) \\}\\) where for all \\(i\\) , \\(x_i \\in \\mathbb{R}^d\\) and \\(y \\in \\{ +1,-1 \\}\\) . The output of this algorithm is a \"Decision Tree\". What is a Decision Tree? It is a tree-like structure where each internal node represents a decision based on the value of a specific feature, each branch represents the outcome of that decision, and each leaf node represents the final predicted outcome or class label. How do we get a prediction for a point? To get the prediction of a datapoint \\(x_\\text{test}\\) , we simply traverese through the decision tree , asking questions until we reach the leaf nodes. At the end we assign the value of the leaf node to \\(y_\\text{test}\\) . The model here (which wasnt in there in KNN) is the decision tree , after training our dataset on the decision tree , we can get rid of the dataset and predict new \"test points\" by solely relying on the decision tree. Whats a question in a decision tree? In a decision tree , a question is a feature-value pair. Where the \"feature\" is the feature in the datapoint and the \"value\" is the number with which we compare the \"feature\". How to measure goodness of a question? For a dataset \\(D = \\{ (x_1 , y_1), (x_2 , y_2), ... (x_n , y_n) \\}\\) , for a particular feature , we want to ask such a question that (ideally) it matches each datapoint to its label , in other words our predicition should be the same as the labels assigned to a datapoint in the dataset \\(D\\) . In reality , such questions may not exist , this means we have to somehow capture the notion of \"impurity\" for these questions. Measure of impurity for a set of datapoints For a dataset \\(D = \\{ (x_1 , y_1), (x_2 , y_2), ... (x_n , y_n) \\}\\) , an \"impurity\" function for a question can be given as \\[\\begin{equation*} \\begin{split} \\text{Entropy}(y_1 , y_2 , ... y_n) &= \\text{Entropy}(p) \\\\ &= - (p \\log_2 p + (1-p) \\log_2 (1-p) ) \\;\\;\\;\\;\\;\\;\\;\\; [\\text{convention} \\;\\; \\log_2 0 = 0] \\end{split} \\end{equation*}\\] When \\(p=0\\) or \\(p=1\\) , it means that all of the datapoints are classified to a single label. At \\(p=0.5\\) , entropy is the highest , hence its the worst case. Note that the entropy at \\(p\\) and \\(1-p\\) is always the same. By convention , \\(p = \\frac{\\text{Total Number of 1s}}{\\text{Total Number of DataPoints}}\\) We know that for a given question , we assign labels to datapoints based on the question asked. A question always assigns datapoints to 2 labels , either a 1 or -1. We can measure the entropy of the dataset \\(D\\) and the points which belong to the assigned labels , but how do we combine these 3 values (entropy of D , entropy of points assigned to label 1 , entropy of points assigned to label -1) into a singular value measure overall impurity? Information Gain Information Gain for a feature,value pair is given as, \\[\\begin{equation*} \\begin{split} \\text{Information Gain}(\\text{feature,value}) &= \\text{Entropy}(D) - [\\gamma \\text{Entropy}(D_\\text{yes}) + (1- \\gamma)\\text{Entropy}(D_\\text{no})] \\\\ \\\\ \\text{where,}\\\\ \\\\ \\gamma &= \\frac{|D_\\text{yes}|}{|D|} \\end{split} \\end{equation*}\\] We use \\(\\gamma\\) to take into account the number of datapoints. A dataset \\(\\alpha\\) with 100 datapoints , from which 99 are classified as label 1 and another dataset \\(\\beta\\) with 10000 datapoints , from which 9900 are classified as label 1 , will have the same entropy but the measure of information gain will be higher for \\(\\beta\\) when compared to \\(\\alpha\\) , as the decision tree trained on \\(\\beta\\) was able to classify more datapoints. Decision Tree Algorithm Discretize each feature in the [min,max] range. Pick the question that has the highest information gain. Repeat the procedure for \\(D_\\text{yes}\\) and \\(D_\\text{no}\\) . We can keep adding questions to the decision tree by using the above procedure , but where do we stop? We stop adding new questions to the decision tree after we reach a certain \"threshold of purity\" , this is generally around 90% purity. Example of Decision Tree We classify the blue point on the right side as outlier when we use cross validation method. If we were to keep adding more questions such that the outlier no longer lies in an \"incorrect\" region , the purity of the decision tree may increase and so will the complexity. Our goal is to make small decision trees with a respectable measure of \"purity\". Generative and Discriminative Models In classical classification problems, two types of models are commonly employed: generative models and discriminative models. Generative models capture the joint distribution between features and labels and are represented as: \\[P(x,y)\\] These models focus on modeling the feature generation process. On the other hand, discriminative models directly model the conditional probability of labels given the features and are represented as: \\[P(y|x)\\] Discriminative models generate labels solely based on the provided data. It is important to understand the differences between generative and discriminative models when choosing an appropriate modeling approach for a given classification problem.","title":"Classification"},{"location":"classification/#classification","text":"","title":"Classification"},{"location":"classification/#introduction-to-binary-classification","text":"For a set of datapoints \\(\\{ x_1 , x_2 , x_3 ... x_n \\}\\) where \\(x \\in \\mathbb{R}^d\\) and labels \\(\\{ y_1 , y_2 , y_3 ... y_n \\}\\) where \\(y \\in \\{ 0,1 \\} / \\{ -1,1 \\}\\) . Our goal is to get a function \\(h\\) which maps the datapoints to the classes ( \\(h:\\mathbb{R}^d \\to \\{ 0,1 \\}\\) ).","title":"Introduction to Binary Classification"},{"location":"classification/#loss-function","text":"\\[\\text{Loss}(h) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{1}(h(x_i) \\neq y_i) \\quad \\quad \\quad \\mathbb{1}(z) = \\begin{cases} 1 & \\text{if true} \\\\ 0 & \\text{otherwise} \\end{cases}\\] Like in the case of linear regression , where we restricted the space for the loss function , to a linear space ; We can do something similar to that here too \\[\\begin{equation*} \\begin{split} \\underset{h \\in \\mathcal{H}_\\text{linear}}{\\min} \\sum_{i=1}^n \\mathbb{1}(h(x_i) \\neq y_i) \\\\ \\\\ \\text{where,}\\\\ \\\\ \\mathcal{H}_\\text{linear} = \\{ h_w : h_w(x) = \\text{sign}(w^T x) \\} \\\\ \\text{sign}(z) = \\begin{cases} 1 & \\text{if z>0} \\\\ 0 & \\text{otherwise} \\end{cases} \\end{split} \\end{equation*}\\] The above problem is an NP-Hard Problem in general. In regression we used to take the gradient , set it to zero and find the \\(w\\) , that doesnt apply here because the loss here can only take 2 discrete values (0 or 1) which makes this loss function non - differentiable an non - continuous. Can we somehow use linear regression to solve for this classification problem? For a dataset \\(\\{(x_1 , y_1) , (x_2,y_2) , ... (x_n , y_n) \\}\\) , then we input this dataset to a linear regression model which gives out a \\(w \\in \\mathbb{R}^d\\) . At the end we use this \\(w\\) to get \\(h_w\\) by solving for \\(w^Tx =0\\) ( \\(h_w : \\mathbb{R}^d \\to \\{ 0,1 \\}\\) ). Is this a good idea? Does this help us? Now if we add more positive label datapoints , We can see that the \\(w\\) gets skewed which in turn changes \\(h_w\\) . The above algorithm depends on a linear regression model which solves for \\(w\\) by taking into account all the datapoints. From a linear classification point of view , the classifying line \\(h_w\\) shouldnt be changed when new datapoints are introduced on either side but from a regression point of view these datapoints which are far apart from the classifying line ,we are trying to minimize over all the datapoints , because of which our \\(w\\) gets tilted back and forth. This will give us lines which dont actually classify the datapoints properly. Conclusion Regression is sensitive to location of the datapoints and not just the \"side\" on which the data lies with respect to the separator.","title":"Loss Function"},{"location":"classification/#k-nearest-neighbours-knn","text":"For some set of datapoints with corresponding labels , if a new datapoint's (for which we predict the label) belongs to the training dataset , then we already have an answer for its (new datapoint's) label, but what to do when it doesnt belong to the training dataset? Given a test points \\(x_\\text{test} \\in \\mathbb{R}^d\\) , find the closest point \\(x^*\\) to \\(x_\\text{test}\\) in the training set. Predict \\(y_\\text{test} = y^*\\) Is this a good algorithm? Issues with this algorithm This algorithm can get affected by outliers. Lets say our nearest point to \\(x_\\text{test}\\) happens to be an outlier, then our algorithm will predict the same value as \\(y^*\\) , which may label the point wrongly. Simple Fix for Issue Above Given \\(x_\\text{test}\\) , find the \\(k\\) closest points in the training dataset \\((x_1^* , x_2^* , ... x_k^*)\\) . Predict \\(y_\\text{test} = \\text{Majority}(y_1^* , y_2^* , ... y_k^*)\\) We have to supply the parameter \\(k\\) for the above fix , which the data does not tell us, so how many neighbours we should look for?","title":"K-Nearest Neighbours (KNN)"},{"location":"classification/#decision-boundry","text":"","title":"Decision Boundry"},{"location":"classification/#case-1-k-1","text":"Lets look at the case when \\(k=1\\) , Note that these datapoints already have specified labels which comes from the dataset. Because \\(k=1\\) we get \"holes\" in certain regions for the decision boundry, looking at it objectively the blue point in the red region and vis a vis are certainly outliers and should be ignored , but our classification algorithm is sensitive to outliers. Also on the extreme right end we can see that there are some red points , which makes the whole area on the right region as red , ideally that should have been blue , as those extreme right red points are outliers. Now we know that taking \\(k=1\\) is a bad idea. ;C Animation The nearest datapoint to \\(x_\\text{test}\\) (White Dot) is the Positively Labelled (Red) Dot.","title":"Case 1 (k = 1)"},{"location":"classification/#case-2-kn","text":"Now lets look at the case when \\(k=n\\) Note that these datapoints already have specified labels which comes from the dataset. Our algorithm here considers all the datapoints as closest , because \\(k=n\\) , hence we take the majority of the labels and make the decision boundry. In our case the majority of the labels are blue (negative) and hence the whole region is considered to be blue (negative). In other words if we do a prediciton for a label of some point , our answer will always be negative (blue) label. Problems Encountered Asking too few ( \\(k=1\\) ) neighbours gives us an outlier issue. Asking too many ( \\(k=n\\) ) neighbours gives us the \"majority label\". Animation The Majority Label in the dataset is +1 (Red Dot) hence, \\(x_\\text{test}\\) (White Dot) is Positively Labelled. What to do then? We must find such a \\(k^*\\) that it ignores the outliers and yet it maintains a resonable decision boundry. Animation Here we took k=25. It can be seen that \\(x_\\text{test}\\) (White Dot) lies near a cluster of Positively Labelled (Red) Dots. How do find the right number of neighbours ( \\(k^*\\) )?","title":"Case 2 (k=n)"},{"location":"classification/#chossing-k","text":"We know that we can treat \\(k\\) as an hyperparameter , because its not a part of the algorithm but rather the input which goes into the algorithm. We also know that , smaller the \\(k\\) the more complicated the decision boundry is. To solve this , we choose different values of \\(k\\) and cross validate for them respectively. Issues with KNN Algorithm Choosing distance function (to identify the closest \\(k\\) neighbours) might become an issue. When predicting a new datapoint \\(x_\\text{test}\\) we measure the distance of \\(x_\\text{test}\\) from all the datapoints and then identify the \\(k\\) nearest neighbours. Now if we want to predict the label for another datapoint , we have to repeat the whole procedure again. This shows that we dont actually learn a \"model\" and our algorithm solely relies on the datapoints. We cannot throw away the datapoints after learning a \"model\" , because in the first place , there is no model for KNN . This is the biggest issue with KNN Algorithm.","title":"Chossing k*"},{"location":"classification/#introduction-to-decision-trees","text":"The input for decision tree algorithm is the usual dataset \\(\\{ (x_1,y_1) , (x_2,y_2) , ... (x_n,y_n) \\}\\) where for all \\(i\\) , \\(x_i \\in \\mathbb{R}^d\\) and \\(y \\in \\{ +1,-1 \\}\\) . The output of this algorithm is a \"Decision Tree\". What is a Decision Tree? It is a tree-like structure where each internal node represents a decision based on the value of a specific feature, each branch represents the outcome of that decision, and each leaf node represents the final predicted outcome or class label. How do we get a prediction for a point? To get the prediction of a datapoint \\(x_\\text{test}\\) , we simply traverese through the decision tree , asking questions until we reach the leaf nodes. At the end we assign the value of the leaf node to \\(y_\\text{test}\\) . The model here (which wasnt in there in KNN) is the decision tree , after training our dataset on the decision tree , we can get rid of the dataset and predict new \"test points\" by solely relying on the decision tree. Whats a question in a decision tree? In a decision tree , a question is a feature-value pair. Where the \"feature\" is the feature in the datapoint and the \"value\" is the number with which we compare the \"feature\". How to measure goodness of a question? For a dataset \\(D = \\{ (x_1 , y_1), (x_2 , y_2), ... (x_n , y_n) \\}\\) , for a particular feature , we want to ask such a question that (ideally) it matches each datapoint to its label , in other words our predicition should be the same as the labels assigned to a datapoint in the dataset \\(D\\) . In reality , such questions may not exist , this means we have to somehow capture the notion of \"impurity\" for these questions.","title":"Introduction to Decision Trees"},{"location":"classification/#measure-of-impurity-for-a-set-of-datapoints","text":"For a dataset \\(D = \\{ (x_1 , y_1), (x_2 , y_2), ... (x_n , y_n) \\}\\) , an \"impurity\" function for a question can be given as \\[\\begin{equation*} \\begin{split} \\text{Entropy}(y_1 , y_2 , ... y_n) &= \\text{Entropy}(p) \\\\ &= - (p \\log_2 p + (1-p) \\log_2 (1-p) ) \\;\\;\\;\\;\\;\\;\\;\\; [\\text{convention} \\;\\; \\log_2 0 = 0] \\end{split} \\end{equation*}\\] When \\(p=0\\) or \\(p=1\\) , it means that all of the datapoints are classified to a single label. At \\(p=0.5\\) , entropy is the highest , hence its the worst case. Note that the entropy at \\(p\\) and \\(1-p\\) is always the same. By convention , \\(p = \\frac{\\text{Total Number of 1s}}{\\text{Total Number of DataPoints}}\\) We know that for a given question , we assign labels to datapoints based on the question asked. A question always assigns datapoints to 2 labels , either a 1 or -1. We can measure the entropy of the dataset \\(D\\) and the points which belong to the assigned labels , but how do we combine these 3 values (entropy of D , entropy of points assigned to label 1 , entropy of points assigned to label -1) into a singular value measure overall impurity?","title":"Measure of impurity for a set of datapoints"},{"location":"classification/#information-gain","text":"Information Gain for a feature,value pair is given as, \\[\\begin{equation*} \\begin{split} \\text{Information Gain}(\\text{feature,value}) &= \\text{Entropy}(D) - [\\gamma \\text{Entropy}(D_\\text{yes}) + (1- \\gamma)\\text{Entropy}(D_\\text{no})] \\\\ \\\\ \\text{where,}\\\\ \\\\ \\gamma &= \\frac{|D_\\text{yes}|}{|D|} \\end{split} \\end{equation*}\\] We use \\(\\gamma\\) to take into account the number of datapoints. A dataset \\(\\alpha\\) with 100 datapoints , from which 99 are classified as label 1 and another dataset \\(\\beta\\) with 10000 datapoints , from which 9900 are classified as label 1 , will have the same entropy but the measure of information gain will be higher for \\(\\beta\\) when compared to \\(\\alpha\\) , as the decision tree trained on \\(\\beta\\) was able to classify more datapoints.","title":"Information Gain"},{"location":"classification/#decision-tree-algorithm","text":"Discretize each feature in the [min,max] range. Pick the question that has the highest information gain. Repeat the procedure for \\(D_\\text{yes}\\) and \\(D_\\text{no}\\) . We can keep adding questions to the decision tree by using the above procedure , but where do we stop? We stop adding new questions to the decision tree after we reach a certain \"threshold of purity\" , this is generally around 90% purity.","title":"Decision Tree Algorithm"},{"location":"classification/#example-of-decision-tree","text":"We classify the blue point on the right side as outlier when we use cross validation method. If we were to keep adding more questions such that the outlier no longer lies in an \"incorrect\" region , the purity of the decision tree may increase and so will the complexity. Our goal is to make small decision trees with a respectable measure of \"purity\".","title":"Example of Decision Tree"},{"location":"classification/#generative-and-discriminative-models","text":"In classical classification problems, two types of models are commonly employed: generative models and discriminative models. Generative models capture the joint distribution between features and labels and are represented as: \\[P(x,y)\\] These models focus on modeling the feature generation process. On the other hand, discriminative models directly model the conditional probability of labels given the features and are represented as: \\[P(y|x)\\] Discriminative models generate labels solely based on the provided data. It is important to understand the differences between generative and discriminative models when choosing an appropriate modeling approach for a given classification problem.","title":"Generative and Discriminative Models"},{"location":"clustering/","text":"Clustering Clustering is a technique in unsupervised machine learning that involves grouping similar data points together into clusters or groups based on some similarity or distance measure. The goal of clustering is to discover hidden patterns, structures, or natural groupings within a dataset without any prior knowledge of the groups or categories. Problems with Clustering The goal of this weak is to understand the information about datapoints which are clustered together. If we were to cluster datapoints into \\(k\\) different clusters the total number of ways \\(n\\) datapoints can be clustered is \\(k^n\\) (this includes empty clusters). For a set of datapoints from \\(\\{x_1 , x_2 , x_3 .... x_n \\}\\) and the cluster indicators from \\(\\{z_1 , z_2 , z_3 , .... z_n \\}\\) , we need to develop a metric to get an idea of how good the clusters are. A common algorithm which can be used for this purpose is to measure the distance of the datapoints from the mean of their respective clusters and summing those values for each cluster individually , a lower value indicates that the points are closely packed within a cluster , while a higher value indicates that the points are spread apart. This algorithm can formalized into a function as follows \\[ F(z_1 , z_2 , z_3 , .... z_n ) = \\sum_{i=1}^{n} ||x_i - u_{z_i}||_2^2 \\] where \\(\\mu_{z_i}\\) is mean of each \\(z_i^{th}\\) cluster. This way of clustering is considered an NP-Hard problem and its computationally intensive as there are total of \\(k^n\\) possible combinations of datapoints. K-Means Algorithm To solve the above problem of clustering we will take a look at K-Means Algorithm. The first step is Initialization , where each cluster indicator is assigned a cluster between \\(1\\) to \\(k\\) for the \\(0^{th}\\) iteration. \\[ z_1^0 , z_2^0 , z_3^0 , .... z_n^0 \\;\\;\\;\\; \\in \\{1,2,3, .... k \\}\\] Then until Convergence , We first compute the mean of each cluster for the \\(t^{th}\\) iteration. \\[ \\mu_k^t = \\frac{\\sum_{i=1}^{n}x_i \\mathbb{1}(z_i^t = k)}{\\sum_{i=1}^{n}\\mathbb{1}(z_i^t = k)} \\;\\;\\;\\;\\;\\; \\forall k \\] The next step is reassignment of the datapoints, \\[ Z_i^{t+1} = \\underset{k}{\\text{arg } \\min} ||x_i - \\mu_t^k ||_2^2 \\] This step compares every point's distance to the mean of every other cluster , if the distane to the mean is strictly less than the distance to mean of the current cluster then the datapoint is assigned to the next cluster. Note K-Means Algorithm does not always produce the optimal solution but usally produces reasonable clusters. But what if the algorithm never actually converges? The short answer is Yes , the algorithm does converge. But how? Convergence of K-Means Algorithm FACT 1 Let \\(X_1 , X_2 , X_3 ..... X_l \\in \\mathbb{R}^d\\) \\[ v^* = \\underset{v \\in \\mathbb{R}^d}{\\text{arg min }} \\sum_{i=1}^{l} {|| x_i - v ||}^2\\] Using differentitation to solve this problem \\[ v^* = \\frac{1}{l}\\sum_{i=1}^{n}X_i \\] The actual proof of convergence of K-Means Algorithm starts here. Lets assume that we are at iteration \\(t\\) of Lloyd's/K-Means Algorithm. Then our current assignment of cluster indicators would look like \\[ Z_1^t , Z_2^t , Z_3^t .... Z_n^t \\;\\;\\;\\;\\; \\in \\{ 1, 2, 3 .... k\\}\\] Here \\(t\\) corresponds to the iteration number and \\(n\\) /subscript corresponds to the data point. Also , \\(\\mu_k^t\\) is the mean for cluster \\(k\\) in the \\(t^{\\text{th}}\\) (current) iteration. Now lets assume the algorithm does not converge and see what happens. If it doesnt converge , then the cluster indicators would be reassigned. \\[ Z_1^{t+1} , Z_2^{t+1} , Z_3^{t+1} , ...... Z_n^{t+1} \\;\\;\\;\\;\\;\\;\\; \\in \\{1,2,3....k\\} \\] After this reassignment we dont know for sure that this assignment of cluster indicators is better than the previous one , to solve this problem we will take a look at the \"objective function\". \\[ \\begin{equation} \\sum_{i=1}^n {|| x_i - \\mu_{Z_i^t}^t ||}^2 \\tag{1} \\label{1} \\end{equation} \\] Here we can see that we are in the \\(t^{th}\\) iteration in which every point is being measured in this experession to the mean of the box it is assigned to. Basically , this experession captures the distances of each point to its own mean in the \\(t^{\\text{th}}\\) iteration. \\[ \\begin{equation} \\sum_{i=1}^{n} {||x_i - \\mu_{Z_i^{t+1}}^t ||}^2 \\tag{2} \\label{2} \\end{equation} \\] Here every point is measured to the mean of the cluster it wants to switch to . Some points might have lesser distance to their current cluster than the new cluster mean, they want to switch to , in that case \\(Z_i^{t+1}\\) is the same as \\(Z_i^t\\) . While the other points might have distance closer to the new cluster mean than their current mean , in that case they jump to the new cluster and thats when the actual reassignment happens. As our assumption above , if the algorithm does not converge then there must be some points who want to jump/switch to a new cluster mean which is closer to them. This means that the sum of \\((\\ref{2})\\) will be less than the sum of \\((\\ref{1})\\) . Nature of Clusters Now that we know that the algorithm converges , what can we say about the clusters formed using this algorithm Lets understand this with an example where there are only 2 clusters. The means of the 2 clusters are \\(\\mu_1\\) and \\(\\mu_2\\) . By the algorithm's construction we know that every point is happy with their own mean , this also can be thought of as that every point that is assigned to cluster 1 is closer to \\(\\mu_1\\) than it is to \\(\\mu_2\\) . This can be expressed as, \\[ {||x - \\mu_1||}^2 \\leq {||x - \\mu_2||}^2 \\] This equation can further be changed into \\[ \\begin{equation*} \\begin{split} {||x||}^2 + {||\\mu_1||}^2 - 2x^T\\mu_1 \\leq {||x||}^2 + {||\\mu_2||}^2 - 2x^T\\mu_2 \\\\ x^T(\\mu_2 - \\mu_1) \\leq \\frac{{||\\mu_2||}^2 - {||\\mu_1||}^2}{2} \\end{split} \\end{equation*}\\] Now how do we visualize this ? , what does this actually represent? Here the black line is the difference between \\(\\mu_1\\) and \\(\\mu_2\\) and the green line perpendicular to the black one. The yellow line is drawn from the middle point of \\(\\mu_2 - \\mu_1\\) parallel to the green line , this is the line that divides the plane into 2 regions for cluster 1 and cluster 2. Initialization of Centroids, K-Means++ Initially we assigned the points to random boxes , but this is not the best way to assign the points. In K-Means++ Algorithm we pick means of points which are as far apart as they can be. It chooses first mean \\(\\mu_1^0\\) uniformly at random from \\(\\{x_1 , x_2 , x_3 , .... , x_n \\}\\) For \\(l = 2 , 3 , .... k\\) , (where \\(l\\) represents the \\(l^{\\text{th}}\\) mean that we are going to pick) choose \\(\\mu_l^0\\) probabilistically proportional to score. Here score is a positive number. \\[ S(x) = \\underset{j=1,2,.... l-1}{\\min} {||x - \\mu_j^0||}^2 \\]","title":"Clustering"},{"location":"clustering/#clustering","text":"Clustering is a technique in unsupervised machine learning that involves grouping similar data points together into clusters or groups based on some similarity or distance measure. The goal of clustering is to discover hidden patterns, structures, or natural groupings within a dataset without any prior knowledge of the groups or categories.","title":"Clustering"},{"location":"clustering/#problems-with-clustering","text":"The goal of this weak is to understand the information about datapoints which are clustered together. If we were to cluster datapoints into \\(k\\) different clusters the total number of ways \\(n\\) datapoints can be clustered is \\(k^n\\) (this includes empty clusters). For a set of datapoints from \\(\\{x_1 , x_2 , x_3 .... x_n \\}\\) and the cluster indicators from \\(\\{z_1 , z_2 , z_3 , .... z_n \\}\\) , we need to develop a metric to get an idea of how good the clusters are. A common algorithm which can be used for this purpose is to measure the distance of the datapoints from the mean of their respective clusters and summing those values for each cluster individually , a lower value indicates that the points are closely packed within a cluster , while a higher value indicates that the points are spread apart. This algorithm can formalized into a function as follows \\[ F(z_1 , z_2 , z_3 , .... z_n ) = \\sum_{i=1}^{n} ||x_i - u_{z_i}||_2^2 \\] where \\(\\mu_{z_i}\\) is mean of each \\(z_i^{th}\\) cluster. This way of clustering is considered an NP-Hard problem and its computationally intensive as there are total of \\(k^n\\) possible combinations of datapoints.","title":"Problems with Clustering"},{"location":"clustering/#k-means-algorithm","text":"To solve the above problem of clustering we will take a look at K-Means Algorithm. The first step is Initialization , where each cluster indicator is assigned a cluster between \\(1\\) to \\(k\\) for the \\(0^{th}\\) iteration. \\[ z_1^0 , z_2^0 , z_3^0 , .... z_n^0 \\;\\;\\;\\; \\in \\{1,2,3, .... k \\}\\] Then until Convergence , We first compute the mean of each cluster for the \\(t^{th}\\) iteration. \\[ \\mu_k^t = \\frac{\\sum_{i=1}^{n}x_i \\mathbb{1}(z_i^t = k)}{\\sum_{i=1}^{n}\\mathbb{1}(z_i^t = k)} \\;\\;\\;\\;\\;\\; \\forall k \\] The next step is reassignment of the datapoints, \\[ Z_i^{t+1} = \\underset{k}{\\text{arg } \\min} ||x_i - \\mu_t^k ||_2^2 \\] This step compares every point's distance to the mean of every other cluster , if the distane to the mean is strictly less than the distance to mean of the current cluster then the datapoint is assigned to the next cluster. Note K-Means Algorithm does not always produce the optimal solution but usally produces reasonable clusters. But what if the algorithm never actually converges? The short answer is Yes , the algorithm does converge. But how?","title":"K-Means Algorithm"},{"location":"clustering/#convergence-of-k-means-algorithm","text":"FACT 1 Let \\(X_1 , X_2 , X_3 ..... X_l \\in \\mathbb{R}^d\\) \\[ v^* = \\underset{v \\in \\mathbb{R}^d}{\\text{arg min }} \\sum_{i=1}^{l} {|| x_i - v ||}^2\\] Using differentitation to solve this problem \\[ v^* = \\frac{1}{l}\\sum_{i=1}^{n}X_i \\] The actual proof of convergence of K-Means Algorithm starts here. Lets assume that we are at iteration \\(t\\) of Lloyd's/K-Means Algorithm. Then our current assignment of cluster indicators would look like \\[ Z_1^t , Z_2^t , Z_3^t .... Z_n^t \\;\\;\\;\\;\\; \\in \\{ 1, 2, 3 .... k\\}\\] Here \\(t\\) corresponds to the iteration number and \\(n\\) /subscript corresponds to the data point. Also , \\(\\mu_k^t\\) is the mean for cluster \\(k\\) in the \\(t^{\\text{th}}\\) (current) iteration. Now lets assume the algorithm does not converge and see what happens. If it doesnt converge , then the cluster indicators would be reassigned. \\[ Z_1^{t+1} , Z_2^{t+1} , Z_3^{t+1} , ...... Z_n^{t+1} \\;\\;\\;\\;\\;\\;\\; \\in \\{1,2,3....k\\} \\] After this reassignment we dont know for sure that this assignment of cluster indicators is better than the previous one , to solve this problem we will take a look at the \"objective function\". \\[ \\begin{equation} \\sum_{i=1}^n {|| x_i - \\mu_{Z_i^t}^t ||}^2 \\tag{1} \\label{1} \\end{equation} \\] Here we can see that we are in the \\(t^{th}\\) iteration in which every point is being measured in this experession to the mean of the box it is assigned to. Basically , this experession captures the distances of each point to its own mean in the \\(t^{\\text{th}}\\) iteration. \\[ \\begin{equation} \\sum_{i=1}^{n} {||x_i - \\mu_{Z_i^{t+1}}^t ||}^2 \\tag{2} \\label{2} \\end{equation} \\] Here every point is measured to the mean of the cluster it wants to switch to . Some points might have lesser distance to their current cluster than the new cluster mean, they want to switch to , in that case \\(Z_i^{t+1}\\) is the same as \\(Z_i^t\\) . While the other points might have distance closer to the new cluster mean than their current mean , in that case they jump to the new cluster and thats when the actual reassignment happens. As our assumption above , if the algorithm does not converge then there must be some points who want to jump/switch to a new cluster mean which is closer to them. This means that the sum of \\((\\ref{2})\\) will be less than the sum of \\((\\ref{1})\\) .","title":"Convergence of K-Means Algorithm"},{"location":"clustering/#nature-of-clusters","text":"Now that we know that the algorithm converges , what can we say about the clusters formed using this algorithm Lets understand this with an example where there are only 2 clusters. The means of the 2 clusters are \\(\\mu_1\\) and \\(\\mu_2\\) . By the algorithm's construction we know that every point is happy with their own mean , this also can be thought of as that every point that is assigned to cluster 1 is closer to \\(\\mu_1\\) than it is to \\(\\mu_2\\) . This can be expressed as, \\[ {||x - \\mu_1||}^2 \\leq {||x - \\mu_2||}^2 \\] This equation can further be changed into \\[ \\begin{equation*} \\begin{split} {||x||}^2 + {||\\mu_1||}^2 - 2x^T\\mu_1 \\leq {||x||}^2 + {||\\mu_2||}^2 - 2x^T\\mu_2 \\\\ x^T(\\mu_2 - \\mu_1) \\leq \\frac{{||\\mu_2||}^2 - {||\\mu_1||}^2}{2} \\end{split} \\end{equation*}\\] Now how do we visualize this ? , what does this actually represent? Here the black line is the difference between \\(\\mu_1\\) and \\(\\mu_2\\) and the green line perpendicular to the black one. The yellow line is drawn from the middle point of \\(\\mu_2 - \\mu_1\\) parallel to the green line , this is the line that divides the plane into 2 regions for cluster 1 and cluster 2.","title":"Nature of Clusters"},{"location":"clustering/#initialization-of-centroids-k-means","text":"Initially we assigned the points to random boxes , but this is not the best way to assign the points. In K-Means++ Algorithm we pick means of points which are as far apart as they can be. It chooses first mean \\(\\mu_1^0\\) uniformly at random from \\(\\{x_1 , x_2 , x_3 , .... , x_n \\}\\) For \\(l = 2 , 3 , .... k\\) , (where \\(l\\) represents the \\(l^{\\text{th}}\\) mean that we are going to pick) choose \\(\\mu_l^0\\) probabilistically proportional to score. Here score is a positive number. \\[ S(x) = \\underset{j=1,2,.... l-1}{\\min} {||x - \\mu_j^0||}^2 \\]","title":"Initialization of Centroids, K-Means++"},{"location":"estimation/","text":"Estimation There is some probabilistic mechanism that generates the data about which we dont know all the parameters. The goal of estimation is to find the parameters we dont know about. Maximal Likelihood Estimation (ML) Is there a principaled way to get estimators from data? The way one can do this is by using the likelihood function. Lets say \\(x \\in \\{1,0,1,1\\}\\) , the \\(P(X_1 =1 , X_2 = 0 , X_3 = 1 , X_4 = 1)\\) will be If we were to guess the probability that \\(X\\) takes a certain value , \\(\\frac{3}{4}\\) would be what most people agree on , this guess is also justified with the plot above but how do we actually end up with this guess? , is there a certain mathematical way/formula to get here? Fisher's Principle of ML \\[\\begin{equation*} \\begin{split} L(p , \\{x_1 , x_2 , .... x_n \\}) &= P(x_1 , x_2 , x_3 , ... x_n ; p) \\\\ &= P(x_1 ;p) \\cdot P(x_2;p) .... P(x_n;p) \\\\ &= \\prod_{i=1}^{n} p^{x_i} (1-p)^{1- x_i} \\end{split} \\end{equation*}\\] From the above likelihood function we can see that the following function is to be maximized \\[\\begin{equation*} \\begin{split} \\hat{P}_{\\text{ML}} &= \\underset{p}{\\arg \\max} \\prod_{i=1}^n p^{x_i} (1-p)^{1 - x_i} \\\\ \\\\ \\text{As log is monotonous increasing function,} \\\\ \\text{the point where } \\hat{P}_{\\text{ML}} \\text{ and } \\log (\\hat{P}_{\\text{ML}}) \\\\ \\text{ take the maximum value is the same}\\\\ &= \\underset{p}{\\arg \\max} \\log \\left(\\prod_{i=1}^n p^{x_i} (1-p)^{1 - x_i} \\right) \\\\ &= \\underset{p}{\\arg \\max} \\left(\\sum_{i=1}^n \\log p * {x_i} * \\log (1-p)*(1 - x_i) \\right) \\\\ \\\\ \\text{Taking derivative of }\\log (\\hat{P}_{\\text{ML}}) \\\\ \\hat{P}_{\\text{ML}} &= \\frac{1}{n}\\sum_{i=1}^n x_i \\\\ \\end{split} \\end{equation*}\\] We can now see where our guess previously originates from","title":"Estimation"},{"location":"estimation/#estimation","text":"There is some probabilistic mechanism that generates the data about which we dont know all the parameters. The goal of estimation is to find the parameters we dont know about.","title":"Estimation"},{"location":"estimation/#maximal-likelihood-estimation-ml","text":"Is there a principaled way to get estimators from data? The way one can do this is by using the likelihood function. Lets say \\(x \\in \\{1,0,1,1\\}\\) , the \\(P(X_1 =1 , X_2 = 0 , X_3 = 1 , X_4 = 1)\\) will be If we were to guess the probability that \\(X\\) takes a certain value , \\(\\frac{3}{4}\\) would be what most people agree on , this guess is also justified with the plot above but how do we actually end up with this guess? , is there a certain mathematical way/formula to get here?","title":"Maximal Likelihood Estimation (ML)"},{"location":"estimation/#fishers-principle-of-ml","text":"\\[\\begin{equation*} \\begin{split} L(p , \\{x_1 , x_2 , .... x_n \\}) &= P(x_1 , x_2 , x_3 , ... x_n ; p) \\\\ &= P(x_1 ;p) \\cdot P(x_2;p) .... P(x_n;p) \\\\ &= \\prod_{i=1}^{n} p^{x_i} (1-p)^{1- x_i} \\end{split} \\end{equation*}\\] From the above likelihood function we can see that the following function is to be maximized \\[\\begin{equation*} \\begin{split} \\hat{P}_{\\text{ML}} &= \\underset{p}{\\arg \\max} \\prod_{i=1}^n p^{x_i} (1-p)^{1 - x_i} \\\\ \\\\ \\text{As log is monotonous increasing function,} \\\\ \\text{the point where } \\hat{P}_{\\text{ML}} \\text{ and } \\log (\\hat{P}_{\\text{ML}}) \\\\ \\text{ take the maximum value is the same}\\\\ &= \\underset{p}{\\arg \\max} \\log \\left(\\prod_{i=1}^n p^{x_i} (1-p)^{1 - x_i} \\right) \\\\ &= \\underset{p}{\\arg \\max} \\left(\\sum_{i=1}^n \\log p * {x_i} * \\log (1-p)*(1 - x_i) \\right) \\\\ \\\\ \\text{Taking derivative of }\\log (\\hat{P}_{\\text{ML}}) \\\\ \\hat{P}_{\\text{ML}} &= \\frac{1}{n}\\sum_{i=1}^n x_i \\\\ \\end{split} \\end{equation*}\\] We can now see where our guess previously originates from","title":"Fisher's Principle of ML"},{"location":"kernel_pca/","text":"Kernel PCA Issues with PCA In the previous week we learned about using PCA to reduce the number of features required to represent the dataset without much loss. There are 2 main issues/concerns with PCA Time Complexity : Finding the eigenvector and eigenvalues of matrix \\(C \\in \\mathbb{R}^(d \\times d)\\) typically takes \\(O(d^3)\\) time. This becomes an issue when the number of features is much higher than number of points. PCA works only linearly : For a dataset with 3 features , if the features are not related linearly then using PCA does not give good results. Time Complexity Issue with PCA The time complexity issue occurs when the number of features is much much greater than the number of points in the dataset , i.e. \\(d >> n\\) . According to our algorithm the solution for maximizing the variance of the dataset was to find the eigenvector corresponding to the max eigenvalue of \\(C\\) , where \\(C = \\frac{1}{n}\\sum_{i=1}^{n}x_ix_i^T\\) if \\(X \\in \\mathbb{R}^{d \\times n}\\) is the matrix which represents all the points in the dataset , \\(C\\) can also be written as \\(C = \\frac{1}{n}XX^T\\) This was just recalling and putting in the context to what is about to happen now Let \\(w_k\\) be the eigenvector corresponding to the \\(k^{th}\\) largest eigenvalue of \\(C\\) ( \\(\\lambda_k\\) ). \\[\\begin{equation*} \\begin{split} C w_k &= \\lambda_k w_k \\\\ \\left(\\frac{1}{n} \\sum_{i=1}^{n} x_ix_i^T \\right) w_k &= \\lambda_k w_k \\\\ w_k &= \\frac{1}{n \\lambda_k} \\sum_{i=1}^{n} (x_i^Tw_k)x_i \\\\ w_k &= \\sum_{i=1}^{n} \\left( \\frac{x_i^Tw_k }{n \\lambda_k} \\right)x_i \\\\ \\end{split} \\end{equation*} \\tag{1} \\label{Wk1}\\] From this equation we can obeserve that \\(\\mathbf{w_k}\\) is a linear combination of datapoints , assuming \\(\\lambda_k \\neq 0\\) . \\[\\implies w_k = X\\alpha_k \\tag{2} \\label{Wk2}\\] for some \\(\\alpha_k \\in \\mathbb{R}^n\\) To find the value \\(w_k\\) from the above equation , we need to know the value of \\(\\alpha_k\\) But if we compare \\(\\eqref{Wk1}\\) and \\(\\eqref{Wk2}\\) we can see that , \\(\\alpha_k = \\sum_{i=1}^{n} \\left( \\frac{x_i^Tw_k }{n \\lambda_k} \\right)\\) From the above equation , if we want to find \\(\\alpha_k\\) we need to know the value \\(w_k\\) itself. This is a chicken and egg problem,to solve this problem we will take a look at another equation. We know, \\[\\begin{equation*} \\begin{split} w_k &= X \\alpha_k \\;\\;\\;\\;\\;\\;\\;\\;\\; \\forall k \\\\ \\\\ Cw_k &= \\lambda_k w_k \\\\ \\left( \\frac{1}{n} XX^T \\right)(X \\alpha_k) &= \\lambda_k X \\alpha_k \\\\ \\\\ \\text{Premultiplying by} X^T \\\\ \\\\ X^T((XX^T) X \\alpha_k) &= X^T(n \\lambda_k X \\alpha_k) \\\\ (X^TX)(X^TX)\\alpha_k &= n \\lambda_k (X^TX) \\alpha_k \\\\ \\\\ \\text{Let} X^TX = K \\\\ \\\\ K^2 \\alpha_k &= n \\lambda_k K \\alpha_k \\\\ \\boxed{K \\alpha_k = (n \\lambda_k) \\alpha_k} \\\\ \\end{split} \\end{equation*}\\] It is observed that for any \\(\\alpha_k\\) lets say \\(u\\) which satisfies this equation , there exists a scaled version of \\(u\\) (Example \\(4u\\) ) which also satisfies this equation. To prevent the possibilities of scaled values of \\(\\alpha_k\\) , we will narrow down the possible values using the idea that length of \\(w_k\\) is 1 i.e. \\(||w_k|| = 1\\) . We know, \\[\\begin{equation*} \\begin{split} w_k &= X \\alpha_k \\\\ w_k^Tw_k &= (X\\alpha_k)^T(X\\alpha_k) = \\alpha_k^T (X^TX) \\alpha_k \\\\ &\\boxed{1 = \\alpha_k^T K \\alpha_k} \\\\ \\end{split} \\end{equation*}\\] Now we have 2 equations , which \\(\\alpha_k\\) must satisfy. Up until now , we wanted the eigenvectors of \\(XX^T\\) which is \\(d \\times d\\) matrix , but now we are saying we can solve the eigenequation of \\(K\\) , where \\(K = X^TX\\) which is a \\(n \\times n\\) matrix. But now the issue is , to get the value of \\(K\\) in the equation \\[K \\alpha_k = (n \\lambda_k) \\alpha_k\\] the value of \\(\\lambda_k\\) is required , which we can only get when we solve for \\(XX^T\\) matrix. Back to square one \ud83d\ude2d. Now to find \\(\\lambda_k\\) we will find a relation between eigenvalues of \\(XX^T\\) and \\(X^TX\\) . Linear Algebra Fact :The non zero eigenvalues of \\(XX^T\\) and \\(X^TX\\) are exactly the same. \\[\\begin{equation*} \\begin{split} \\text{For } C, \\\\ C &= \\frac{1}{n} XX^T \\hspace{1cm} & \\text{Eigenvectors} = \\{w_1 , w_2 .... , w_n \\} \\hspace{1cm} & \\forall k , ||w_k||^2 = 1 \\\\ && \\text{Eigenvalues} = \\{ \\lambda_1 \\geq \\lambda_2 \\geq .... \\geq \\lambda_n \\} \\\\ \\\\ \\\\ \\text{For } XX^T, \\\\ XX^T &= n \\times C & \\text{Eigenvectors} = \\{ w_1 , w_2 .... , w_l \\} \\\\ && \\text{Eigenvalues} = \\{ n\\lambda_1 \\geq n\\lambda_2 \\geq .... \\geq n\\lambda_l \\} \\\\ \\\\ \\\\ \\text{For } X^TX \\\\ &X^TX & \\text{Eigenvectors} = \\{\\beta_1 , \\beta_2 , ..... \\beta_l \\} \\hspace{1cm} & \\forall k, ||\\beta_k||^2 = 1 \\\\ && \\text{Eigenvalues} = \\{ n\\lambda_1 \\geq n\\lambda_2 \\geq .... \\geq n\\lambda_l \\} \\\\ \\end{split} \\end{equation*}\\] Note that the eigenvalues of \\(XX^T\\) are the same \\(X^TX\\) from the linear algebra fact. This gives us the final result as, \\[\\boxed{K \\beta_k = (n \\lambda_k)\\beta_k}\\] Now we need to check if \\(\\beta_k = \\alpha_k\\) ? \\[\\begin{equation*} \\beta_k^T K \\beta_k = \\beta_k^T(n \\lambda_k \\beta_k) = n \\lambda_k \\beta_k^T\\beta_k = \\boxed{n \\lambda_k} \\end{equation*}\\] Therefore, \\[\\alpha_k = \\frac{\\beta_k}{\\sqrt{n\\lambda_k}} , \\;\\;\\;\\; \\forall k \\] The gist of this solution is that , when \\(d >> n\\) we use the eigenvectors of \\(X^TX \\in \\mathbb{R}^{n \\times n}\\) instead of \\(XX^T \\in \\mathbb{R}^{d \\times d}\\) which reduces the computation time required to get the eigenvectors. New Algorithm when \\(d >> n\\) Compute \\(K = X^TX\\) , where \\(K \\in \\mathbb{R}^{n \\times n}\\) Compute the eigendecomposition of \\(K\\) to get the eigenvectors and the eigenvalues. Calculate for \\(\\alpha_k\\) , \\(\\alpha_k = \\frac{\\beta_k}{\\sqrt{n \\lambda_k}} \\;\\;\\;\\;\\; \\forall k\\) \\(\\boxed{w_k = X\\alpha_k} \\;\\;\\;\\;\\;\\; \\forall k\\) Non-Linear Relationship of Datapoints The issue of non-linear relationship arises when the datapoints are related to each other in a \"non-linear\" way , it might be quadratic , cubic or even biquadratic. Now if the points of the dataset lie in a circle , determining the \"best fit line\" is pointless , almost all of the lines will be the best fit lines for this circle with a marginal difference of \"reconstruction error\" between them. The relation features of in this dataset can be represented using equation of a circle, \\[(f_1 - a)^2 + (f_2 - b)^2 = r^2\\] We can see that this equation has quadratic terms which cannot be represented linearly. To solve this problem , we will map it to function with more features in order to represent it linearly. \\[\\begin{equation*} \\begin{split} \\underset{\\mathbb{R}^2}{[f_1 , f_2]} \\xrightarrow{\\phi} \\overset{\\phi(x)}{\\underset{\\mathbb{R}^6}{[1 , f_1^2 , f_2^2 , f_1 f_2 , f_1 , f_2]}} \\\\ \\\\ \\text{Let } u \\in \\mathbb{R}^6 \\hspace{1cm} [a^2 + b^2 - r^2 , 1 , 1, 0 , -2a , -2b] \\\\ \\end{split} \\end{equation*}\\] The function to map \\([f_1 , f_2]\\) to \\(\\mathbb{R}^6\\) is called \\(\\phi\\) and \\(u\\) is a point such that each datapoint of the original circular dataset satisfies, \\[\\boxed{\\phi(x)^Tw = 0}\\] The equation above shows that the datapoints after mapping to \\(\\mathbb{R}^6\\) lie in a linear subspace. Note Mapping \\(d\\) features to a polynomial of power \\(p\\) results in \\(^{d+p} C_d\\) new features. In the above equation of circle , it can be see in that mapping 2 features ( \\(d\\) ) to a polynomial of degree 2 ( \\(p\\) ), results in ( \\(^{2+2} C_2\\) ) 6 features. How does this solve our problem of non linear datapoints? For a dataset \\(S\\) , whose datapoints have a non-linear relationship ( \\(x_i \\in \\mathbb{R}^d\\) ), we can map this dataset to a higher dimension (linear subspace), such that after mapping , \\(x_i \\in \\mathbb{R}^D\\) . Note that \\(D > d\\) As the points (after mapping to higher dimension) are in a linear subspace , our PCA algorithm will work much better than before. Also note that , we already have a solution for the case when dimension of the datapoints is much much larger than the number of datapoints ( \\(d >> n\\) ). Kernel Functions Issues with Calculating \\(\\phi(x)\\) To run PCA on non-linear features , we came up with the solution of mapping ( \\(\\phi(x)\\) ) the features to a higher dimension and then instead of calculating eigenvectors for a \\(d \\times d\\) (Covariance) matrix , we calculated eigenvectors for \\(n \\times n\\) ( \\(X^TX\\) ) if \\(d >> n\\) . The issue with current implementation of PCA is that calculating \\(\\phi(x)\\) is nearly the same number of calculations as \\(O(d^p)\\) . This means that as the power of the polynomial increases , the calculation of individual features in the mapping \\(\\phi(x)\\) rises exponentially. Example : If 2 features are mapped to a 20 power polynomial , the resulting number of features will be nearly \\(2^{20}\\) , which is simply too many calculations. To solve this problem we will now take a look at Kernel Functions. The eigendirections we compute (when mapping datapoints to a higher dimension) in PCA is of \\(\\phi(x)^T \\phi(x)\\) . To get to \\(K_{ij}\\) we first need to calculate \\(\\phi(x_i)^T \\phi(x_j)\\) , but is there a way to directly go from \\(x_i \\quad x_j\\) to \\(K_{ij}\\) without computing \\(\\phi(x)\\) ? The function \\((x^T x^{'} + 1)^2\\) is one such function through which we can directly get to \\(\\phi(x)^T \\phi(x^{'})\\) , without having to compute \\(\\phi(x)\\) and \\(\\phi(x^{'})\\) . Lets say, \\(x = [f_1 , f_2] \\quad x^{'} = [g_1 , g_2]\\) \\[\\begin{equation*} \\begin{split} (x^T x^{'} + 1)^2 &= \\left( \\begin{bmatrix}f_1 & f_2 \\end{bmatrix} \\begin{bmatrix} g_1 \\\\ g_2 \\end{bmatrix} + 1 \\right)^2 \\\\ &= (f_1 g_1 + f_2 g_2 + 1)^2 \\\\ &= f_1^2 g_1^2 + f_2^2 g_2^2 + 1 + 2f_1 g_1 f_2 g_2 + 2f_1 g_1 + 2f_2 g_2 \\\\ \\end{split} \\end{equation*}\\] This can also be written as , \\[\\begin{equation*} \\begin{split} (x^T x^{'} + 1)^2 &= \\begin{bmatrix}f_1^2 & f_2^2 & 1 & \\sqrt{2}f_1 f_2 & \\sqrt{2}f_1 & \\sqrt{2}f_2 \\end{bmatrix} \\begin{bmatrix}g_1^2 \\\\ g_2^2 \\\\ 1 \\\\ \\sqrt{2}g_1 g_2 \\\\ \\sqrt{2}g_1 \\\\ \\sqrt{2}g_2 \\end{bmatrix} \\\\ &= \\phi(x)^T \\phi(x^{'}) \\\\ \\\\ \\text{where,}\\\\ \\\\ \\phi(x) &= \\phi \\left( \\begin{bmatrix} a \\\\ b \\end{bmatrix}\\right) = \\begin{bmatrix}a^2 \\\\ b^2 \\\\ 1 \\\\ \\sqrt{2}a b \\\\ \\sqrt{2}a \\\\ \\sqrt{2}b \\end{bmatrix} \\\\\\end{split} \\end{equation*}\\] We can see that , to calculate \\(\\phi(x)^T \\phi(x^{'})\\) we can instead solve for \\((x^T x^{'} + 1)^2\\) and skip the step for calculating \\(\\phi(x)\\) and \\(\\phi(x^{'})\\) . Valid Kernel Functions There are 2 ways to check if a function is a valid kernel function, There exists a mapping from \\(\\mathbb{R}^d\\) to \\(\\mathbb{R}\\) such that \\(k(x , x^{'}) = \\phi(x)^T \\phi(x^{'})\\) . A kernel function is considered valid if , \\(k\\) is symmetric , i.e. , \\(k(x , x^{'}) = k(x^{'} , x)\\) The kernel matrix \\(K\\) , where \\(K_{ij} = k(x_i , x_j)\\) , is positive semi-definite. Common Kernel Functions Polynomial Kernel : \\(k(x , x^{'}) = (x^Tx^{'} + 1)^p\\) Gaussian Kernel : \\(k(x , x^{'}) = \\exp \\left( - \\frac{|| x - x^{'}||^2}{2 \\sigma^2} \\right)\\) , for some \\(\\sigma > 0\\) . \\(\\phi(x)\\) in this case is mapped to infinite dimension. Kenrel PCA Now we need everything that is required to perform Kernel PCA on a non-linear dataset , lets put down an algorithm for Kernel PCA. Input \\(\\{x_1 , x_2 , \\cdots , x_n \\}\\) \\(x_i \\in \\mathbb{R}^d\\) , kernel \\(k : \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\) Compute \\(K \\in \\mathbb{R}^{n \\times n}\\) where \\(K_{ij} = k(x_i,x_j) \\quad \\forall i,j\\) Center the kernel using, \\[K^C = K - IK - KI + IKI\\] where \\(K^C\\) is the centered kernel and \\(I \\in \\mathbb{R}^{n \\times n}\\) is a matrix with all elements equal to \\(\\frac{1}{n}\\) Compute \\(\\beta_1 , \\beta_2 , \\cdots , \\beta_l\\) eigenvectors and \\(n\\lambda_1 , n\\lambda_2 , \\cdots , n\\lambda_l\\) eigenvectors of K and normalize to get \\(\\alpha_k = \\frac{\\beta_k}{\\sqrt{n \\lambda_k}}\\) Compute the compressed representation using, \\[\\begin{equation*} \\begin{split} \\phi(x_i)^T w_k &= \\phi(x_j)^T \\left( \\sum_{j=1}^n \\phi(x_i) \\alpha_{kj} \\right) \\\\ &= \\sum_{j=1}^n \\alpha_{kj} \\phi(x_i)^T \\phi(x_j) \\\\ &= \\sum_{j=1}^n \\alpha_{kj} K_{ij} \\\\ \\end{split} \\end{equation*}\\] Compute \\(\\sum_{j=1}^n \\alpha_{kj} K_{ij} \\quad \\forall k\\) \\[\\phi(\\mathbf{x}_i)^T\\mathbf{w} \\in \\mathbb{R}^{d} \\to \\left [ \\begin{array}{cccc} \\displaystyle \\sum_{j=1}^{n} \\alpha_{1j} \\mathbf{K}^C_{ij} & \\displaystyle \\sum_{j=1}^{n} \\alpha_{2j} \\mathbf{K}^C_{ij} & \\ldots & \\displaystyle \\sum_{j=1}^{n} \\alpha_{nj} \\mathbf{K}^C_{ij} \\end{array} \\right ]\\]","title":"Kernel PCA"},{"location":"kernel_pca/#kernel-pca","text":"","title":"Kernel PCA"},{"location":"kernel_pca/#issues-with-pca","text":"In the previous week we learned about using PCA to reduce the number of features required to represent the dataset without much loss. There are 2 main issues/concerns with PCA Time Complexity : Finding the eigenvector and eigenvalues of matrix \\(C \\in \\mathbb{R}^(d \\times d)\\) typically takes \\(O(d^3)\\) time. This becomes an issue when the number of features is much higher than number of points. PCA works only linearly : For a dataset with 3 features , if the features are not related linearly then using PCA does not give good results.","title":"Issues with PCA"},{"location":"kernel_pca/#time-complexity-issue-with-pca","text":"The time complexity issue occurs when the number of features is much much greater than the number of points in the dataset , i.e. \\(d >> n\\) . According to our algorithm the solution for maximizing the variance of the dataset was to find the eigenvector corresponding to the max eigenvalue of \\(C\\) , where \\(C = \\frac{1}{n}\\sum_{i=1}^{n}x_ix_i^T\\) if \\(X \\in \\mathbb{R}^{d \\times n}\\) is the matrix which represents all the points in the dataset , \\(C\\) can also be written as \\(C = \\frac{1}{n}XX^T\\) This was just recalling and putting in the context to what is about to happen now Let \\(w_k\\) be the eigenvector corresponding to the \\(k^{th}\\) largest eigenvalue of \\(C\\) ( \\(\\lambda_k\\) ). \\[\\begin{equation*} \\begin{split} C w_k &= \\lambda_k w_k \\\\ \\left(\\frac{1}{n} \\sum_{i=1}^{n} x_ix_i^T \\right) w_k &= \\lambda_k w_k \\\\ w_k &= \\frac{1}{n \\lambda_k} \\sum_{i=1}^{n} (x_i^Tw_k)x_i \\\\ w_k &= \\sum_{i=1}^{n} \\left( \\frac{x_i^Tw_k }{n \\lambda_k} \\right)x_i \\\\ \\end{split} \\end{equation*} \\tag{1} \\label{Wk1}\\] From this equation we can obeserve that \\(\\mathbf{w_k}\\) is a linear combination of datapoints , assuming \\(\\lambda_k \\neq 0\\) . \\[\\implies w_k = X\\alpha_k \\tag{2} \\label{Wk2}\\] for some \\(\\alpha_k \\in \\mathbb{R}^n\\) To find the value \\(w_k\\) from the above equation , we need to know the value of \\(\\alpha_k\\) But if we compare \\(\\eqref{Wk1}\\) and \\(\\eqref{Wk2}\\) we can see that , \\(\\alpha_k = \\sum_{i=1}^{n} \\left( \\frac{x_i^Tw_k }{n \\lambda_k} \\right)\\) From the above equation , if we want to find \\(\\alpha_k\\) we need to know the value \\(w_k\\) itself. This is a chicken and egg problem,to solve this problem we will take a look at another equation. We know, \\[\\begin{equation*} \\begin{split} w_k &= X \\alpha_k \\;\\;\\;\\;\\;\\;\\;\\;\\; \\forall k \\\\ \\\\ Cw_k &= \\lambda_k w_k \\\\ \\left( \\frac{1}{n} XX^T \\right)(X \\alpha_k) &= \\lambda_k X \\alpha_k \\\\ \\\\ \\text{Premultiplying by} X^T \\\\ \\\\ X^T((XX^T) X \\alpha_k) &= X^T(n \\lambda_k X \\alpha_k) \\\\ (X^TX)(X^TX)\\alpha_k &= n \\lambda_k (X^TX) \\alpha_k \\\\ \\\\ \\text{Let} X^TX = K \\\\ \\\\ K^2 \\alpha_k &= n \\lambda_k K \\alpha_k \\\\ \\boxed{K \\alpha_k = (n \\lambda_k) \\alpha_k} \\\\ \\end{split} \\end{equation*}\\] It is observed that for any \\(\\alpha_k\\) lets say \\(u\\) which satisfies this equation , there exists a scaled version of \\(u\\) (Example \\(4u\\) ) which also satisfies this equation. To prevent the possibilities of scaled values of \\(\\alpha_k\\) , we will narrow down the possible values using the idea that length of \\(w_k\\) is 1 i.e. \\(||w_k|| = 1\\) . We know, \\[\\begin{equation*} \\begin{split} w_k &= X \\alpha_k \\\\ w_k^Tw_k &= (X\\alpha_k)^T(X\\alpha_k) = \\alpha_k^T (X^TX) \\alpha_k \\\\ &\\boxed{1 = \\alpha_k^T K \\alpha_k} \\\\ \\end{split} \\end{equation*}\\] Now we have 2 equations , which \\(\\alpha_k\\) must satisfy. Up until now , we wanted the eigenvectors of \\(XX^T\\) which is \\(d \\times d\\) matrix , but now we are saying we can solve the eigenequation of \\(K\\) , where \\(K = X^TX\\) which is a \\(n \\times n\\) matrix. But now the issue is , to get the value of \\(K\\) in the equation \\[K \\alpha_k = (n \\lambda_k) \\alpha_k\\] the value of \\(\\lambda_k\\) is required , which we can only get when we solve for \\(XX^T\\) matrix. Back to square one \ud83d\ude2d. Now to find \\(\\lambda_k\\) we will find a relation between eigenvalues of \\(XX^T\\) and \\(X^TX\\) . Linear Algebra Fact :The non zero eigenvalues of \\(XX^T\\) and \\(X^TX\\) are exactly the same. \\[\\begin{equation*} \\begin{split} \\text{For } C, \\\\ C &= \\frac{1}{n} XX^T \\hspace{1cm} & \\text{Eigenvectors} = \\{w_1 , w_2 .... , w_n \\} \\hspace{1cm} & \\forall k , ||w_k||^2 = 1 \\\\ && \\text{Eigenvalues} = \\{ \\lambda_1 \\geq \\lambda_2 \\geq .... \\geq \\lambda_n \\} \\\\ \\\\ \\\\ \\text{For } XX^T, \\\\ XX^T &= n \\times C & \\text{Eigenvectors} = \\{ w_1 , w_2 .... , w_l \\} \\\\ && \\text{Eigenvalues} = \\{ n\\lambda_1 \\geq n\\lambda_2 \\geq .... \\geq n\\lambda_l \\} \\\\ \\\\ \\\\ \\text{For } X^TX \\\\ &X^TX & \\text{Eigenvectors} = \\{\\beta_1 , \\beta_2 , ..... \\beta_l \\} \\hspace{1cm} & \\forall k, ||\\beta_k||^2 = 1 \\\\ && \\text{Eigenvalues} = \\{ n\\lambda_1 \\geq n\\lambda_2 \\geq .... \\geq n\\lambda_l \\} \\\\ \\end{split} \\end{equation*}\\] Note that the eigenvalues of \\(XX^T\\) are the same \\(X^TX\\) from the linear algebra fact. This gives us the final result as, \\[\\boxed{K \\beta_k = (n \\lambda_k)\\beta_k}\\] Now we need to check if \\(\\beta_k = \\alpha_k\\) ? \\[\\begin{equation*} \\beta_k^T K \\beta_k = \\beta_k^T(n \\lambda_k \\beta_k) = n \\lambda_k \\beta_k^T\\beta_k = \\boxed{n \\lambda_k} \\end{equation*}\\] Therefore, \\[\\alpha_k = \\frac{\\beta_k}{\\sqrt{n\\lambda_k}} , \\;\\;\\;\\; \\forall k \\] The gist of this solution is that , when \\(d >> n\\) we use the eigenvectors of \\(X^TX \\in \\mathbb{R}^{n \\times n}\\) instead of \\(XX^T \\in \\mathbb{R}^{d \\times d}\\) which reduces the computation time required to get the eigenvectors. New Algorithm when \\(d >> n\\) Compute \\(K = X^TX\\) , where \\(K \\in \\mathbb{R}^{n \\times n}\\) Compute the eigendecomposition of \\(K\\) to get the eigenvectors and the eigenvalues. Calculate for \\(\\alpha_k\\) , \\(\\alpha_k = \\frac{\\beta_k}{\\sqrt{n \\lambda_k}} \\;\\;\\;\\;\\; \\forall k\\) \\(\\boxed{w_k = X\\alpha_k} \\;\\;\\;\\;\\;\\; \\forall k\\)","title":"Time Complexity Issue with PCA"},{"location":"kernel_pca/#non-linear-relationship-of-datapoints","text":"The issue of non-linear relationship arises when the datapoints are related to each other in a \"non-linear\" way , it might be quadratic , cubic or even biquadratic. Now if the points of the dataset lie in a circle , determining the \"best fit line\" is pointless , almost all of the lines will be the best fit lines for this circle with a marginal difference of \"reconstruction error\" between them. The relation features of in this dataset can be represented using equation of a circle, \\[(f_1 - a)^2 + (f_2 - b)^2 = r^2\\] We can see that this equation has quadratic terms which cannot be represented linearly. To solve this problem , we will map it to function with more features in order to represent it linearly. \\[\\begin{equation*} \\begin{split} \\underset{\\mathbb{R}^2}{[f_1 , f_2]} \\xrightarrow{\\phi} \\overset{\\phi(x)}{\\underset{\\mathbb{R}^6}{[1 , f_1^2 , f_2^2 , f_1 f_2 , f_1 , f_2]}} \\\\ \\\\ \\text{Let } u \\in \\mathbb{R}^6 \\hspace{1cm} [a^2 + b^2 - r^2 , 1 , 1, 0 , -2a , -2b] \\\\ \\end{split} \\end{equation*}\\] The function to map \\([f_1 , f_2]\\) to \\(\\mathbb{R}^6\\) is called \\(\\phi\\) and \\(u\\) is a point such that each datapoint of the original circular dataset satisfies, \\[\\boxed{\\phi(x)^Tw = 0}\\] The equation above shows that the datapoints after mapping to \\(\\mathbb{R}^6\\) lie in a linear subspace. Note Mapping \\(d\\) features to a polynomial of power \\(p\\) results in \\(^{d+p} C_d\\) new features. In the above equation of circle , it can be see in that mapping 2 features ( \\(d\\) ) to a polynomial of degree 2 ( \\(p\\) ), results in ( \\(^{2+2} C_2\\) ) 6 features. How does this solve our problem of non linear datapoints? For a dataset \\(S\\) , whose datapoints have a non-linear relationship ( \\(x_i \\in \\mathbb{R}^d\\) ), we can map this dataset to a higher dimension (linear subspace), such that after mapping , \\(x_i \\in \\mathbb{R}^D\\) . Note that \\(D > d\\) As the points (after mapping to higher dimension) are in a linear subspace , our PCA algorithm will work much better than before. Also note that , we already have a solution for the case when dimension of the datapoints is much much larger than the number of datapoints ( \\(d >> n\\) ).","title":"Non-Linear Relationship of Datapoints"},{"location":"kernel_pca/#kernel-functions","text":"Issues with Calculating \\(\\phi(x)\\) To run PCA on non-linear features , we came up with the solution of mapping ( \\(\\phi(x)\\) ) the features to a higher dimension and then instead of calculating eigenvectors for a \\(d \\times d\\) (Covariance) matrix , we calculated eigenvectors for \\(n \\times n\\) ( \\(X^TX\\) ) if \\(d >> n\\) . The issue with current implementation of PCA is that calculating \\(\\phi(x)\\) is nearly the same number of calculations as \\(O(d^p)\\) . This means that as the power of the polynomial increases , the calculation of individual features in the mapping \\(\\phi(x)\\) rises exponentially. Example : If 2 features are mapped to a 20 power polynomial , the resulting number of features will be nearly \\(2^{20}\\) , which is simply too many calculations. To solve this problem we will now take a look at Kernel Functions. The eigendirections we compute (when mapping datapoints to a higher dimension) in PCA is of \\(\\phi(x)^T \\phi(x)\\) . To get to \\(K_{ij}\\) we first need to calculate \\(\\phi(x_i)^T \\phi(x_j)\\) , but is there a way to directly go from \\(x_i \\quad x_j\\) to \\(K_{ij}\\) without computing \\(\\phi(x)\\) ? The function \\((x^T x^{'} + 1)^2\\) is one such function through which we can directly get to \\(\\phi(x)^T \\phi(x^{'})\\) , without having to compute \\(\\phi(x)\\) and \\(\\phi(x^{'})\\) . Lets say, \\(x = [f_1 , f_2] \\quad x^{'} = [g_1 , g_2]\\) \\[\\begin{equation*} \\begin{split} (x^T x^{'} + 1)^2 &= \\left( \\begin{bmatrix}f_1 & f_2 \\end{bmatrix} \\begin{bmatrix} g_1 \\\\ g_2 \\end{bmatrix} + 1 \\right)^2 \\\\ &= (f_1 g_1 + f_2 g_2 + 1)^2 \\\\ &= f_1^2 g_1^2 + f_2^2 g_2^2 + 1 + 2f_1 g_1 f_2 g_2 + 2f_1 g_1 + 2f_2 g_2 \\\\ \\end{split} \\end{equation*}\\] This can also be written as , \\[\\begin{equation*} \\begin{split} (x^T x^{'} + 1)^2 &= \\begin{bmatrix}f_1^2 & f_2^2 & 1 & \\sqrt{2}f_1 f_2 & \\sqrt{2}f_1 & \\sqrt{2}f_2 \\end{bmatrix} \\begin{bmatrix}g_1^2 \\\\ g_2^2 \\\\ 1 \\\\ \\sqrt{2}g_1 g_2 \\\\ \\sqrt{2}g_1 \\\\ \\sqrt{2}g_2 \\end{bmatrix} \\\\ &= \\phi(x)^T \\phi(x^{'}) \\\\ \\\\ \\text{where,}\\\\ \\\\ \\phi(x) &= \\phi \\left( \\begin{bmatrix} a \\\\ b \\end{bmatrix}\\right) = \\begin{bmatrix}a^2 \\\\ b^2 \\\\ 1 \\\\ \\sqrt{2}a b \\\\ \\sqrt{2}a \\\\ \\sqrt{2}b \\end{bmatrix} \\\\\\end{split} \\end{equation*}\\] We can see that , to calculate \\(\\phi(x)^T \\phi(x^{'})\\) we can instead solve for \\((x^T x^{'} + 1)^2\\) and skip the step for calculating \\(\\phi(x)\\) and \\(\\phi(x^{'})\\) .","title":"Kernel Functions"},{"location":"kernel_pca/#valid-kernel-functions","text":"There are 2 ways to check if a function is a valid kernel function, There exists a mapping from \\(\\mathbb{R}^d\\) to \\(\\mathbb{R}\\) such that \\(k(x , x^{'}) = \\phi(x)^T \\phi(x^{'})\\) . A kernel function is considered valid if , \\(k\\) is symmetric , i.e. , \\(k(x , x^{'}) = k(x^{'} , x)\\) The kernel matrix \\(K\\) , where \\(K_{ij} = k(x_i , x_j)\\) , is positive semi-definite. Common Kernel Functions Polynomial Kernel : \\(k(x , x^{'}) = (x^Tx^{'} + 1)^p\\) Gaussian Kernel : \\(k(x , x^{'}) = \\exp \\left( - \\frac{|| x - x^{'}||^2}{2 \\sigma^2} \\right)\\) , for some \\(\\sigma > 0\\) . \\(\\phi(x)\\) in this case is mapped to infinite dimension.","title":"Valid Kernel Functions"},{"location":"kernel_pca/#kenrel-pca","text":"Now we need everything that is required to perform Kernel PCA on a non-linear dataset , lets put down an algorithm for Kernel PCA. Input \\(\\{x_1 , x_2 , \\cdots , x_n \\}\\) \\(x_i \\in \\mathbb{R}^d\\) , kernel \\(k : \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\) Compute \\(K \\in \\mathbb{R}^{n \\times n}\\) where \\(K_{ij} = k(x_i,x_j) \\quad \\forall i,j\\) Center the kernel using, \\[K^C = K - IK - KI + IKI\\] where \\(K^C\\) is the centered kernel and \\(I \\in \\mathbb{R}^{n \\times n}\\) is a matrix with all elements equal to \\(\\frac{1}{n}\\) Compute \\(\\beta_1 , \\beta_2 , \\cdots , \\beta_l\\) eigenvectors and \\(n\\lambda_1 , n\\lambda_2 , \\cdots , n\\lambda_l\\) eigenvectors of K and normalize to get \\(\\alpha_k = \\frac{\\beta_k}{\\sqrt{n \\lambda_k}}\\) Compute the compressed representation using, \\[\\begin{equation*} \\begin{split} \\phi(x_i)^T w_k &= \\phi(x_j)^T \\left( \\sum_{j=1}^n \\phi(x_i) \\alpha_{kj} \\right) \\\\ &= \\sum_{j=1}^n \\alpha_{kj} \\phi(x_i)^T \\phi(x_j) \\\\ &= \\sum_{j=1}^n \\alpha_{kj} K_{ij} \\\\ \\end{split} \\end{equation*}\\] Compute \\(\\sum_{j=1}^n \\alpha_{kj} K_{ij} \\quad \\forall k\\) \\[\\phi(\\mathbf{x}_i)^T\\mathbf{w} \\in \\mathbb{R}^{d} \\to \\left [ \\begin{array}{cccc} \\displaystyle \\sum_{j=1}^{n} \\alpha_{1j} \\mathbf{K}^C_{ij} & \\displaystyle \\sum_{j=1}^{n} \\alpha_{2j} \\mathbf{K}^C_{ij} & \\ldots & \\displaystyle \\sum_{j=1}^{n} \\alpha_{nj} \\mathbf{K}^C_{ij} \\end{array} \\right ]\\]","title":"Kenrel PCA"},{"location":"pca/","text":"Principal Component Analysis (PCA) Introduction Unsupervised learning, specifically \"representation learning,\" is a subset of machine learning where the goal is to automatically discover meaningful representations or features from raw data without explicit supervision or labeled examples. In representation learning, the algorithms aim to capture the underlying structure, patterns, or features within the data itself. This can be highly valuable for various tasks like data compression, feature extraction, data visualization, and even for improving the performance of other machine learning models. Representation Learning (1) The main objective of representation learning is to transform the input data into a more meaningful and compact representation. This representation should capture the essential characteristics of the data, making it easier to analyze or use in downstream tasks. What is the need for compression of data points? Compressing a dataset, especially in the context of unsupervised learning or data preprocessing, can serve several important purposes and provide various benefits , mainly: Reduced Storage Space : Large datasets can require significant storage space. Compressing the dataset reduces storage requirements, which can be cost-effective, especially when dealing with massive datasets in cloud storage or on limited storage devices. Faster Data Transfer : Smaller datasets transfer more quickly over networks, which is crucial when moving data between systems or uploading/downloading data from the internet. Faster Training : When working with machine learning models, smaller datasets can lead to faster training times. This is particularly important when experimenting with different models, hyperparameters, or architectures. How to Compress Data Points? Lets say there are 4 data points \\(\\left\\{ \\stackrel{x_1}{\\begin{bmatrix} -7 \\\\ -14 \\end{bmatrix}} , \\stackrel{x_2}{\\begin{bmatrix} 2.5 \\\\ 5 \\end{bmatrix}} , \\stackrel{x_3}{\\begin{bmatrix} 0.5 \\\\ 1 \\end{bmatrix}} , \\stackrel{x_4}{\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}} \\right\\}\\) Now one might ask , \"how many data points are needed to store this dataset?\" The naive answer would be to say , \"as there are 8 data points , hence 8 real numbers are required to store this dataset.\" A better way to represent this dataset would be to find a \"function\" which takes 1 real number and outputs a matrix of 2 real numbers. If we look at the dataset we can see that the first coordinate is half of the second coordinate. We can exploit this feature of the dataset to reduce the number of data points to be stored. One way to form this \"function\" would be to get a \"representative\" \\(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) and \"coefficients\" \\(\\{ -7 , 2.5 , 0.5 ,1 \\}\\) . Now we can get the back our dataset by multiplying them with the coefficients. Using this way we only need to store 6 real numbers (4 for coefficients + 2 for representative). Similarly, on a dataset of \\(2n\\) points , we can store them as \\(n +2\\) real numbers ( \\(n\\) coefficients + 2 for representative). Working with a Realistic Dataset In the above example we were able construct a function which reduced the number of real numbers required to represent the dataset , but in a realistic dataset all the points do not lie on the same line , they are scattered and its very hard to derive meaningful conclusions from them. \"What to do if all the points dont lie in the dataset?\" A simple answer would be, \"We can get more representatives to accommodate the points which do not lie on the same line.\" This approach does accommodate the outliers but it also increases the number of real numbers required to represent the dataset. \"Whats the solution then?\" There has to be a tradeoff between the \"accuracy\" or the \"size\" of the dataset. If we want to reduce the size of the \"size\" then approximating the outlier to the \"line\" with least lost would be the best bet. To represent outliers and reduce the size of the dataset at the same time , we must project the outlier onto the line. Let there be a vector \\(\\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}\\) which represents the \"line\". The projection of the vector \\(\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\) onto the line would be \\[ \\left[ \\frac{x_1w_1 + x_2w_2}{w_1^2 + w_2^2} \\right] \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\] This will give us a point on the \"line\" with the least loss/distance. Now if we pick vector \\((w_1 , w_2)\\) which lies on the \"line\" such that \\(w_1^2 + w_2^2 = 1\\) , the above equation becomes \\[ \\left[ {x_1w_1 + x_2w_2} \\right] \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\] Representation Learning (2) Our original objective was to find a \"compressed\" representation of the data when all the data-points not necessarily fall on the same line. \"How do we know that the line we picked is the best line?\" One could argue that the same line doesnt fit all the data-points with the \" least loss \" , where \" loss \" is the average of all the \" errors \" when projecting outliers onto the given \" line \". How to find the Best Line? To find the Best Line we should first have a certain way to compare 2 different lines. In our case it would be the line with the least \" reconstruction error \". Lets say for a dataset \\(\\{ x_1 , x_2 , .... x_n \\}\\) , where \\(x_i \\in \\mathbb{R}^d\\) \\[\\begin{equation*} \\begin{split} \\text{Error}(\\text{line} , \\text{dataset}) & = \\sum_{i=1}^n \\text{error}(\\text{line} , x_i) \\\\ & = \\sum_{i=1}^n \\text{length}^2(x - (x^Tw)w) \\\\ & = \\sum_{i=1}^n || x - (x^Tw).w||^2 \\\\ \\end{split} \\end{equation*}\\] To minimize the above equation we can think of it as a function \\(f(W)\\) \\[\\begin{equation*} \\begin{split} f(W) &= \\frac{1}{n}\\sum_{i=1}^{n} || x - (x^Tw).w ||^2 \\\\ &= \\frac{1}{n}\\sum_{i=1}^{n} (x - (x^Tw).w)^T(x - (x^Tw).w) \\\\ &= \\frac{1}{n}\\sum_{i=1}^{n} \\left[ x_i^Tx_i - (x_i^Tw)^2 - (x_i^Tw)^2 + (x_i^Tw)^2(1) \\right] \\\\ &= \\frac{1}{n}\\sum_{i=1}^{n} \\left( x_i^Tx_i - (x_i^Tw)^2 \\right) \\end{split} \\end{equation*}\\] Here we are minimzing \\(f(W)\\) with respect to \\(w\\) , the first term of the above equation \\(x_i^Tx_i\\) can be ignored as its a constant. So we can write the new function as \\(g(W)\\) \\[\\begin{equation*} \\begin{split} \\min_{W_{||w||^2 = 1}} g(W) &= \\frac{1}{n}\\sum_{i=1}^n - (x_i^Tw)^2 \\end{split} \\end{equation*}\\] Alternatively this same function can be written as \\[\\begin{equation*} \\begin{split} \\max_{W_{||w||^2 = 1}} g(W) = \\frac{1}{n}\\sum_{i=1}^n (x_i^Tw)^2 &= \\frac{1}{n} \\sum_{i=1}^{n} (w^Tx_i)(x_i^Tw) \\\\ &= \\frac{1}{n} \\sum_{i=1}^{n} w^T(x_ix_i^T)w \\\\ &= \\frac{1}{n} w^T(\\sum_{i=1}^{n} x_ix_i^T)w \\\\ \\end{split} \\end{equation*}\\] The above equation can also be written as \\[ \\max_{w_{||w||^2 = 1}} w^TCw \\] where \\(C = \\frac{1}{n}\\sum_{i=1}^{n}x_ix_i^T\\) Note that \\(C\\) is also the covariance matrix and the solution for the above maximization equation would be the eigenvector corresponding to the largest eignevalue of \\(C\\) . Error Vector has Information Our hypothesis was that there is a line which best represents the data but , if all the data points lied along a plane then this part (dotted orange lines) which we are imagining to be error may not be the error but rather useful information because the necessary information , the structure is in a plane but not on a line , so the bits we lose while selecting the best line will also contain some information. How do we extract this information? One possible algorithm could be , Input : \\(\\{ x_1 , x_2 , x_3 ...... x_n \\} x_i \\in \\mathbb{R}^d\\) Find the \" best \" line \\(w_1 \\in \\mathbb{R}^d\\) Replace \\(x_i\\) with \\(x_i - (x_i^Tw)w\\) Repeat to obtain \\(w_2\\) Note Sometimes the data might not be centered around the origin. To counter that we can subtract the average of the dataset from the points. Principal Component Analysis (1) From the algorithm above we can find a \\(w_2\\) vector which represents the line which passes through the \"error/residues\" generated while finding out \\(w_1\\) . Observations made while finding out \\(\\mathbf{w_2}\\) All \"residues/errors\" are orthogonal to \\(w_1\\) . Any line which minimizes sum of errors w.r.t. residues must also be orthogonal to \\(w_1\\) A question which comes to mind is , \"is there a relationship between \\(w_1\\) and \\(w_2\\) ?\" The answer is \"Yes , there is a relationship.\" \\(w_1\\) and \\(w_2\\) are orthognal to each other. \\[\\implies w_1^Tw_2 = 0\\] By continuing this procedure for dataset of \\(d\\) dimension, we get a set of vectors \\(\\{ w_1 , w_2 , w_3 .... w_d \\}\\) with the following properties. \\(||w_k||^2 = 1 \\; \\; \\; \\forall k\\) \\(w_iw_j = 0 \\;\\;\\;\\;\\; \\forall i \\neq j\\) Hence , we get a set of Orthonormal Vectors. What is the use of Set \\(D\\) of Orthonormal Vectors? Residue after round 1 is \\(\\left\\{ x_1 - (x_1^Tw_1)w_1 , ..... , x_n - (x_n^Tw_1)w_1 \\right\\} \\;\\;\\;\\; \\forall \\;\\; \\text{vectors} \\in \\mathbb{R}^d\\) Residue after round 2 is \\(\\{ x_1 - (x_1^Tw_1)w_1 - (x_1 - (x_1^Tw_1)w_1)^Tw_2 , ..... \\}\\) \\(\\{ x_1 - (x_1^Tw_1)w_1 - (x_1^Tw_2 - (x_1^Tw_1).w_1^Tw_2)w_2 , ...... \\}\\) \\(\\{ x_1 - (x_1^Tw_1)w_1 - (x_1^Tw_2)w_2 , ...... \\}\\) as \\([w_1^Tw_2 = 0]\\) Residue after \\(d\\) rounds is \\(\\forall i \\;\\;\\;\\;\\;\\; x_i - ((x_i^Tw_1)w_1 + (x_i^Tw_2)w_2 + ...... (x_i^Tw_d)w_d) = 0\\) After \\(d\\) rounds all the error vectors become zero vectors. \\(\\forall i \\;\\;\\;\\;\\;\\; x_i = ((x_i^Tw_1)w_1 + (x_i^Tw_2)w_2 + ...... (x_i^Tw_d)w_d)\\) This leads to the conclusion that if data lives in a \"low\" dimensional linear sub-space, then residue becomes 0 much earlier than \\(d\\) rounds. What does the above statement actually mean? Lets say for a dataset \\(\\{x_1 , x_2 \\cdots , x_n \\}\\) , where \\(x_i \\in \\mathbb{R}^d\\) , all the residues/error vectors become 0 after 3 rounds. This means that every datapoint can be expressed as the sum of projections itself onto the residues/error vectors. \\[ \\forall i \\;\\; x_i = (x_i^Tw_1)w_1 + (x_i^Tw_2)w_2 + (x_i^Tw_3)w_3 \\] where \\(\\{w_1 ,w_2 ,w_3 \\} \\in \\mathbb{R}^d\\) Note that the \\(\\{w_1 ,w_2 ,w_3 \\}\\) are the representatives and \\(\\{x_i^T w_1 , x_i^T w_2 , x_i^T w_3 \\}\\) are the coefficients for a datapoint \\(x_i\\) Example Lets assume for a dataset of dimension \\(d\\) and \\(n\\) points , after \\(k\\) rounds the error vectors become zero vectors. This means that now the dataset can be represented with \\(\\mathbf{d \\times k + k \\times n}\\) points instead of \\(d \\times n\\) points. In the case shown above where the residues/error vectors become 0 after \\(k = 3\\) rounds, we can represent the whole dataset as, \\(d \\times 3\\) + \\(3 \\times n\\) Where , \\(d \\times 3\\) = Total numbers required to store the representatives \\(3 \\times n\\) = Total numbers required to store datapoints Principal Component Analysis (2) Our original problem was \\[ \\max_{w_{||w||^2 = 1}} w^TCw \\] where \\(C = \\frac{1}{n}\\sum_{i=1}^{n}x_ix_i^T\\) , and the solution for this problem was the eigenvector corresponding to the maximum eigenvalue . We also observed that the set of eigenvectors \\((\\{w_1 , w_2 , .... w_d\\})\\) coresspnding to the eigenvalues of \\(C\\) form an orthonormal basis. Now lets look at this problem in a more \"linear algebraic\" way, We know, \\[\\begin{equation*} \\begin{split} Cw_1 &= \\lambda_1 w_1 \\\\ w_1^TCw_1 &= w_1^T(\\lambda_1w_1) = \\lambda_1 \\\\ \\lambda_1 &= w_1^TCw_1 = w_1^T(\\frac{1}{n}\\sum_{i=1}^{n}x_ix_i^T)w_1 \\\\ \\lambda_1 &= \\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw_1)^2 \\end{split} \\end{equation*}\\] Usually we take highest \\(L\\) lambdas such that 95% of the variance in the dataset is captured, \\[\\frac{\\sum_{i=1}^L \\lambda_i }{ \\sum_{i=1}^d \\lambda_i } \\geq 0.95\\] where , \\(\\lambda_i\\) are the eigenvalues of the covariance matrix . Relation between Variance and \\(\\mathbf{\\lambda}\\) For an arbitrary set of points \\((\\{(x_1^Tw) , (x_2^Tw) ...... (x_n^T)w \\})\\) projected onto line represented by vector \\(w\\) . The average \\(\\mu\\) of the projected points will be \\(\\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw)\\) . If the data is centered then, \\[\\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw) = (\\frac{1}{n}\\sum_{i=1}^{n}x_i)w = 0w = 0\\] This implies that average for a centered dataset is \\(\\mu = 0\\) . The variance of this same dataset will be , \\[\\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw - \\mu)^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw)^2\\] We can see that the variance is same as \\(\\lambda\\) required to solve the maximization problem. Hence we can say that , variance maximization is the same as error minimization on centered dataset","title":"Principal Component Analysis (PCA)"},{"location":"pca/#principal-component-analysis-pca","text":"","title":"Principal Component Analysis (PCA)"},{"location":"pca/#introduction","text":"Unsupervised learning, specifically \"representation learning,\" is a subset of machine learning where the goal is to automatically discover meaningful representations or features from raw data without explicit supervision or labeled examples. In representation learning, the algorithms aim to capture the underlying structure, patterns, or features within the data itself. This can be highly valuable for various tasks like data compression, feature extraction, data visualization, and even for improving the performance of other machine learning models.","title":"Introduction"},{"location":"pca/#representation-learning-1","text":"The main objective of representation learning is to transform the input data into a more meaningful and compact representation. This representation should capture the essential characteristics of the data, making it easier to analyze or use in downstream tasks. What is the need for compression of data points? Compressing a dataset, especially in the context of unsupervised learning or data preprocessing, can serve several important purposes and provide various benefits , mainly: Reduced Storage Space : Large datasets can require significant storage space. Compressing the dataset reduces storage requirements, which can be cost-effective, especially when dealing with massive datasets in cloud storage or on limited storage devices. Faster Data Transfer : Smaller datasets transfer more quickly over networks, which is crucial when moving data between systems or uploading/downloading data from the internet. Faster Training : When working with machine learning models, smaller datasets can lead to faster training times. This is particularly important when experimenting with different models, hyperparameters, or architectures. How to Compress Data Points? Lets say there are 4 data points \\(\\left\\{ \\stackrel{x_1}{\\begin{bmatrix} -7 \\\\ -14 \\end{bmatrix}} , \\stackrel{x_2}{\\begin{bmatrix} 2.5 \\\\ 5 \\end{bmatrix}} , \\stackrel{x_3}{\\begin{bmatrix} 0.5 \\\\ 1 \\end{bmatrix}} , \\stackrel{x_4}{\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}} \\right\\}\\) Now one might ask , \"how many data points are needed to store this dataset?\" The naive answer would be to say , \"as there are 8 data points , hence 8 real numbers are required to store this dataset.\" A better way to represent this dataset would be to find a \"function\" which takes 1 real number and outputs a matrix of 2 real numbers. If we look at the dataset we can see that the first coordinate is half of the second coordinate. We can exploit this feature of the dataset to reduce the number of data points to be stored. One way to form this \"function\" would be to get a \"representative\" \\(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) and \"coefficients\" \\(\\{ -7 , 2.5 , 0.5 ,1 \\}\\) . Now we can get the back our dataset by multiplying them with the coefficients. Using this way we only need to store 6 real numbers (4 for coefficients + 2 for representative). Similarly, on a dataset of \\(2n\\) points , we can store them as \\(n +2\\) real numbers ( \\(n\\) coefficients + 2 for representative).","title":"Representation Learning (1)"},{"location":"pca/#working-with-a-realistic-dataset","text":"In the above example we were able construct a function which reduced the number of real numbers required to represent the dataset , but in a realistic dataset all the points do not lie on the same line , they are scattered and its very hard to derive meaningful conclusions from them. \"What to do if all the points dont lie in the dataset?\" A simple answer would be, \"We can get more representatives to accommodate the points which do not lie on the same line.\" This approach does accommodate the outliers but it also increases the number of real numbers required to represent the dataset. \"Whats the solution then?\" There has to be a tradeoff between the \"accuracy\" or the \"size\" of the dataset. If we want to reduce the size of the \"size\" then approximating the outlier to the \"line\" with least lost would be the best bet. To represent outliers and reduce the size of the dataset at the same time , we must project the outlier onto the line. Let there be a vector \\(\\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}\\) which represents the \"line\". The projection of the vector \\(\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\) onto the line would be \\[ \\left[ \\frac{x_1w_1 + x_2w_2}{w_1^2 + w_2^2} \\right] \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\] This will give us a point on the \"line\" with the least loss/distance. Now if we pick vector \\((w_1 , w_2)\\) which lies on the \"line\" such that \\(w_1^2 + w_2^2 = 1\\) , the above equation becomes \\[ \\left[ {x_1w_1 + x_2w_2} \\right] \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\]","title":"Working with a Realistic Dataset"},{"location":"pca/#representation-learning-2","text":"Our original objective was to find a \"compressed\" representation of the data when all the data-points not necessarily fall on the same line. \"How do we know that the line we picked is the best line?\" One could argue that the same line doesnt fit all the data-points with the \" least loss \" , where \" loss \" is the average of all the \" errors \" when projecting outliers onto the given \" line \". How to find the Best Line? To find the Best Line we should first have a certain way to compare 2 different lines. In our case it would be the line with the least \" reconstruction error \". Lets say for a dataset \\(\\{ x_1 , x_2 , .... x_n \\}\\) , where \\(x_i \\in \\mathbb{R}^d\\) \\[\\begin{equation*} \\begin{split} \\text{Error}(\\text{line} , \\text{dataset}) & = \\sum_{i=1}^n \\text{error}(\\text{line} , x_i) \\\\ & = \\sum_{i=1}^n \\text{length}^2(x - (x^Tw)w) \\\\ & = \\sum_{i=1}^n || x - (x^Tw).w||^2 \\\\ \\end{split} \\end{equation*}\\] To minimize the above equation we can think of it as a function \\(f(W)\\) \\[\\begin{equation*} \\begin{split} f(W) &= \\frac{1}{n}\\sum_{i=1}^{n} || x - (x^Tw).w ||^2 \\\\ &= \\frac{1}{n}\\sum_{i=1}^{n} (x - (x^Tw).w)^T(x - (x^Tw).w) \\\\ &= \\frac{1}{n}\\sum_{i=1}^{n} \\left[ x_i^Tx_i - (x_i^Tw)^2 - (x_i^Tw)^2 + (x_i^Tw)^2(1) \\right] \\\\ &= \\frac{1}{n}\\sum_{i=1}^{n} \\left( x_i^Tx_i - (x_i^Tw)^2 \\right) \\end{split} \\end{equation*}\\] Here we are minimzing \\(f(W)\\) with respect to \\(w\\) , the first term of the above equation \\(x_i^Tx_i\\) can be ignored as its a constant. So we can write the new function as \\(g(W)\\) \\[\\begin{equation*} \\begin{split} \\min_{W_{||w||^2 = 1}} g(W) &= \\frac{1}{n}\\sum_{i=1}^n - (x_i^Tw)^2 \\end{split} \\end{equation*}\\] Alternatively this same function can be written as \\[\\begin{equation*} \\begin{split} \\max_{W_{||w||^2 = 1}} g(W) = \\frac{1}{n}\\sum_{i=1}^n (x_i^Tw)^2 &= \\frac{1}{n} \\sum_{i=1}^{n} (w^Tx_i)(x_i^Tw) \\\\ &= \\frac{1}{n} \\sum_{i=1}^{n} w^T(x_ix_i^T)w \\\\ &= \\frac{1}{n} w^T(\\sum_{i=1}^{n} x_ix_i^T)w \\\\ \\end{split} \\end{equation*}\\] The above equation can also be written as \\[ \\max_{w_{||w||^2 = 1}} w^TCw \\] where \\(C = \\frac{1}{n}\\sum_{i=1}^{n}x_ix_i^T\\) Note that \\(C\\) is also the covariance matrix and the solution for the above maximization equation would be the eigenvector corresponding to the largest eignevalue of \\(C\\) .","title":"Representation Learning (2)"},{"location":"pca/#error-vector-has-information","text":"Our hypothesis was that there is a line which best represents the data but , if all the data points lied along a plane then this part (dotted orange lines) which we are imagining to be error may not be the error but rather useful information because the necessary information , the structure is in a plane but not on a line , so the bits we lose while selecting the best line will also contain some information. How do we extract this information? One possible algorithm could be , Input : \\(\\{ x_1 , x_2 , x_3 ...... x_n \\} x_i \\in \\mathbb{R}^d\\) Find the \" best \" line \\(w_1 \\in \\mathbb{R}^d\\) Replace \\(x_i\\) with \\(x_i - (x_i^Tw)w\\) Repeat to obtain \\(w_2\\) Note Sometimes the data might not be centered around the origin. To counter that we can subtract the average of the dataset from the points.","title":"Error Vector has Information"},{"location":"pca/#principal-component-analysis-1","text":"From the algorithm above we can find a \\(w_2\\) vector which represents the line which passes through the \"error/residues\" generated while finding out \\(w_1\\) . Observations made while finding out \\(\\mathbf{w_2}\\) All \"residues/errors\" are orthogonal to \\(w_1\\) . Any line which minimizes sum of errors w.r.t. residues must also be orthogonal to \\(w_1\\) A question which comes to mind is , \"is there a relationship between \\(w_1\\) and \\(w_2\\) ?\" The answer is \"Yes , there is a relationship.\" \\(w_1\\) and \\(w_2\\) are orthognal to each other. \\[\\implies w_1^Tw_2 = 0\\] By continuing this procedure for dataset of \\(d\\) dimension, we get a set of vectors \\(\\{ w_1 , w_2 , w_3 .... w_d \\}\\) with the following properties. \\(||w_k||^2 = 1 \\; \\; \\; \\forall k\\) \\(w_iw_j = 0 \\;\\;\\;\\;\\; \\forall i \\neq j\\) Hence , we get a set of Orthonormal Vectors. What is the use of Set \\(D\\) of Orthonormal Vectors? Residue after round 1 is \\(\\left\\{ x_1 - (x_1^Tw_1)w_1 , ..... , x_n - (x_n^Tw_1)w_1 \\right\\} \\;\\;\\;\\; \\forall \\;\\; \\text{vectors} \\in \\mathbb{R}^d\\) Residue after round 2 is \\(\\{ x_1 - (x_1^Tw_1)w_1 - (x_1 - (x_1^Tw_1)w_1)^Tw_2 , ..... \\}\\) \\(\\{ x_1 - (x_1^Tw_1)w_1 - (x_1^Tw_2 - (x_1^Tw_1).w_1^Tw_2)w_2 , ...... \\}\\) \\(\\{ x_1 - (x_1^Tw_1)w_1 - (x_1^Tw_2)w_2 , ...... \\}\\) as \\([w_1^Tw_2 = 0]\\) Residue after \\(d\\) rounds is \\(\\forall i \\;\\;\\;\\;\\;\\; x_i - ((x_i^Tw_1)w_1 + (x_i^Tw_2)w_2 + ...... (x_i^Tw_d)w_d) = 0\\) After \\(d\\) rounds all the error vectors become zero vectors. \\(\\forall i \\;\\;\\;\\;\\;\\; x_i = ((x_i^Tw_1)w_1 + (x_i^Tw_2)w_2 + ...... (x_i^Tw_d)w_d)\\) This leads to the conclusion that if data lives in a \"low\" dimensional linear sub-space, then residue becomes 0 much earlier than \\(d\\) rounds. What does the above statement actually mean? Lets say for a dataset \\(\\{x_1 , x_2 \\cdots , x_n \\}\\) , where \\(x_i \\in \\mathbb{R}^d\\) , all the residues/error vectors become 0 after 3 rounds. This means that every datapoint can be expressed as the sum of projections itself onto the residues/error vectors. \\[ \\forall i \\;\\; x_i = (x_i^Tw_1)w_1 + (x_i^Tw_2)w_2 + (x_i^Tw_3)w_3 \\] where \\(\\{w_1 ,w_2 ,w_3 \\} \\in \\mathbb{R}^d\\) Note that the \\(\\{w_1 ,w_2 ,w_3 \\}\\) are the representatives and \\(\\{x_i^T w_1 , x_i^T w_2 , x_i^T w_3 \\}\\) are the coefficients for a datapoint \\(x_i\\) Example Lets assume for a dataset of dimension \\(d\\) and \\(n\\) points , after \\(k\\) rounds the error vectors become zero vectors. This means that now the dataset can be represented with \\(\\mathbf{d \\times k + k \\times n}\\) points instead of \\(d \\times n\\) points. In the case shown above where the residues/error vectors become 0 after \\(k = 3\\) rounds, we can represent the whole dataset as, \\(d \\times 3\\) + \\(3 \\times n\\) Where , \\(d \\times 3\\) = Total numbers required to store the representatives \\(3 \\times n\\) = Total numbers required to store datapoints","title":"Principal Component Analysis (1)"},{"location":"pca/#principal-component-analysis-2","text":"Our original problem was \\[ \\max_{w_{||w||^2 = 1}} w^TCw \\] where \\(C = \\frac{1}{n}\\sum_{i=1}^{n}x_ix_i^T\\) , and the solution for this problem was the eigenvector corresponding to the maximum eigenvalue . We also observed that the set of eigenvectors \\((\\{w_1 , w_2 , .... w_d\\})\\) coresspnding to the eigenvalues of \\(C\\) form an orthonormal basis. Now lets look at this problem in a more \"linear algebraic\" way, We know, \\[\\begin{equation*} \\begin{split} Cw_1 &= \\lambda_1 w_1 \\\\ w_1^TCw_1 &= w_1^T(\\lambda_1w_1) = \\lambda_1 \\\\ \\lambda_1 &= w_1^TCw_1 = w_1^T(\\frac{1}{n}\\sum_{i=1}^{n}x_ix_i^T)w_1 \\\\ \\lambda_1 &= \\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw_1)^2 \\end{split} \\end{equation*}\\] Usually we take highest \\(L\\) lambdas such that 95% of the variance in the dataset is captured, \\[\\frac{\\sum_{i=1}^L \\lambda_i }{ \\sum_{i=1}^d \\lambda_i } \\geq 0.95\\] where , \\(\\lambda_i\\) are the eigenvalues of the covariance matrix .","title":"Principal Component Analysis (2)"},{"location":"pca/#relation-between-variance-and-mathbflambda","text":"For an arbitrary set of points \\((\\{(x_1^Tw) , (x_2^Tw) ...... (x_n^T)w \\})\\) projected onto line represented by vector \\(w\\) . The average \\(\\mu\\) of the projected points will be \\(\\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw)\\) . If the data is centered then, \\[\\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw) = (\\frac{1}{n}\\sum_{i=1}^{n}x_i)w = 0w = 0\\] This implies that average for a centered dataset is \\(\\mu = 0\\) . The variance of this same dataset will be , \\[\\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw - \\mu)^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw)^2\\] We can see that the variance is same as \\(\\lambda\\) required to solve the maximization problem. Hence we can say that , variance maximization is the same as error minimization on centered dataset","title":"Relation between Variance and \\(\\mathbf{\\lambda}\\)"},{"location":"perceptron_learning/","text":"Logistic Regression Introduction Our goal in this week is to discover discriminative models which can be used for classification. We want to create a discriminative model for \\(P(y=1|x)\\) and the simplest model that one can think of is a linear classification model. \\[\\begin{equation*} P(y=1|x) = \\begin{cases} 1 & \\text{for } w^Tx \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases} \\end{equation*}\\] In generative model we looked at how \\(x\\) was generated , but for a discriminative model we only care about how \\(y|x\\) is generated. Linear Separatability Assumption For our above linear model to be able to classify datapoints , the datapoints must have either label 1 or label -1. This means there should be no outliers in a dataset and all the points should belong to either side of \\(w^Tx = 0\\) (Linear Separator). Here the data is linearly separable , there are no outliers in this dataset. This dataset is not allowed because there are outliers and hence the dataset is not linearly spearabale. If the \"Not Allowed Dataset\" is given to us then our assumption would be that the labeler used some \\(w\\) which correctly classified all the datapoints , but according to our current model such datasets are not possible. Hence we say that this dataset is not allowed under our model. When we make strong assumptions like linear separatability of the dataset, we hope to build fast and efficient algorithms, but do such algorithms really exist? The short answer is Yes. Our goal here was to get a discriminative model for classification which minimizes the zero-one loss over a dataset. \\[\\underset{h \\in \\mathcal{H}}{\\min} \\sum_{i=1}^n \\mathbb{1}( h(x_i) \\neq y_i )\\] For a general dataset this is an NP-HARD problem even if \\(\\mathcal{H}\\) is considered to be linear. Now if we get back to our \"Linear Separatability Assumption\" , then the loss for our algorithm will be 0 (on the training dataset) as it will be able to correctly classify all the datapoints because there are no outliers present. \\[\\exists w \\in \\mathbb{R}^d \\text{ s.t. } \\text{sign}(w^Tx) =y_i \\forall i \\in [n]\\] There exists a \\(w\\) such that sign( \\(w^Tx\\) ) = \\(y_i\\) (we make the correct prediction) for all the \\(i\\) (datapoints) in \\([n]\\) (dataset) Perceptron Algorithm The Input for this algorithm is \\(\\{ (x_1,y_1) , (x_2,y_2) , ... (x_n,y_n) \\}\\) where \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{+1,-1 \\}\\) . The algorithm is trying to find a \\(w\\) that correctly classifies all the datapoints, if such a \\(w\\) exists. This algorithm is an iterative algorithm and it starts with a \\(w^0\\) , where \\(0\\) indicates the iteration number and initially \\(w^0 = [0,0,0,...0]\\) i.e. \\(w^0\\) is a zero vector. Until Convergence \\[\\begin{align} & \\text{Pick } (x_i, y_i) \\text{ pair from the dataset}\\\\ & \\text{If sign}(w^Tx_i) = y_i \\\\ \\\\ & \\quad \\text{Do nothing} \\\\ \\\\ & \\text{else}\\\\ \\\\ & \\quad \\boxed{w^{t+1} = w^t + x_i y_i} \\\\ \\\\ & \\text{end} \\\\ \\end{align}\\] Basically , we check if our current \\(w\\) predicts the datapoint correctly , if it doesnt predict the datapoint correctly then we multiply the datapoint with its label (+1 or -1) and add this product to our current \\(w\\) until convergence. Also note that the update rule here is the boxed equation, \\[ \\boxed{w^{t+1} = w^t + x_i y_i} \\] Understanding Perceptron Update Rule In our current perceptron algorithm , two types of mistakes can happen Mistake Type 1 Predicted Label = +1 (sign \\((w^Tx_i) \\geq 0\\) ) Actual Label = -1 ( \\(y_i\\) = -1) Mistake Type 2 Predicted Label = -1 (sign \\((w^Tx_i) < 0\\) ) Actual Label = +1 ( \\(y_i\\) = +1) When we encounter a mistake , we either make a mistake in prediction of Type 1 Category or Type 2 Category and then we update \\(w\\) accordingly. A general question to ask here would be , we have updated our \\(w\\) on some datapoint at \\(t^\\text{th}\\) iteration, but how does this \\(w^{t+1}\\) ( \\(w^t\\) after update) perform on the point where we made the mistake? We know that, \\[\\begin{equation*} \\begin{split} w^{t+1} &= w^t + x_i y_i \\\\ \\\\ \\text{Multiplying both sides by } x_i \\\\ \\\\ (w^{t+1})^T x_i &= (w^t + x_i y_i)^T x_i \\\\ &= w^{t^T} x_i + y_i ||x_i||^2 \\\\ \\end{split} \\end{equation*}\\] Lets assume that a mistake of Type 1 occurs. \\[ (w^{t+1})^T x_i = \\underbrace{w^{t^T} x_i}_{\\geq 0} + \\underbrace{\\underbrace{y_i}_{-1} \\underbrace{||x_i||^2}_{\\geq 0}}_{\\text{Negative}}\\] Here \\(y_i = -1\\) represents the \"actual\" label of \\(x_i\\) , \\(||x_i||^2\\) will always be positive because it is squared. The product of \\(y_i\\) and \\(||x_i||^2\\) will be less than zero (negative). Now what does all this mean? In a Type 1 Mistake , we predicted the label to be positive (+1) because our \\(w^T x_i \\geq 0\\) , but it should have been negative (as the actual label is -1). We can see that the product of \\(y_i\\) and \\(||x_i||^2\\) will be negative and will get subtracted from \\(w^{t^T} x_i\\) which will shift the \\(w\\) towards the negative direction. This doesnt mean that \\(w\\) will immediately give a negative dot product just after this iteration (where we made the Type 1 Mistake) but it does moves/shifts the \\(w\\) to the correct direction. Conclusion : Update rule pushes \\(w\\) in the right direction. The update of \\(w\\) we discussed fixes the prediction for the \"current\" datapoint, but does it affect the prediction of previous datapoints (which was predicted correctly )? In on overall sense , does our current algorithm give use the best \\(w\\) ? Redifining Linear Separatability Case 1 Here we can see that updating \\(w\\) for a point \\(x\\) leads to misclassification of some datapoints which were correctly classified before. Animation It can be seen that at the end when the new weight vector (yellow) is created , it misclassifies the circled points , which were correctly classified by the old weight vector (white). Case 2 Is this data linearly separable? At first glance this dataset might look linearly separable , but in according to our current algorithm this dataset is not linearly separable . How? Why? One might say that \\(w\\) lying on x-axis (below diagram) separates the dataset. Now lets see how the perceptron algorithm work on this dataset. Iteration 1 Initially \\(w^0 = [0,0]\\) \\(\\large{\\substack{w^{0^T}x_1 = 0 \\\\ \\hat{y} = +1}, \\;\\; \\substack{w^{0^T}x_2 = 0 \\\\ \\hat{y} = +1}, \\;\\; \\boxed{\\substack{w^{0^T}x_3 = 0 \\\\ \\hat{y} = +1}}}\\) Iteration 2 Our algorithm made a mistake in prediciton of the third (boxed) datapoint. Now we will update \\(w\\) as a mistake in prediciton occured, \\(w^1 = w^0 + x_3 y_3 = \\begin{bmatrix} 1 \\\\ -1/2 \\end{bmatrix}\\) After updating \\(w\\) we again run the algorithm, \\(\\large{\\boxed{\\substack{w^{1^T}x_1 = -0.5 \\\\ \\hat{y} = -1}}, \\;\\; \\substack{w^{1^T}x_2 = 0.5 \\\\ \\hat{y} = +1}, \\;\\; \\substack{w^{1^T}x_3 = -1.25 \\\\ \\hat{y} = -1}}\\) Iteration 3 Our algorithm made a mistake in prediciton of the third (boxed) datapoint. Now we will update \\(w\\) as a mistake in prediciton occured, \\(w^2 = w^1 + x_1 y_1 = \\begin{bmatrix} 1 \\\\ 1/2 \\end{bmatrix}\\) \\(\\large{\\substack{w^{1^T}x_1 = 0.5 \\\\ \\hat{y} = +1}, \\;\\; \\boxed{\\substack{w^{1^T}x_2 = -0.5 \\\\ \\hat{y} = -1}}, \\;\\; \\substack{w^{1^T}x_3 = -0.75 \\\\ \\hat{y} = -1}}\\) Iteration 4 Our algorithm made a mistake in prediciton of the second (boxed) datapoint. Now we will update \\(w\\) as a mistake in prediciton occured, \\(w^3 = w^0 + x_2 y_2 = \\begin{bmatrix} 1 \\\\ -1/2 \\end{bmatrix}\\) We can see that this \\(w^3\\) is the same as \\(w^1\\) , which means if we go ahead with this iteration we run into again predict the second datapoint wrong and hence the loop will keep on running without stopping. At the end , our perceptron algorithm will never give us a \\(w\\) which classifies all the points correctly and the algorithm will never converge. In such an edge case , there exists no \\(w\\) which will correctly classify all the datapoints , even though , at first the dataset may look linearly separable Animation In each iteration the incorrectly predicted point is highlighted in yellow color. Points on the right side of the Decision Boundry are predicted as +1 (positive), while points on the left side are predicted as -1 (negative) Even after several iterations it can be seen that the weight vector keeps going back and forth between w=[1,0.5] and w=[1,-0.5] . Hence we can see that the algorithm will never converge. Why does this happen? One of the reasons is , the given dataset is not strictly linearly separable as some of the datapoints lie on the decision boundry itself and we chose to label such datapoints as +1 , though they can also be labeled as -1. What to do to solve this issue? We can apply more stricter \"assumptions\" on our dataset to account for the edge case described above. A dataset is linearly separable with \\(\\gamma\\) margin if, \\(\\exists w^* \\in \\mathbb{R}^d \\quad \\text{s.t. } \\quad (w^{*^T}x_i)y_i \\geq \\gamma \\forall i \\quad \\text{for some } \\gamma > 0\\) (There exists a \\(w^*\\) such that \\((w^{*^T}x_i) y_i \\geq \\gamma\\) for all the datapoints where \\(\\gamma > 0\\) ) This assumption makes it so that there are no datapoints between the parallel dotted lines , in other words , this also means that no points will lie on \\(w^T x = 0\\) Proof of Convergence of Perceptron Algorithm To prove the convergence of the algorithm we are going to make a few assumptions about the dataset Linear Separatability with \\(\\gamma\\) margin. Radius Assumption \\(\\forall i \\in D \\quad ||x_i||_2 \\leq R \\quad \\text{for some } R > 0\\) This basically means that all the points in dataset \\(D\\) fall within a circle of radius \\(R\\) . Without loss of generality , assume \\(||w^*|| = 1\\) This basically means that we have normalized our \\(w\\) get \\(w^*\\) . We will now try to quantify the number of mistakes our aglorithm can make , if the number of mistakes is finite then it means the number of iterations is also finite , therefore , our algorithm must converge. Analysis of 'Mistakes' of Perceptron Algorithm Observe that an update in perceptron algorithm only happens when a mistake occurs. Say \\(w^l\\) is the current guess and a mistake happens w.r.t (x,y). Bound 1 Now we will take look at what happens to the length of \\(w\\) after an update \\[\\begin{equation*} \\begin{split} w^l &= w^{l-1} + xy \\\\ ||w^l||^2 &= ||w^{l-1} + xy||^2 \\\\ &= (w^{l-1} + xy)^T (w^{l-1} + xy) \\\\ &= \\underbrace{||w^{l-1}||^2}_{\\geq 0} + \\underbrace{2(w^{{l-1}^T} x)y}_{\\leq 0} + \\underbrace{||x||^2 \\underbrace{y^2}_{\\pm1}}_{\\leq R^2} \\\\ \\\\ \\therefore ||w^l||^2 &\\leq ||w^{l-1}||^2 + R^2 \\\\ &\\leq (||w^{l-2}||^2 +R^2 ) + R^2 \\\\ &\\vdots \\\\ &\\leq ||w^0||^2 + l R^2 \\\\ \\\\ \\boxed{\\therefore ||w^l||^2 \\leq l R^2} \\\\ \\end{split} \\end{equation*}\\] Note The reason why \\(y^2\\) is always greater than zero because it is this label of of the datapoints and can only take value of either +1 or -1. ||w||^2 will also be always greater than or equal to zero as square of any real number is always a positive number ( \\(w\\) is just a vector comprising of real numbers) Product of \\(||x||^2\\) and \\(y^2\\) will always be less than or equal to \\(R^2\\) because as per our assumption , all the points lie within a circl of radius \\(R\\) . \\(2(w^{{l-1}^T} x) y\\) will always be less than one because an update only happens when a mistake occurs, If actual label is +1 and predicted label is -1 , this implies \\(w^T x < 0\\) i.e. its a negative value and the (actual) label is a positive value , product of negative and positive values is always negative. Similarly , if actual label is -1 and predicted label is +1, this implies \\(w^T x \\geq 0\\) i.e its a positive value and the (actual) label is a negative value , product of positive and negative values is always negative. Bound 2 We will now use \\(w^*\\) (the best \\(w\\) which exists) to obtain another bound for number of mistakes. \\[\\begin{equation*} \\begin{split} w^l &= w^{l-1} + x y \\\\ (w^l)^T w^* &= (w^{l-1} + xy)^T w^* \\\\ &= w^{{l-1}^T} w^* + \\underbrace{(w^{*^T} x)y}_{\\geq \\gamma} \\\\ \\\\ \\therefore (w^l)^T w^* &\\geq (w^{l-1})^T w^* + \\gamma \\\\ & \\geq (w^{{l-2}^T} w^* + \\gamma) + \\gamma \\\\ & \\vdots \\\\ & \\geq \\underbrace{(w^0)^T w^*}_{0} + l \\gamma \\\\ \\\\ \\therefore (w^l)^T w^* &\\geq l \\gamma \\\\ \\\\ ((w^l)^T w^*)^2 &\\geq l^2 \\gamma^2 \\\\ \\\\ ||w^l||^2 \\underbrace{||w^*||^2}_{1} &\\geq l^2 \\gamma^2 \\text{ Using Cauchy-Schwarz Inequality } \\\\ \\boxed {\\therefore ||w^l||^2 \\geq l^2 \\gamma^2} \\end{split} \\end{equation*}\\] Cauchy-Schwarz Inequaltiy We know that, \\[ -1 \\leq \\cos(\\theta) \\leq 1 \\] Multiplying the product of norm of some vectors \\(v\\) and \\(w\\) in the above equation, \\[\\begin{equation*} \\begin{split} -||v|| \\times ||w|| \\leq ||v|| &\\times ||w|| \\cos(\\theta) \\leq ||v|| \\times ||w|| \\\\ -||v|| \\times ||w|| \\leq v &\\cdot w \\leq ||v|| \\times ||w|| \\\\ |v \\cdot w| &\\leq ||v|| \\times ||w|| \\end{split} \\end{equation*}\\] Note : Dot product of any 2 vectors is given by \\(x \\cdot y = ||x|| \\times ||y|| \\cos(\\theta)\\) Note In the above assumptions we said that norm of \\(w^*\\) is 1. Radius Margin Bound Now that we have both upper and lower bound of \\(||w^l||\\) , \\[\\begin{equation*} \\begin{split} l^2 \\gamma^2 &\\leq ||w^l||^2 \\leq l R^2 \\\\ l^2 \\gamma^2 &\\leq l R^2 \\\\ \\therefore l &\\leq \\frac{R^2}{\\gamma^2}\\\\ \\end{split} \\end{equation*}\\] Conclusion : A dataset which follows all the assumptions above , its mistakes are always less than or equal to Radius in which all the datapoints lie divided by the margin gamma with respect to the optimal \\(w^*\\) . As the number of mistakes now can be quantified , we can say that this perceptron algorithm will converge. Sigmoid Function for Modeling Class Probabilities We know that Perceptron Algorithm makes a linear separatability assumption , if we were to write that in probabilistic manner , then one way to do that would be, \\[\\begin{equation*} P(y=1|x) = \\begin{cases} 1 & \\text{if } w^T x \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases} \\end{equation*}\\] As the probabilities of the are 1 and 0 for a given datapoint , the perceptron algorithm only works on linearly separable datasets. If the dataset has points which are not linearly separable but appear to be outliers, our perceptron algorithm is unable to run on such a dataset. Can we somehow relax the probabilities for a given datapoint? So that the perceptron algorithm also works on datasets which are not linearly separable. Simple Linear Probabilistic Model We will now start building up to a reasonable algorithm which has realaxed probabilities. We will start with a simple linear model where the score ( \\(z\\) ) of a datapoint is given by \\(z = w^T x\\) . We are going to allow any point to be labeled as +1 or -1 , but the deciding factor for which label will +1 or -1 will be based on the score ( \\(z\\) ) of the datapoint. The higher the score of a label is , the higher will be its probability of being labeled as +1 , similarly the lower the score of the datapoint is , the higher will be its probability of being labeled as -1. An intuitive way to think about this would be that , \\(x_2\\) is farther away from the decision boundry/linear separator , hence we're more confident that \\(x_2\\) should be labeled as +1 when compared to \\(x_1\\) which is much much closer to the decision boundry. Example In the above diagram , \\(x_1\\) will have a lower score when compared to \\(x_2\\) ( \\(w^T x_1 < w^T x_2\\) ), this means that probability of \\(x_1\\) being labeled as +1 is lower than probability of \\(x_2\\) being labeled as +1 ( \\(P(y = 1 | x_1) < P(y = 1 | x_2)\\) ). Similarly , \\(x_4\\) is farther away than \\(x_3\\) , which means that \\(x_4\\) has higher probability of being labeled as -1 when compared to \\(x_3\\) ( \\(P(y = -1 | x_3) < P(y = -1 | x_4)\\) ). According to this simple linear model , every point has the probability to be labeled as +1 or -1 , just the chance that it gets labeled (+1 or -1) depends on how far it is form the decision boundry. Problems With Current Linear Model We can calculate the score for any datapoint given a \\(w\\) but we dont have function/method to convert this score to a probability. To convert the score to a probability will use the Sigmoid Function. This function goes from \\(0 \\to 1\\) over the domain of \\((-\\infty , \\infty)\\) and is given by \\(g(z) = \\frac{1}{1 + e^{-z}}\\) . Here , \\(z = w^T x\\) . Hence, with sigmoid function we define probabilities of a datapoint being labeled as +1 or -1 which means datasets which were previously not allowed are now allowed for our Probabilistic Perceptron Algorithm. Logistic Regression We have developed Probabilistic Perceptron Algorithm , which labels datapoints on certain score (z) , but the problem with our current approach is that there may exist many \\(w\\) which correctly classify the datapoints. We dont have any method to identify the best \\(w\\) among all the \\(w\\) 's which classify the datapoints. Here \\(w_1\\) and \\(w_2\\) both will be able to classify the datapoints , but which amongst them is a better \\(w\\) ? This seems to be problem of maximum liklihood which can be solved using using the traditional method of derivative of log-likelihood. For a dataset \\(D = \\{(x_1 , y_1) , (x_2 , y_2) , ... (x_n , y_n) \\}\\) , where \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{-1 , 1 \\}\\) We know that, \\[ P(y=1|x) = g(w^T x_i) = \\frac{1}{1 + e^{- w^T x}} \\] The maximum likelihood expression will be, \\[\\begin{equation*} \\begin{split} \\mathcal{L}(w;D) &= \\prod_{i=1}^n (g(w^T x_i))^{y_i} (1 - g(w^T x_i))^{1 - y_i} \\\\ \\log( \\mathcal{L}(w;D) ) &= \\sum_{i=1}^{n} y_i \\log(g(w^T x_i)) + (1 - y_i) \\log(1 - g(w^T x_i)) \\\\ &= \\sum_{i=1}^{n} y_i \\log (\\frac{1}{1 + e^{-w^T x_i}}) + (1 - y_i) \\log (\\frac{e^{-w^T x_i}}{1 + w^{-w^T x_i}}) \\\\ &= \\sum_{i=1}^{n} [(1 - y_i)(-w^T x_i) -log(1 + e^{-w^T x_i})] \\\\ \\end{split} \\end{equation*}\\] Note Our assumption here is the probabilities are generated independent of other labels. The above Maximum Likelihood is very similar to Maximal Likelihood of a bernoulli random variable. Our goal now is maximize the above equation with respect to \\(w\\) , so that we get the best possible \\(w\\) , \\[ \\underset{w}{\\max} \\sum_{i=1}^{n} [(1 - y_i)(- w^T x_i) - \\log(1 + e^{- w^T x_i})] \\] However , the above equation doesnt have a closed-form solution when a derivative is taken to solve further. Therefore, we will use gradient descent to identify the best \\(w\\) . The gradient descent of the above log-likelihood function will be, \\[\\begin{equation*} \\begin{split} \\nabla \\log (\\mathcal{L}(w;D)) &= \\sum_{i=1}^{n} \\left[(1 - y_i)(-x_i) - \\frac{e^{-w^T x_i}}{1 + e^{-w^Tx)i}} (-x_i) \\right] \\\\ &= \\sum_{i=1}^{n} \\left[ -x_i + x_i y_i + x_i \\left( \\frac{e^{-w^T x_i}}{1 + e^{-w^Tx_i}} \\right) \\right] \\\\ &= \\sum_{i=1}^{n} \\left[ x_iy_i - x_i \\left( \\frac{1}{1 + e^{-w^T x_i}} \\right) \\right] \\\\ \\nabla \\log(\\mathcal{L}(w;D)) &= \\sum_{i=1}^{n} \\left[ x_i \\left(y_i - \\frac{1}{1 + e^{-w^T x_i}} \\right) \\right] \\end{split} \\end{equation*}\\] Using the Gradient Descent formula we get, \\[\\begin{equation*} \\begin{split} w_{t+1} &= w_t + \\mathcal{n}_t \\nabla \\log ( \\mathcal{L}(w;D))\\\\ &= w_t + \\mathcal{n}_t \\left( \\sum_{i=1}^n x_i \\left(\\overbrace{y_i - \\underbrace{\\frac{1}{1 + e^{-w^Tx_i}}}_{g(w^T x_i)}}^{\\theta_i} \\right) \\right) \\end{split} \\end{equation*}\\] Note The term after \\(y_i\\) is the Sigmoid Function which outputs a probability for a certain score of a datapoint. Lets say for a point the actual label is +1 and it is predicted correctly as +1 ( \\(y_i = 1\\) ) , this means that the point has a higher probability (lets say \\(g(w^Tx_i) = 0.9\\) ) when compared to the other binary label, which was derived from the sigmoid function. This also means that \\(\\theta_i\\) will be a small value as \\(1 - 0.9 = 0.1\\) . This also means that this particular datapoint will not have much effect on the direction of the gradient descent algorithm. Prediction of a datapoint \\(x_\\text{test}\\) is given by , \\[ y_\\text{test} = \\text{sign}( \\hat{w}^T x_\\text{test} ) \\] Advantages of Logistic Regression There is a kernel version for the above equation as it can be argued that \\(w = \\sum_{i=1}^n \\alpha_i x_i\\) Regularized Version for the above equation is , \\[\\underset{w}{\\min} \\sum_{i=1}^n \\left[ \\log(1 + e^{-w^T x_i}) + w^T x_i (1-y_i) \\right] + \\underbrace{\\frac{\\lambda}{2} ||w||^2}_{\\text{Regularizer}} \\] Here \\(\\lambda\\) is Cross Validated Hyperparameter.","title":"Logistic Regression"},{"location":"perceptron_learning/#logistic-regression","text":"","title":"Logistic Regression"},{"location":"perceptron_learning/#introduction","text":"Our goal in this week is to discover discriminative models which can be used for classification. We want to create a discriminative model for \\(P(y=1|x)\\) and the simplest model that one can think of is a linear classification model. \\[\\begin{equation*} P(y=1|x) = \\begin{cases} 1 & \\text{for } w^Tx \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases} \\end{equation*}\\] In generative model we looked at how \\(x\\) was generated , but for a discriminative model we only care about how \\(y|x\\) is generated.","title":"Introduction"},{"location":"perceptron_learning/#linear-separatability-assumption","text":"For our above linear model to be able to classify datapoints , the datapoints must have either label 1 or label -1. This means there should be no outliers in a dataset and all the points should belong to either side of \\(w^Tx = 0\\) (Linear Separator). Here the data is linearly separable , there are no outliers in this dataset. This dataset is not allowed because there are outliers and hence the dataset is not linearly spearabale. If the \"Not Allowed Dataset\" is given to us then our assumption would be that the labeler used some \\(w\\) which correctly classified all the datapoints , but according to our current model such datasets are not possible. Hence we say that this dataset is not allowed under our model. When we make strong assumptions like linear separatability of the dataset, we hope to build fast and efficient algorithms, but do such algorithms really exist? The short answer is Yes. Our goal here was to get a discriminative model for classification which minimizes the zero-one loss over a dataset. \\[\\underset{h \\in \\mathcal{H}}{\\min} \\sum_{i=1}^n \\mathbb{1}( h(x_i) \\neq y_i )\\] For a general dataset this is an NP-HARD problem even if \\(\\mathcal{H}\\) is considered to be linear. Now if we get back to our \"Linear Separatability Assumption\" , then the loss for our algorithm will be 0 (on the training dataset) as it will be able to correctly classify all the datapoints because there are no outliers present. \\[\\exists w \\in \\mathbb{R}^d \\text{ s.t. } \\text{sign}(w^Tx) =y_i \\forall i \\in [n]\\] There exists a \\(w\\) such that sign( \\(w^Tx\\) ) = \\(y_i\\) (we make the correct prediction) for all the \\(i\\) (datapoints) in \\([n]\\) (dataset)","title":"Linear Separatability Assumption"},{"location":"perceptron_learning/#perceptron-algorithm","text":"The Input for this algorithm is \\(\\{ (x_1,y_1) , (x_2,y_2) , ... (x_n,y_n) \\}\\) where \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{+1,-1 \\}\\) . The algorithm is trying to find a \\(w\\) that correctly classifies all the datapoints, if such a \\(w\\) exists. This algorithm is an iterative algorithm and it starts with a \\(w^0\\) , where \\(0\\) indicates the iteration number and initially \\(w^0 = [0,0,0,...0]\\) i.e. \\(w^0\\) is a zero vector. Until Convergence \\[\\begin{align} & \\text{Pick } (x_i, y_i) \\text{ pair from the dataset}\\\\ & \\text{If sign}(w^Tx_i) = y_i \\\\ \\\\ & \\quad \\text{Do nothing} \\\\ \\\\ & \\text{else}\\\\ \\\\ & \\quad \\boxed{w^{t+1} = w^t + x_i y_i} \\\\ \\\\ & \\text{end} \\\\ \\end{align}\\] Basically , we check if our current \\(w\\) predicts the datapoint correctly , if it doesnt predict the datapoint correctly then we multiply the datapoint with its label (+1 or -1) and add this product to our current \\(w\\) until convergence. Also note that the update rule here is the boxed equation, \\[ \\boxed{w^{t+1} = w^t + x_i y_i} \\]","title":"Perceptron Algorithm"},{"location":"perceptron_learning/#understanding-perceptron-update-rule","text":"In our current perceptron algorithm , two types of mistakes can happen Mistake Type 1 Predicted Label = +1 (sign \\((w^Tx_i) \\geq 0\\) ) Actual Label = -1 ( \\(y_i\\) = -1) Mistake Type 2 Predicted Label = -1 (sign \\((w^Tx_i) < 0\\) ) Actual Label = +1 ( \\(y_i\\) = +1) When we encounter a mistake , we either make a mistake in prediction of Type 1 Category or Type 2 Category and then we update \\(w\\) accordingly. A general question to ask here would be , we have updated our \\(w\\) on some datapoint at \\(t^\\text{th}\\) iteration, but how does this \\(w^{t+1}\\) ( \\(w^t\\) after update) perform on the point where we made the mistake? We know that, \\[\\begin{equation*} \\begin{split} w^{t+1} &= w^t + x_i y_i \\\\ \\\\ \\text{Multiplying both sides by } x_i \\\\ \\\\ (w^{t+1})^T x_i &= (w^t + x_i y_i)^T x_i \\\\ &= w^{t^T} x_i + y_i ||x_i||^2 \\\\ \\end{split} \\end{equation*}\\] Lets assume that a mistake of Type 1 occurs. \\[ (w^{t+1})^T x_i = \\underbrace{w^{t^T} x_i}_{\\geq 0} + \\underbrace{\\underbrace{y_i}_{-1} \\underbrace{||x_i||^2}_{\\geq 0}}_{\\text{Negative}}\\] Here \\(y_i = -1\\) represents the \"actual\" label of \\(x_i\\) , \\(||x_i||^2\\) will always be positive because it is squared. The product of \\(y_i\\) and \\(||x_i||^2\\) will be less than zero (negative). Now what does all this mean? In a Type 1 Mistake , we predicted the label to be positive (+1) because our \\(w^T x_i \\geq 0\\) , but it should have been negative (as the actual label is -1). We can see that the product of \\(y_i\\) and \\(||x_i||^2\\) will be negative and will get subtracted from \\(w^{t^T} x_i\\) which will shift the \\(w\\) towards the negative direction. This doesnt mean that \\(w\\) will immediately give a negative dot product just after this iteration (where we made the Type 1 Mistake) but it does moves/shifts the \\(w\\) to the correct direction. Conclusion : Update rule pushes \\(w\\) in the right direction. The update of \\(w\\) we discussed fixes the prediction for the \"current\" datapoint, but does it affect the prediction of previous datapoints (which was predicted correctly )? In on overall sense , does our current algorithm give use the best \\(w\\) ?","title":"Understanding Perceptron Update Rule"},{"location":"perceptron_learning/#redifining-linear-separatability","text":"Case 1 Here we can see that updating \\(w\\) for a point \\(x\\) leads to misclassification of some datapoints which were correctly classified before. Animation It can be seen that at the end when the new weight vector (yellow) is created , it misclassifies the circled points , which were correctly classified by the old weight vector (white). Case 2 Is this data linearly separable? At first glance this dataset might look linearly separable , but in according to our current algorithm this dataset is not linearly separable . How? Why? One might say that \\(w\\) lying on x-axis (below diagram) separates the dataset. Now lets see how the perceptron algorithm work on this dataset. Iteration 1 Initially \\(w^0 = [0,0]\\) \\(\\large{\\substack{w^{0^T}x_1 = 0 \\\\ \\hat{y} = +1}, \\;\\; \\substack{w^{0^T}x_2 = 0 \\\\ \\hat{y} = +1}, \\;\\; \\boxed{\\substack{w^{0^T}x_3 = 0 \\\\ \\hat{y} = +1}}}\\) Iteration 2 Our algorithm made a mistake in prediciton of the third (boxed) datapoint. Now we will update \\(w\\) as a mistake in prediciton occured, \\(w^1 = w^0 + x_3 y_3 = \\begin{bmatrix} 1 \\\\ -1/2 \\end{bmatrix}\\) After updating \\(w\\) we again run the algorithm, \\(\\large{\\boxed{\\substack{w^{1^T}x_1 = -0.5 \\\\ \\hat{y} = -1}}, \\;\\; \\substack{w^{1^T}x_2 = 0.5 \\\\ \\hat{y} = +1}, \\;\\; \\substack{w^{1^T}x_3 = -1.25 \\\\ \\hat{y} = -1}}\\) Iteration 3 Our algorithm made a mistake in prediciton of the third (boxed) datapoint. Now we will update \\(w\\) as a mistake in prediciton occured, \\(w^2 = w^1 + x_1 y_1 = \\begin{bmatrix} 1 \\\\ 1/2 \\end{bmatrix}\\) \\(\\large{\\substack{w^{1^T}x_1 = 0.5 \\\\ \\hat{y} = +1}, \\;\\; \\boxed{\\substack{w^{1^T}x_2 = -0.5 \\\\ \\hat{y} = -1}}, \\;\\; \\substack{w^{1^T}x_3 = -0.75 \\\\ \\hat{y} = -1}}\\) Iteration 4 Our algorithm made a mistake in prediciton of the second (boxed) datapoint. Now we will update \\(w\\) as a mistake in prediciton occured, \\(w^3 = w^0 + x_2 y_2 = \\begin{bmatrix} 1 \\\\ -1/2 \\end{bmatrix}\\) We can see that this \\(w^3\\) is the same as \\(w^1\\) , which means if we go ahead with this iteration we run into again predict the second datapoint wrong and hence the loop will keep on running without stopping. At the end , our perceptron algorithm will never give us a \\(w\\) which classifies all the points correctly and the algorithm will never converge. In such an edge case , there exists no \\(w\\) which will correctly classify all the datapoints , even though , at first the dataset may look linearly separable Animation In each iteration the incorrectly predicted point is highlighted in yellow color. Points on the right side of the Decision Boundry are predicted as +1 (positive), while points on the left side are predicted as -1 (negative) Even after several iterations it can be seen that the weight vector keeps going back and forth between w=[1,0.5] and w=[1,-0.5] . Hence we can see that the algorithm will never converge. Why does this happen? One of the reasons is , the given dataset is not strictly linearly separable as some of the datapoints lie on the decision boundry itself and we chose to label such datapoints as +1 , though they can also be labeled as -1. What to do to solve this issue? We can apply more stricter \"assumptions\" on our dataset to account for the edge case described above. A dataset is linearly separable with \\(\\gamma\\) margin if, \\(\\exists w^* \\in \\mathbb{R}^d \\quad \\text{s.t. } \\quad (w^{*^T}x_i)y_i \\geq \\gamma \\forall i \\quad \\text{for some } \\gamma > 0\\) (There exists a \\(w^*\\) such that \\((w^{*^T}x_i) y_i \\geq \\gamma\\) for all the datapoints where \\(\\gamma > 0\\) ) This assumption makes it so that there are no datapoints between the parallel dotted lines , in other words , this also means that no points will lie on \\(w^T x = 0\\)","title":"Redifining Linear Separatability"},{"location":"perceptron_learning/#proof-of-convergence-of-perceptron-algorithm","text":"To prove the convergence of the algorithm we are going to make a few assumptions about the dataset Linear Separatability with \\(\\gamma\\) margin. Radius Assumption \\(\\forall i \\in D \\quad ||x_i||_2 \\leq R \\quad \\text{for some } R > 0\\) This basically means that all the points in dataset \\(D\\) fall within a circle of radius \\(R\\) . Without loss of generality , assume \\(||w^*|| = 1\\) This basically means that we have normalized our \\(w\\) get \\(w^*\\) . We will now try to quantify the number of mistakes our aglorithm can make , if the number of mistakes is finite then it means the number of iterations is also finite , therefore , our algorithm must converge.","title":"Proof of Convergence of Perceptron Algorithm"},{"location":"perceptron_learning/#analysis-of-mistakes-of-perceptron-algorithm","text":"Observe that an update in perceptron algorithm only happens when a mistake occurs. Say \\(w^l\\) is the current guess and a mistake happens w.r.t (x,y).","title":"Analysis of 'Mistakes' of Perceptron Algorithm"},{"location":"perceptron_learning/#bound-1","text":"Now we will take look at what happens to the length of \\(w\\) after an update \\[\\begin{equation*} \\begin{split} w^l &= w^{l-1} + xy \\\\ ||w^l||^2 &= ||w^{l-1} + xy||^2 \\\\ &= (w^{l-1} + xy)^T (w^{l-1} + xy) \\\\ &= \\underbrace{||w^{l-1}||^2}_{\\geq 0} + \\underbrace{2(w^{{l-1}^T} x)y}_{\\leq 0} + \\underbrace{||x||^2 \\underbrace{y^2}_{\\pm1}}_{\\leq R^2} \\\\ \\\\ \\therefore ||w^l||^2 &\\leq ||w^{l-1}||^2 + R^2 \\\\ &\\leq (||w^{l-2}||^2 +R^2 ) + R^2 \\\\ &\\vdots \\\\ &\\leq ||w^0||^2 + l R^2 \\\\ \\\\ \\boxed{\\therefore ||w^l||^2 \\leq l R^2} \\\\ \\end{split} \\end{equation*}\\] Note The reason why \\(y^2\\) is always greater than zero because it is this label of of the datapoints and can only take value of either +1 or -1. ||w||^2 will also be always greater than or equal to zero as square of any real number is always a positive number ( \\(w\\) is just a vector comprising of real numbers) Product of \\(||x||^2\\) and \\(y^2\\) will always be less than or equal to \\(R^2\\) because as per our assumption , all the points lie within a circl of radius \\(R\\) . \\(2(w^{{l-1}^T} x) y\\) will always be less than one because an update only happens when a mistake occurs, If actual label is +1 and predicted label is -1 , this implies \\(w^T x < 0\\) i.e. its a negative value and the (actual) label is a positive value , product of negative and positive values is always negative. Similarly , if actual label is -1 and predicted label is +1, this implies \\(w^T x \\geq 0\\) i.e its a positive value and the (actual) label is a negative value , product of positive and negative values is always negative.","title":"Bound 1"},{"location":"perceptron_learning/#bound-2","text":"We will now use \\(w^*\\) (the best \\(w\\) which exists) to obtain another bound for number of mistakes. \\[\\begin{equation*} \\begin{split} w^l &= w^{l-1} + x y \\\\ (w^l)^T w^* &= (w^{l-1} + xy)^T w^* \\\\ &= w^{{l-1}^T} w^* + \\underbrace{(w^{*^T} x)y}_{\\geq \\gamma} \\\\ \\\\ \\therefore (w^l)^T w^* &\\geq (w^{l-1})^T w^* + \\gamma \\\\ & \\geq (w^{{l-2}^T} w^* + \\gamma) + \\gamma \\\\ & \\vdots \\\\ & \\geq \\underbrace{(w^0)^T w^*}_{0} + l \\gamma \\\\ \\\\ \\therefore (w^l)^T w^* &\\geq l \\gamma \\\\ \\\\ ((w^l)^T w^*)^2 &\\geq l^2 \\gamma^2 \\\\ \\\\ ||w^l||^2 \\underbrace{||w^*||^2}_{1} &\\geq l^2 \\gamma^2 \\text{ Using Cauchy-Schwarz Inequality } \\\\ \\boxed {\\therefore ||w^l||^2 \\geq l^2 \\gamma^2} \\end{split} \\end{equation*}\\] Cauchy-Schwarz Inequaltiy We know that, \\[ -1 \\leq \\cos(\\theta) \\leq 1 \\] Multiplying the product of norm of some vectors \\(v\\) and \\(w\\) in the above equation, \\[\\begin{equation*} \\begin{split} -||v|| \\times ||w|| \\leq ||v|| &\\times ||w|| \\cos(\\theta) \\leq ||v|| \\times ||w|| \\\\ -||v|| \\times ||w|| \\leq v &\\cdot w \\leq ||v|| \\times ||w|| \\\\ |v \\cdot w| &\\leq ||v|| \\times ||w|| \\end{split} \\end{equation*}\\] Note : Dot product of any 2 vectors is given by \\(x \\cdot y = ||x|| \\times ||y|| \\cos(\\theta)\\) Note In the above assumptions we said that norm of \\(w^*\\) is 1.","title":"Bound 2"},{"location":"perceptron_learning/#radius-margin-bound","text":"Now that we have both upper and lower bound of \\(||w^l||\\) , \\[\\begin{equation*} \\begin{split} l^2 \\gamma^2 &\\leq ||w^l||^2 \\leq l R^2 \\\\ l^2 \\gamma^2 &\\leq l R^2 \\\\ \\therefore l &\\leq \\frac{R^2}{\\gamma^2}\\\\ \\end{split} \\end{equation*}\\] Conclusion : A dataset which follows all the assumptions above , its mistakes are always less than or equal to Radius in which all the datapoints lie divided by the margin gamma with respect to the optimal \\(w^*\\) . As the number of mistakes now can be quantified , we can say that this perceptron algorithm will converge.","title":"Radius Margin Bound"},{"location":"perceptron_learning/#sigmoid-function-for-modeling-class-probabilities","text":"We know that Perceptron Algorithm makes a linear separatability assumption , if we were to write that in probabilistic manner , then one way to do that would be, \\[\\begin{equation*} P(y=1|x) = \\begin{cases} 1 & \\text{if } w^T x \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases} \\end{equation*}\\] As the probabilities of the are 1 and 0 for a given datapoint , the perceptron algorithm only works on linearly separable datasets. If the dataset has points which are not linearly separable but appear to be outliers, our perceptron algorithm is unable to run on such a dataset. Can we somehow relax the probabilities for a given datapoint? So that the perceptron algorithm also works on datasets which are not linearly separable.","title":"Sigmoid Function for Modeling Class Probabilities"},{"location":"perceptron_learning/#simple-linear-probabilistic-model","text":"We will now start building up to a reasonable algorithm which has realaxed probabilities. We will start with a simple linear model where the score ( \\(z\\) ) of a datapoint is given by \\(z = w^T x\\) . We are going to allow any point to be labeled as +1 or -1 , but the deciding factor for which label will +1 or -1 will be based on the score ( \\(z\\) ) of the datapoint. The higher the score of a label is , the higher will be its probability of being labeled as +1 , similarly the lower the score of the datapoint is , the higher will be its probability of being labeled as -1. An intuitive way to think about this would be that , \\(x_2\\) is farther away from the decision boundry/linear separator , hence we're more confident that \\(x_2\\) should be labeled as +1 when compared to \\(x_1\\) which is much much closer to the decision boundry. Example In the above diagram , \\(x_1\\) will have a lower score when compared to \\(x_2\\) ( \\(w^T x_1 < w^T x_2\\) ), this means that probability of \\(x_1\\) being labeled as +1 is lower than probability of \\(x_2\\) being labeled as +1 ( \\(P(y = 1 | x_1) < P(y = 1 | x_2)\\) ). Similarly , \\(x_4\\) is farther away than \\(x_3\\) , which means that \\(x_4\\) has higher probability of being labeled as -1 when compared to \\(x_3\\) ( \\(P(y = -1 | x_3) < P(y = -1 | x_4)\\) ). According to this simple linear model , every point has the probability to be labeled as +1 or -1 , just the chance that it gets labeled (+1 or -1) depends on how far it is form the decision boundry. Problems With Current Linear Model We can calculate the score for any datapoint given a \\(w\\) but we dont have function/method to convert this score to a probability. To convert the score to a probability will use the Sigmoid Function. This function goes from \\(0 \\to 1\\) over the domain of \\((-\\infty , \\infty)\\) and is given by \\(g(z) = \\frac{1}{1 + e^{-z}}\\) . Here , \\(z = w^T x\\) . Hence, with sigmoid function we define probabilities of a datapoint being labeled as +1 or -1 which means datasets which were previously not allowed are now allowed for our Probabilistic Perceptron Algorithm.","title":"Simple Linear Probabilistic Model"},{"location":"perceptron_learning/#logistic-regression_1","text":"We have developed Probabilistic Perceptron Algorithm , which labels datapoints on certain score (z) , but the problem with our current approach is that there may exist many \\(w\\) which correctly classify the datapoints. We dont have any method to identify the best \\(w\\) among all the \\(w\\) 's which classify the datapoints. Here \\(w_1\\) and \\(w_2\\) both will be able to classify the datapoints , but which amongst them is a better \\(w\\) ? This seems to be problem of maximum liklihood which can be solved using using the traditional method of derivative of log-likelihood. For a dataset \\(D = \\{(x_1 , y_1) , (x_2 , y_2) , ... (x_n , y_n) \\}\\) , where \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{-1 , 1 \\}\\) We know that, \\[ P(y=1|x) = g(w^T x_i) = \\frac{1}{1 + e^{- w^T x}} \\] The maximum likelihood expression will be, \\[\\begin{equation*} \\begin{split} \\mathcal{L}(w;D) &= \\prod_{i=1}^n (g(w^T x_i))^{y_i} (1 - g(w^T x_i))^{1 - y_i} \\\\ \\log( \\mathcal{L}(w;D) ) &= \\sum_{i=1}^{n} y_i \\log(g(w^T x_i)) + (1 - y_i) \\log(1 - g(w^T x_i)) \\\\ &= \\sum_{i=1}^{n} y_i \\log (\\frac{1}{1 + e^{-w^T x_i}}) + (1 - y_i) \\log (\\frac{e^{-w^T x_i}}{1 + w^{-w^T x_i}}) \\\\ &= \\sum_{i=1}^{n} [(1 - y_i)(-w^T x_i) -log(1 + e^{-w^T x_i})] \\\\ \\end{split} \\end{equation*}\\] Note Our assumption here is the probabilities are generated independent of other labels. The above Maximum Likelihood is very similar to Maximal Likelihood of a bernoulli random variable. Our goal now is maximize the above equation with respect to \\(w\\) , so that we get the best possible \\(w\\) , \\[ \\underset{w}{\\max} \\sum_{i=1}^{n} [(1 - y_i)(- w^T x_i) - \\log(1 + e^{- w^T x_i})] \\] However , the above equation doesnt have a closed-form solution when a derivative is taken to solve further. Therefore, we will use gradient descent to identify the best \\(w\\) . The gradient descent of the above log-likelihood function will be, \\[\\begin{equation*} \\begin{split} \\nabla \\log (\\mathcal{L}(w;D)) &= \\sum_{i=1}^{n} \\left[(1 - y_i)(-x_i) - \\frac{e^{-w^T x_i}}{1 + e^{-w^Tx)i}} (-x_i) \\right] \\\\ &= \\sum_{i=1}^{n} \\left[ -x_i + x_i y_i + x_i \\left( \\frac{e^{-w^T x_i}}{1 + e^{-w^Tx_i}} \\right) \\right] \\\\ &= \\sum_{i=1}^{n} \\left[ x_iy_i - x_i \\left( \\frac{1}{1 + e^{-w^T x_i}} \\right) \\right] \\\\ \\nabla \\log(\\mathcal{L}(w;D)) &= \\sum_{i=1}^{n} \\left[ x_i \\left(y_i - \\frac{1}{1 + e^{-w^T x_i}} \\right) \\right] \\end{split} \\end{equation*}\\] Using the Gradient Descent formula we get, \\[\\begin{equation*} \\begin{split} w_{t+1} &= w_t + \\mathcal{n}_t \\nabla \\log ( \\mathcal{L}(w;D))\\\\ &= w_t + \\mathcal{n}_t \\left( \\sum_{i=1}^n x_i \\left(\\overbrace{y_i - \\underbrace{\\frac{1}{1 + e^{-w^Tx_i}}}_{g(w^T x_i)}}^{\\theta_i} \\right) \\right) \\end{split} \\end{equation*}\\] Note The term after \\(y_i\\) is the Sigmoid Function which outputs a probability for a certain score of a datapoint. Lets say for a point the actual label is +1 and it is predicted correctly as +1 ( \\(y_i = 1\\) ) , this means that the point has a higher probability (lets say \\(g(w^Tx_i) = 0.9\\) ) when compared to the other binary label, which was derived from the sigmoid function. This also means that \\(\\theta_i\\) will be a small value as \\(1 - 0.9 = 0.1\\) . This also means that this particular datapoint will not have much effect on the direction of the gradient descent algorithm. Prediction of a datapoint \\(x_\\text{test}\\) is given by , \\[ y_\\text{test} = \\text{sign}( \\hat{w}^T x_\\text{test} ) \\]","title":"Logistic Regression"},{"location":"perceptron_learning/#advantages-of-logistic-regression","text":"There is a kernel version for the above equation as it can be argued that \\(w = \\sum_{i=1}^n \\alpha_i x_i\\) Regularized Version for the above equation is , \\[\\underset{w}{\\min} \\sum_{i=1}^n \\left[ \\log(1 + e^{-w^T x_i}) + w^T x_i (1-y_i) \\right] + \\underbrace{\\frac{\\lambda}{2} ||w||^2}_{\\text{Regularizer}} \\] Here \\(\\lambda\\) is Cross Validated Hyperparameter.","title":"Advantages of Logistic Regression"},{"location":"regression/","text":"Regression Goodness of Maximum Likelihood Estimator for linear Regression In the previous week we observed that \\(w^*\\) which comes from the squared error equation is the same as \\(\\hat{w}_{\\text{ML}}\\) which came from the maximal Likelihood equation. Now we are going to look at \\(\\hat{w}_{\\text{ML}}\\) and its properties , which will gives us a better idea of \\(w\\) . For \\(\\hat{w}_{\\text{ML}}\\) our assumption was that, \\[y|x = w^T x + \\epsilon \\] where \\(\\epsilon\\) (gaussian noise) belongs to a gaussian distribution with mean 0 and variance \\(\\sigma^2\\) ( \\(\\epsilon \\sim \\mathcal{N}(0 , \\sigma^2)\\) ). For every \\(x\\) in our data \\(y\\) was generated using some \\(w^T x\\) , using some unknown but fixed \\(w\\) and then adding 0 mean gasussian noise to it. Hence , \\(y\\) given \\(x\\) can be thought of as gaussian distribution with mean \\(w^T x\\) and variance \\(\\sigma^2\\) ( \\(y|x \\sim \\mathcal{N}(w^T x , \\sigma^2)\\) ) For \\(\\hat{w}_{\\text{ML}}\\) we should now look for a way to test that , how good the function is as a guess from true \\(w\\) . A good function to compare \\(\\hat{w}_{\\text{ML}}\\) to \\(w\\) is \\[{||\\hat{w}_{\\text{ML}} - w ||}^2\\] and to see what happens to this value on an average we will look at its expected value over the randomness in \\(y\\) , which can be written as \\[E[{||\\hat{w}_{\\text{ML}} - w ||}^2]\\] If we solve for the expected value further , we get \\[E[{||\\hat{w}_{\\text{ML}} - w ||}^2] = \\sigma^2 \\text{trace}((XX^T)^ \\dagger)\\] where \\(\\sigma^2\\) is the variance from the gaussian noise \\(\\sigma^2\\) in the expected value of the above function cant be reduced , it is the variance / loss which will always happen. Only thing we can reduce is the trace of the inverse of covariance matrix Cross-validation for minimizing MSE We know that trace of a matrix is the sum of the diagonal entries of matrix , but in previous courses we have also seen that trace of a matrix is also equal to the sum of the eigenvalues of that matrix. \\[\\text{tr}(A) = \\sum_{i=1}^{d} \\lambda_i\\] where \\(A\\) is any matrix , \\(d\\) is the dimension of the matrix and \\(\\lambda_i\\) is the \\(i^{\\text{th}}\\) eigenvalue. Let the eigenvalues of \\((XX^T)\\) be \\(\\{\\lambda_1 , \\lambda_2 , .... \\lambda_d \\}\\) . The eigenvalues of \\((XX^T)^{-1}\\) are \\(\\{ \\frac{1}{\\lambda_1} , \\frac{1}{\\lambda_2} , .... \\frac{1}{\\lambda_d} \\}\\) The mean squared error equation \\((\\hat{w}_{\\text{ML}})\\) , \\[E(|| \\hat{w}_{\\text{ML}} - w ||^2) = \\sigma^2 \\left( \\sum_{i=1}^d \\frac{1}{\\lambda_i} \\right)\\] Consider the following estimator: \\[\\hat{w}_{\\text{new}} = (XX^T + \\lambda I )^{-1} XY \\] For some matrix \\(A\\) , let the eigenvalues be \\(\\{ \\lambda_1 , \\lambda_2 , ..... \\lambda_d \\}\\) Now what will be the eigenvalues of \\(A + \\lambda I\\) ? \\[\\begin{equation*} \\begin{split} A v_1 &= \\lambda_1 v_1 \\\\ (A + \\lambda I)v_1 &= A v_1 + \\lambda v_1 \\\\ &= \\lambda_1 v_1 + \\lambda v_1 \\\\ &= (\\lambda_1 + \\lambda) v_1 \\\\ \\end{split} \\end{equation*}\\] \\(\\implies\\) eigenvalues will be \\(\\{ \\lambda_1 + \\lambda , \\lambda_2 + \\lambda , ... \\lambda_d + \\lambda \\}\\) Similarly eigenvalues of \\({(XX^T + \\lambda I)}^{-1}\\) will be \\(\\{ \\frac{1}{\\lambda_1 + \\lambda} , \\frac{1}{\\lambda_2 + \\lambda} , ... \\frac{1}{\\lambda_d + \\lambda} \\}\\) \\[\\therefore \\text{trace}((XX^T + \\lambda I)^{-1}) = \\left( \\sum_{i=1}^d \\frac{1}{\\lambda_i + \\lambda} \\right)\\] If the MSE is really large one of the reasons for it might because the trace of the matrix was really large , which means the eigenvalues were really small (smaller eigenvalues give large trace as they are inversely proportional). To counter this we artifically introduced \\(\\lambda\\) which increases the overall eigenvalues and hence reducing the trace of the matrix , which in turn decreases MSE. Types of Cross Validation Three commonly used techniques for cross-validation are as follows: Training-Validation Split: The training set is randomly divided into a training set and a validation set, typically in an 80:20 ratio. Among various \\(\\lambda\\) values, the one that yields the lowest error is selected. K-Fold Cross Validation: The training set is partitioned into K equallysized parts. The model is trained K times, each time using K-1 parts as the training set and the remaining part as the validation set. The \\(\\lambda\\) value that leads to the lowest average error is chosen. Leave-One-Out Cross Validation: The model is trained using all but one sample in the training set, and the left-out sample is used for validation. This process is repeated for each sample in the dataset. The optimal \\(\\lambda\\) is determined based on the average error across all iterations. Bayesian Modeling for Linear Regression What are conjugate priors? The key property of a conjugate prior is that, when combined with a likelihood function, the resulting posterior distribution belongs to the same family of distributions as the prior. This property simplifies the computation of the posterior distribution. Note: The conjugate prior for a gaussian distribution is gaussian distribution itself. We know that our original likelihood function was, \\[ y|x \\sim \\mathcal{N}(w^Tx , 1) \\] where we are taking \\(\\sigma^2 = 1\\) for convenience , the following derivation will still be valid when variance is just \\(\\sigma^2\\) . Now our prior will be, \\[ w \\sim \\mathcal{N}(0 , \\gamma^2 I) \\] where \\(\\mu \\in \\mathbb{R}^d\\) and \\(\\gamma^2 I \\in \\mathbb{R}^{d \\times d}\\) Now, \\[\\begin{equation*} \\begin{split} P(w| \\{ (x_1 , y_1) , (x_2 , y_2) , .... (x_n , y_n) \\}) &\\propto P(\\{(x_1 , y_1) , (x_2 , y_2) , .... (x_n , y_n) \\}|w) \\times P(w) \\\\ &\\propto \\left(\\prod_{i=1}^{n} e^{-\\frac{(y_i - w^Tx)^2}{2}} \\right) \\times \\left(\\prod_{i=1}^{n} e^{-\\frac{(w_i - 0)^2}{2 \\gamma^2}} \\right) \\\\ &\\propto \\left(\\prod_{i=1}^{n} e^{-\\frac{(y_i - w^Tx)^2}{2}} \\right) \\times e^{- \\frac{||w||^2}{2 \\gamma^2}} \\\\ \\end{split} \\end{equation*}\\] Now how will maximum aposterior (MAP) estimate look like? (Here we are taking log of the above function) \\[\\begin{equation*} \\begin{split} \\hat{W}_{\\text{MAP}} &= \\overset{\\arg}{\\underset{w}{\\max}} \\sum_{i=1}^{n}- \\frac{(y_i - w^Tx_i)^2}{2} - \\frac{||w||^2}{2 \\gamma^2} \\\\ \\hat{W}_{\\text{MAP}} &= \\overset{\\arg}{\\underset{w}{\\min}} \\sum_{i=1}^{n} \\frac{(y_i - w^Tx_i)^2}{2} + \\frac{||w||^2}{2 \\gamma^2} \\\\ \\end{split} \\end{equation*}\\] Taking gradient of this function, \\[\\begin{equation*} \\begin{split} \\nabla f(w) &= (XX^T)w - Xy + \\frac{w}{\\gamma^2} \\\\ \\hat{W}_{\\text{MAP}} &= (XX^T + \\frac{1}{\\gamma^2}I )^{-1}Xy \\end{split} \\end{equation*}\\] We can see that the same estimator can be derived when find out MAP for a gaussian distribution prior. Ridge Regression Previously our linear regression equation was, \\[ \\hat{w}_{\\text{ML}} = \\underset{w}{\\arg \\min} \\sum_{i=1}^{n} (w^T x_i - y_i)^2 \\] The Ridge Regression equation is given as, \\[ \\hat{w}_{\\text{R}} = \\underset{w}{\\arg \\min} \\sum_{i=1}^{n} (w^T x_i - y_i)^2 + \\lambda ||w||^2 \\] where \\(\\sum_{i=1}^{n} (w^T x_i - y_i)^2\\) is the loss and \\(\\lambda ||w||^2\\) is the regularizer. The regularizer has bayesian viewpoint to it as shown in the above derivation, also it can be thought of as adding a penalty on the overall function for prefering a certain type of \\(w\\) . The higher the weights of the \\(w\\) the larger the penalty , it can also be thought of as prefering \\(w\\) which have their features reduced to zero or almost zero. Note Ridge Regression has more error than linear regression. Ridge Regresison increases the training error so that the model does not overfit. Relation Between Solution of Linear Regression and Ridge Regression Lets say for a dataset we solved the linear regression problem and got \\(\\hat{w}_{\\text{ML}}\\) on a 2-d subspace Now is there any way to find the ridge regression solution for the same dataset? We know that equation of ridge regression is, \\[ \\hat{w}_{\\text{R}} = \\underset{w}{\\arg \\min} \\sum_{i=1}^{n} (w^T x_i - y_i)^2 + \\lambda ||w||^2 \\] It can be argued that this problem/equation is the same as constrained optimization problem, \\[\\begin{equation*} \\begin{split} \\underset{w \\in \\mathbb{R}^d}{\\min} & \\sum_{i=1}^n (w^T x_i - y_i)^2 \\\\ \\\\ \\text{such that}\\\\ & ||w||^2 \\leq \\theta \\\\ \\end{split} \\end{equation*}\\] For every choice of \\(\\lambda > 0\\) , there exists a \\(\\theta\\) such that the optimal solutions of Ridge Regression and Constrained Ridge Regression coincide. In a 2-d space , when \\(||w||^2 \\leq \\theta\\) , it can be written as, \\[\\begin{equation*} \\begin{split} ||w||^2 \\leq \\theta \\\\ \\implies w_1^2 + w_2^2 \\leq \\theta \\\\ \\end{split} \\end{equation*}\\] This is very similar to the equation of a circle whose radius (here) is \\(\\theta\\) and it is centered at origin. From the above image we can see that we have found out a constrained area for \\(\\hat{w}_{\\text{Ridge}}\\) , but where it is exactly? What is the loss/error value of linear regression at \\(\\hat{w}_{\\text{ML}}\\) ? Note The loss calculated at \\(\\hat{w}_{\\text{ML}}\\) is the least when compared to any other \\(w\\) . \\[\\begin{equation*} \\begin{split} \\sum_{i=1}^n (\\hat{w}_{\\text{ML}} x_i - y_i)^2 &= f(\\hat{w}_{\\text{ML}}) \\\\ \\end{split} \\end{equation*}\\] Consider the set of \\(w\\) such that \\[\\begin{equation*} \\begin{split} f(w) &= f( \\hat{w}_{\\text{ML}} ) + c \\;\\;\\;\\;\\;\\; c>0 \\\\ S_c &= \\{ w : f(w) = f( \\hat{w}_{\\text{ML}}) + c \\} \\\\ \\end{split} \\end{equation*}\\] Every vector in \\(S_c\\) satisfies, \\[\\begin{equation*} \\begin{split} \\underset{f(w)}{||X^Tw - y||^2} &= \\underset{f(\\hat{w}_{\\text{ML}})}{|| X^T \\hat{w}_{\\text{ML}} - y ||^2 } + c \\\\ \\\\ \\text{on simplification,}\\\\ (w - \\hat{w}_{\\text{ML}})^T (XX^T) (w - \\hat{w}_{\\text{ML}}) &= c' \\\\ \\end{split} \\end{equation*}\\] If we have \\(\\hat{w}_{\\text{ML}}\\) , \\(XX^T\\) and \\(c'\\) we can get a set of all the \\(w\\) which satisfy the above equation such that they are \\(c'\\) distance away in terms of error when compared to \\(\\hat{w}_{\\text{ML}}\\) . If \\(XX^T = I\\) ( \\(I\\) = Identity Matrix), \\[\\begin{equation*} \\begin{split} (w - \\hat{w}_{\\text{ML}})^T I (w - \\hat{w}_{\\text{ML}}) &= c' \\\\ || w - \\hat{w}_{\\text{ML}} ||^2 = c' \\\\ \\end{split} \\end{equation*}\\] This again corresponds to the equation of a circle (with c' radius) in a 2-d space , but thats only the case when \\(XX^T = I\\) . If \\(XX^T \\neq I\\) then instead of a circle , an ellipse is formed around \\(\\hat{w}_{\\text{ML}}\\) . If we keep increasing the \\(c'\\) while using the same values for \\(\\hat{w}_{\\text{ML}}\\) and \\(XX^T\\) , ellipses with increasing size are formed around \\(\\hat{w}_{\\text{ML}}\\) which eventually touch the circle formed by the constrained ridge regression equation. The point where it touches (yellow dot) is the ridge regression solution with the least loss possible when compared to any other \\(\\hat{w}_{\\text{Ridge}}\\) in the green circle. In other words that point (yellow dot) is closest to \\(\\hat{w}_{\\text{ML}}\\) and yet satisfy the constrained ridge regression equation. Conclusion We can see that Ridge Regression pushes the values in the wieght vector ( \\(w\\) ) to 0 , but does not necessarily make them 0. Relation betwen solution of Linear Regression and Lasso Regression Our goal here is to change the regularizer in ridge regression equation in such a way that the elliptical contours around \\(\\hat{w}_{\\text{ML}}\\) hit at a point where some of the features become zero. We know that ridge regression pushes feature values towards 0. But does not necessarily make it 0. An alternate way to regularize would be to use \\(||\\cdot||_1\\) norm instead of \\(||\\cdot||^2_2\\) norm. \\[ ||w||_1 = \\sum_{i=1}^d | w_i | \\] For L1 Regularization , \\[\\begin{equation*} \\begin{split} & \\underset{w \\in \\mathbb{R}^d}{\\min} \\sum_{i=1}^{n} (w^T x_i - y_i)^2 + \\lambda ||w||_1\\\\ \\\\ & \\text{which is similar to} \\\\ \\\\ & \\underset{w \\in \\mathbb{R}^d}{\\min} \\sum_{i=1}^{n} (w^T x_i - y_i)^2 \\\\ & \\text{such that}\\\\ \\\\ ||w||_1 \\leq \\theta \\\\ \\end{split} \\end{equation*}\\] When compared to L2 Constraint on the regularizer , this is how L1 constraint would look like Now when we keep increasing the size of the elliptical contours around \\(\\hat{w}_{\\text{ML}}\\) our hope is that it touches the point where some of the features of weight vector become zero. When looking at this , one can argue that the elliptical contours will not always touch red area in such a way that one of the weight vectors becomes 0; Which is true when looking at it in a 2d subspace , but in higher dimensions the typical case is when some of weight vectors become 0. This way of using L1 Regularizer is called LASSO (Least Absolute Shrinkage and Selection Operator) Characteristics of Lasso regression We know now that lasso regression makes certain weight vectors zero , so why not always use lasso? Advantages of using Ridge Regression vs Lasso Regression, Lasso Regression does not have a closed form solution. Sub-gradient methods are usually used to solve for Lasso Regression. What are Sub-Gradients? For a piecewise linear function at the point \\(x\\) (purple) point the function is not differentiable , sub-gradient provide a lower bound for this function at \\(x\\) (purple point). At the blue point the function is differentiable and hence only 1 sub-gradient exists which is the gradient/slope itself. Now , what is the use of sub-gradients in lasso regression? We know that the regularizer in lasso regression takes the absolute values of weight vectors, At the origin (green point) finding the differential is not possible , hence we bound the function using sub-gradients. Any sub-gradient between \\([-1,1]\\) lower bounds the function ( \\(|x|\\) ). Definition of Sub-Gradients A vector \\(g \\in \\mathbb{R}^d\\) is a sub-gradient of \\(f:\\mathbb{R}^D \\to \\mathbb{R}\\) at a point \\(x \\in \\mathbb{R}^d\\) , if \\[\\forall z \\;\\;\\;\\;\\;\\;\\;\\;\\; f(z) \\geq f(x) + g^T(z - x)\\] Whats the use of Sub-Gradients? If function \\(f\\) to minimize is a convex function , then sub-gradient descent converges.","title":"Regression"},{"location":"regression/#regression","text":"","title":"Regression"},{"location":"regression/#goodness-of-maximum-likelihood-estimator-for-linear-regression","text":"In the previous week we observed that \\(w^*\\) which comes from the squared error equation is the same as \\(\\hat{w}_{\\text{ML}}\\) which came from the maximal Likelihood equation. Now we are going to look at \\(\\hat{w}_{\\text{ML}}\\) and its properties , which will gives us a better idea of \\(w\\) . For \\(\\hat{w}_{\\text{ML}}\\) our assumption was that, \\[y|x = w^T x + \\epsilon \\] where \\(\\epsilon\\) (gaussian noise) belongs to a gaussian distribution with mean 0 and variance \\(\\sigma^2\\) ( \\(\\epsilon \\sim \\mathcal{N}(0 , \\sigma^2)\\) ). For every \\(x\\) in our data \\(y\\) was generated using some \\(w^T x\\) , using some unknown but fixed \\(w\\) and then adding 0 mean gasussian noise to it. Hence , \\(y\\) given \\(x\\) can be thought of as gaussian distribution with mean \\(w^T x\\) and variance \\(\\sigma^2\\) ( \\(y|x \\sim \\mathcal{N}(w^T x , \\sigma^2)\\) ) For \\(\\hat{w}_{\\text{ML}}\\) we should now look for a way to test that , how good the function is as a guess from true \\(w\\) . A good function to compare \\(\\hat{w}_{\\text{ML}}\\) to \\(w\\) is \\[{||\\hat{w}_{\\text{ML}} - w ||}^2\\] and to see what happens to this value on an average we will look at its expected value over the randomness in \\(y\\) , which can be written as \\[E[{||\\hat{w}_{\\text{ML}} - w ||}^2]\\] If we solve for the expected value further , we get \\[E[{||\\hat{w}_{\\text{ML}} - w ||}^2] = \\sigma^2 \\text{trace}((XX^T)^ \\dagger)\\] where \\(\\sigma^2\\) is the variance from the gaussian noise \\(\\sigma^2\\) in the expected value of the above function cant be reduced , it is the variance / loss which will always happen. Only thing we can reduce is the trace of the inverse of covariance matrix","title":"Goodness of Maximum Likelihood Estimator for linear Regression"},{"location":"regression/#cross-validation-for-minimizing-mse","text":"We know that trace of a matrix is the sum of the diagonal entries of matrix , but in previous courses we have also seen that trace of a matrix is also equal to the sum of the eigenvalues of that matrix. \\[\\text{tr}(A) = \\sum_{i=1}^{d} \\lambda_i\\] where \\(A\\) is any matrix , \\(d\\) is the dimension of the matrix and \\(\\lambda_i\\) is the \\(i^{\\text{th}}\\) eigenvalue. Let the eigenvalues of \\((XX^T)\\) be \\(\\{\\lambda_1 , \\lambda_2 , .... \\lambda_d \\}\\) . The eigenvalues of \\((XX^T)^{-1}\\) are \\(\\{ \\frac{1}{\\lambda_1} , \\frac{1}{\\lambda_2} , .... \\frac{1}{\\lambda_d} \\}\\) The mean squared error equation \\((\\hat{w}_{\\text{ML}})\\) , \\[E(|| \\hat{w}_{\\text{ML}} - w ||^2) = \\sigma^2 \\left( \\sum_{i=1}^d \\frac{1}{\\lambda_i} \\right)\\] Consider the following estimator: \\[\\hat{w}_{\\text{new}} = (XX^T + \\lambda I )^{-1} XY \\] For some matrix \\(A\\) , let the eigenvalues be \\(\\{ \\lambda_1 , \\lambda_2 , ..... \\lambda_d \\}\\) Now what will be the eigenvalues of \\(A + \\lambda I\\) ? \\[\\begin{equation*} \\begin{split} A v_1 &= \\lambda_1 v_1 \\\\ (A + \\lambda I)v_1 &= A v_1 + \\lambda v_1 \\\\ &= \\lambda_1 v_1 + \\lambda v_1 \\\\ &= (\\lambda_1 + \\lambda) v_1 \\\\ \\end{split} \\end{equation*}\\] \\(\\implies\\) eigenvalues will be \\(\\{ \\lambda_1 + \\lambda , \\lambda_2 + \\lambda , ... \\lambda_d + \\lambda \\}\\) Similarly eigenvalues of \\({(XX^T + \\lambda I)}^{-1}\\) will be \\(\\{ \\frac{1}{\\lambda_1 + \\lambda} , \\frac{1}{\\lambda_2 + \\lambda} , ... \\frac{1}{\\lambda_d + \\lambda} \\}\\) \\[\\therefore \\text{trace}((XX^T + \\lambda I)^{-1}) = \\left( \\sum_{i=1}^d \\frac{1}{\\lambda_i + \\lambda} \\right)\\] If the MSE is really large one of the reasons for it might because the trace of the matrix was really large , which means the eigenvalues were really small (smaller eigenvalues give large trace as they are inversely proportional). To counter this we artifically introduced \\(\\lambda\\) which increases the overall eigenvalues and hence reducing the trace of the matrix , which in turn decreases MSE.","title":"Cross-validation for minimizing MSE"},{"location":"regression/#types-of-cross-validation","text":"Three commonly used techniques for cross-validation are as follows: Training-Validation Split: The training set is randomly divided into a training set and a validation set, typically in an 80:20 ratio. Among various \\(\\lambda\\) values, the one that yields the lowest error is selected. K-Fold Cross Validation: The training set is partitioned into K equallysized parts. The model is trained K times, each time using K-1 parts as the training set and the remaining part as the validation set. The \\(\\lambda\\) value that leads to the lowest average error is chosen. Leave-One-Out Cross Validation: The model is trained using all but one sample in the training set, and the left-out sample is used for validation. This process is repeated for each sample in the dataset. The optimal \\(\\lambda\\) is determined based on the average error across all iterations.","title":"Types of Cross Validation"},{"location":"regression/#bayesian-modeling-for-linear-regression","text":"What are conjugate priors? The key property of a conjugate prior is that, when combined with a likelihood function, the resulting posterior distribution belongs to the same family of distributions as the prior. This property simplifies the computation of the posterior distribution. Note: The conjugate prior for a gaussian distribution is gaussian distribution itself. We know that our original likelihood function was, \\[ y|x \\sim \\mathcal{N}(w^Tx , 1) \\] where we are taking \\(\\sigma^2 = 1\\) for convenience , the following derivation will still be valid when variance is just \\(\\sigma^2\\) . Now our prior will be, \\[ w \\sim \\mathcal{N}(0 , \\gamma^2 I) \\] where \\(\\mu \\in \\mathbb{R}^d\\) and \\(\\gamma^2 I \\in \\mathbb{R}^{d \\times d}\\) Now, \\[\\begin{equation*} \\begin{split} P(w| \\{ (x_1 , y_1) , (x_2 , y_2) , .... (x_n , y_n) \\}) &\\propto P(\\{(x_1 , y_1) , (x_2 , y_2) , .... (x_n , y_n) \\}|w) \\times P(w) \\\\ &\\propto \\left(\\prod_{i=1}^{n} e^{-\\frac{(y_i - w^Tx)^2}{2}} \\right) \\times \\left(\\prod_{i=1}^{n} e^{-\\frac{(w_i - 0)^2}{2 \\gamma^2}} \\right) \\\\ &\\propto \\left(\\prod_{i=1}^{n} e^{-\\frac{(y_i - w^Tx)^2}{2}} \\right) \\times e^{- \\frac{||w||^2}{2 \\gamma^2}} \\\\ \\end{split} \\end{equation*}\\] Now how will maximum aposterior (MAP) estimate look like? (Here we are taking log of the above function) \\[\\begin{equation*} \\begin{split} \\hat{W}_{\\text{MAP}} &= \\overset{\\arg}{\\underset{w}{\\max}} \\sum_{i=1}^{n}- \\frac{(y_i - w^Tx_i)^2}{2} - \\frac{||w||^2}{2 \\gamma^2} \\\\ \\hat{W}_{\\text{MAP}} &= \\overset{\\arg}{\\underset{w}{\\min}} \\sum_{i=1}^{n} \\frac{(y_i - w^Tx_i)^2}{2} + \\frac{||w||^2}{2 \\gamma^2} \\\\ \\end{split} \\end{equation*}\\] Taking gradient of this function, \\[\\begin{equation*} \\begin{split} \\nabla f(w) &= (XX^T)w - Xy + \\frac{w}{\\gamma^2} \\\\ \\hat{W}_{\\text{MAP}} &= (XX^T + \\frac{1}{\\gamma^2}I )^{-1}Xy \\end{split} \\end{equation*}\\] We can see that the same estimator can be derived when find out MAP for a gaussian distribution prior.","title":"Bayesian Modeling for Linear Regression"},{"location":"regression/#ridge-regression","text":"Previously our linear regression equation was, \\[ \\hat{w}_{\\text{ML}} = \\underset{w}{\\arg \\min} \\sum_{i=1}^{n} (w^T x_i - y_i)^2 \\] The Ridge Regression equation is given as, \\[ \\hat{w}_{\\text{R}} = \\underset{w}{\\arg \\min} \\sum_{i=1}^{n} (w^T x_i - y_i)^2 + \\lambda ||w||^2 \\] where \\(\\sum_{i=1}^{n} (w^T x_i - y_i)^2\\) is the loss and \\(\\lambda ||w||^2\\) is the regularizer. The regularizer has bayesian viewpoint to it as shown in the above derivation, also it can be thought of as adding a penalty on the overall function for prefering a certain type of \\(w\\) . The higher the weights of the \\(w\\) the larger the penalty , it can also be thought of as prefering \\(w\\) which have their features reduced to zero or almost zero. Note Ridge Regression has more error than linear regression. Ridge Regresison increases the training error so that the model does not overfit.","title":"Ridge Regression"},{"location":"regression/#relation-between-solution-of-linear-regression-and-ridge-regression","text":"Lets say for a dataset we solved the linear regression problem and got \\(\\hat{w}_{\\text{ML}}\\) on a 2-d subspace Now is there any way to find the ridge regression solution for the same dataset? We know that equation of ridge regression is, \\[ \\hat{w}_{\\text{R}} = \\underset{w}{\\arg \\min} \\sum_{i=1}^{n} (w^T x_i - y_i)^2 + \\lambda ||w||^2 \\] It can be argued that this problem/equation is the same as constrained optimization problem, \\[\\begin{equation*} \\begin{split} \\underset{w \\in \\mathbb{R}^d}{\\min} & \\sum_{i=1}^n (w^T x_i - y_i)^2 \\\\ \\\\ \\text{such that}\\\\ & ||w||^2 \\leq \\theta \\\\ \\end{split} \\end{equation*}\\] For every choice of \\(\\lambda > 0\\) , there exists a \\(\\theta\\) such that the optimal solutions of Ridge Regression and Constrained Ridge Regression coincide. In a 2-d space , when \\(||w||^2 \\leq \\theta\\) , it can be written as, \\[\\begin{equation*} \\begin{split} ||w||^2 \\leq \\theta \\\\ \\implies w_1^2 + w_2^2 \\leq \\theta \\\\ \\end{split} \\end{equation*}\\] This is very similar to the equation of a circle whose radius (here) is \\(\\theta\\) and it is centered at origin. From the above image we can see that we have found out a constrained area for \\(\\hat{w}_{\\text{Ridge}}\\) , but where it is exactly? What is the loss/error value of linear regression at \\(\\hat{w}_{\\text{ML}}\\) ? Note The loss calculated at \\(\\hat{w}_{\\text{ML}}\\) is the least when compared to any other \\(w\\) . \\[\\begin{equation*} \\begin{split} \\sum_{i=1}^n (\\hat{w}_{\\text{ML}} x_i - y_i)^2 &= f(\\hat{w}_{\\text{ML}}) \\\\ \\end{split} \\end{equation*}\\] Consider the set of \\(w\\) such that \\[\\begin{equation*} \\begin{split} f(w) &= f( \\hat{w}_{\\text{ML}} ) + c \\;\\;\\;\\;\\;\\; c>0 \\\\ S_c &= \\{ w : f(w) = f( \\hat{w}_{\\text{ML}}) + c \\} \\\\ \\end{split} \\end{equation*}\\] Every vector in \\(S_c\\) satisfies, \\[\\begin{equation*} \\begin{split} \\underset{f(w)}{||X^Tw - y||^2} &= \\underset{f(\\hat{w}_{\\text{ML}})}{|| X^T \\hat{w}_{\\text{ML}} - y ||^2 } + c \\\\ \\\\ \\text{on simplification,}\\\\ (w - \\hat{w}_{\\text{ML}})^T (XX^T) (w - \\hat{w}_{\\text{ML}}) &= c' \\\\ \\end{split} \\end{equation*}\\] If we have \\(\\hat{w}_{\\text{ML}}\\) , \\(XX^T\\) and \\(c'\\) we can get a set of all the \\(w\\) which satisfy the above equation such that they are \\(c'\\) distance away in terms of error when compared to \\(\\hat{w}_{\\text{ML}}\\) . If \\(XX^T = I\\) ( \\(I\\) = Identity Matrix), \\[\\begin{equation*} \\begin{split} (w - \\hat{w}_{\\text{ML}})^T I (w - \\hat{w}_{\\text{ML}}) &= c' \\\\ || w - \\hat{w}_{\\text{ML}} ||^2 = c' \\\\ \\end{split} \\end{equation*}\\] This again corresponds to the equation of a circle (with c' radius) in a 2-d space , but thats only the case when \\(XX^T = I\\) . If \\(XX^T \\neq I\\) then instead of a circle , an ellipse is formed around \\(\\hat{w}_{\\text{ML}}\\) . If we keep increasing the \\(c'\\) while using the same values for \\(\\hat{w}_{\\text{ML}}\\) and \\(XX^T\\) , ellipses with increasing size are formed around \\(\\hat{w}_{\\text{ML}}\\) which eventually touch the circle formed by the constrained ridge regression equation. The point where it touches (yellow dot) is the ridge regression solution with the least loss possible when compared to any other \\(\\hat{w}_{\\text{Ridge}}\\) in the green circle. In other words that point (yellow dot) is closest to \\(\\hat{w}_{\\text{ML}}\\) and yet satisfy the constrained ridge regression equation. Conclusion We can see that Ridge Regression pushes the values in the wieght vector ( \\(w\\) ) to 0 , but does not necessarily make them 0.","title":"Relation Between Solution of Linear Regression and Ridge Regression"},{"location":"regression/#relation-betwen-solution-of-linear-regression-and-lasso-regression","text":"Our goal here is to change the regularizer in ridge regression equation in such a way that the elliptical contours around \\(\\hat{w}_{\\text{ML}}\\) hit at a point where some of the features become zero. We know that ridge regression pushes feature values towards 0. But does not necessarily make it 0. An alternate way to regularize would be to use \\(||\\cdot||_1\\) norm instead of \\(||\\cdot||^2_2\\) norm. \\[ ||w||_1 = \\sum_{i=1}^d | w_i | \\] For L1 Regularization , \\[\\begin{equation*} \\begin{split} & \\underset{w \\in \\mathbb{R}^d}{\\min} \\sum_{i=1}^{n} (w^T x_i - y_i)^2 + \\lambda ||w||_1\\\\ \\\\ & \\text{which is similar to} \\\\ \\\\ & \\underset{w \\in \\mathbb{R}^d}{\\min} \\sum_{i=1}^{n} (w^T x_i - y_i)^2 \\\\ & \\text{such that}\\\\ \\\\ ||w||_1 \\leq \\theta \\\\ \\end{split} \\end{equation*}\\] When compared to L2 Constraint on the regularizer , this is how L1 constraint would look like Now when we keep increasing the size of the elliptical contours around \\(\\hat{w}_{\\text{ML}}\\) our hope is that it touches the point where some of the features of weight vector become zero. When looking at this , one can argue that the elliptical contours will not always touch red area in such a way that one of the weight vectors becomes 0; Which is true when looking at it in a 2d subspace , but in higher dimensions the typical case is when some of weight vectors become 0. This way of using L1 Regularizer is called LASSO (Least Absolute Shrinkage and Selection Operator)","title":"Relation betwen solution of Linear Regression and Lasso Regression"},{"location":"regression/#characteristics-of-lasso-regression","text":"We know now that lasso regression makes certain weight vectors zero , so why not always use lasso? Advantages of using Ridge Regression vs Lasso Regression, Lasso Regression does not have a closed form solution. Sub-gradient methods are usually used to solve for Lasso Regression.","title":"Characteristics of Lasso regression"},{"location":"regression/#what-are-sub-gradients","text":"For a piecewise linear function at the point \\(x\\) (purple) point the function is not differentiable , sub-gradient provide a lower bound for this function at \\(x\\) (purple point). At the blue point the function is differentiable and hence only 1 sub-gradient exists which is the gradient/slope itself. Now , what is the use of sub-gradients in lasso regression? We know that the regularizer in lasso regression takes the absolute values of weight vectors, At the origin (green point) finding the differential is not possible , hence we bound the function using sub-gradients. Any sub-gradient between \\([-1,1]\\) lower bounds the function ( \\(|x|\\) ). Definition of Sub-Gradients A vector \\(g \\in \\mathbb{R}^d\\) is a sub-gradient of \\(f:\\mathbb{R}^D \\to \\mathbb{R}\\) at a point \\(x \\in \\mathbb{R}^d\\) , if \\[\\forall z \\;\\;\\;\\;\\;\\;\\;\\;\\; f(z) \\geq f(x) + g^T(z - x)\\] Whats the use of Sub-Gradients? If function \\(f\\) to minimize is a convex function , then sub-gradient descent converges.","title":"What are Sub-Gradients?"},{"location":"supervised_learning/","text":"Supervised Learning For a bunch of datapoints \\(\\{x_1 , x_2 , .... x_n \\} \\;\\;\\;\\; x_i \\in \\mathbb{R}^d\\) are called features/attributes and \\(\\{y_1 , y_2 , .... y_n \\}\\) corresponding to the datapoints are called the labels. These labels provide \"supervision\" for our algorithms. These labels can take different types of values Binary Classification : Where the labels take only two values and they come from \\(\\{+1 , -1 \\}\\) . Multiclass Classification : Where the labels take multiples values/classes from a set like \\(\\{0,1,2,.....9 \\}\\) . Linear Regression For input/training data \\(\\{x_1 , x_2 , .... x_n \\} \\;\\;\\;\\; x_i \\in \\mathbb{R}^d\\) our goal is to learn a function \\(h : \\mathbb{R}^d \\to \\mathbb{R}\\) which converts a feature to a label. There are many functions which map \\(\\mathbb{R}^d \\to \\mathbb{R}\\) , so how do we measure the \"goodness\" of a function? To measure the error of a function \\[\\text{error}(h) = \\sum_{i=1}^n (h(x_i) - y_i)^2 \\] In the best case scenario , how small can this error be? 0 is the least value the error function can take and it only happens when \\(h(x_i) = y_i \\forall i\\) However this \\(h(x)\\) may not always be the best function for the mapping Some of the problems with \\(h(x)\\) are To achieve 0 error , we always output the same label for each feature , this \\(h(x)\\) \"memorizes\" the mapping from \\(\\mathbb{R}^d\\) to \\(\\mathbb{R}\\) and this function may not always be useful. Functions like this tend to overfit the training data and produce considerable errors on testing data. How to prevent overfitting of training data? Our goal now is to use the same squared error function , but impose a certain structure to reduce our search space. One of the simplest structures we could impose is a linear structure. Now our modified goal is \\[\\underset{h_w \\in H_{\\text{linear}}}{\\min} \\sum_{i=1}^{n} (h_w(x_i) - y_i)^2\\] or equivalently \\[\\underset{w \\in \\mathbb{R}^d}{\\min} \\sum_{i=1}^{n} (w^Tx_i - y_i)^2\\] Optimizing the Error Function Now that we have identified a function for our algorithm , we should think of a way to optimize this function The above function can be rewritten as follows \\[\\begin{equation*} \\begin{split} \\underset{w \\in \\mathbb{R}^d}{\\min} & {||X^Tw - y||}^2 \\\\ \\underset{w \\in \\mathbb{R}^d}{\\min} & (X^Tw - y)^T(X^Tw - y) \\\\ \\end{split} \\end{equation*}\\] The above equation is an unconstrained optimization problem , to minimize the equation we will now take the derivative and equate it to zero. \\[\\begin{equation*} \\begin{split} f(w) &= (X^Tw - y)^T(X^Tw - y) \\\\ \\nabla f(w) &= 2 (XX^T)w - 2(Xy) \\\\ (XX^T)w^* &= Xy \\\\ w^* &= (XX^T)^\\dagger(Xy) \\end{split} \\end{equation*}\\] Geometric Interpretation of Linear Regression Lets say for a dataset with number of features to be 2 ( \\(d=2\\) ) and number of points be 3 ( \\(n=3\\) ). How can we interpret \\(w^* = (XX^T)^\\dagger(Xy)\\) geometrically? Now if we draw an \\(n\\) dimensional space , in our case its \\(n =3\\) , the first vector that we will have will be in \\(\\mathbb{R}^3\\) and the other vector also in \\(\\mathbb{R}^3\\) . Note that we are not plotting the the datapoints but the features themselves. Now we plot the label in the same \\(\\mathbb{R}^3\\) subspace. We can see that the linear combinations of the features (green vectors) will lie in the same plane as the features themselves. Now if we also plot the label vector ( \\(y\\) ) onto the same \\(\\mathbb{R}^3\\) subspace , it may or may not lie in the same plane as of the features themselves. In the case it does not lie in the plane spanned by the features , we will find the closest projection of \\(y\\) onto the plane. We also know that the red point will also be a linear combination of the features as it lies in the same plane as of the features themselves. So for some real numbers like \\(\\alpha^*_1 , \\alpha^*_2\\) , the red point can be expressed as the linear combination of features ( \\(\\alpha^*_1f_1 + \\alpha^*_2f_2\\) ) \\[\\implies \\alpha^*_1f_1 + \\alpha^*_2f_2 = X^T \\alpha^*\\] We also know that \\(X^T\\alpha^*\\) and \\(y - X^T \\alpha^*\\) are orthogonal to each other. \\[\\begin{equation*} \\begin{split} \\implies (X^T \\alpha^*)^T(y - X^T \\alpha^*) &= 0 \\\\ y^TX^T \\alpha^* - {\\alpha^*}^T(XX^T) \\alpha^* \\\\ \\end{split} \\end{equation*}\\] Now what happens if we put \\(w^* = \\alpha^*\\) , where \\(w^* = {(XX^T)}^\\dagger Xy\\) The new equation will be, \\[y^T X^T ((XX^T)^\\dagger Xy) - ((XX^T)^\\dagger Xy)^T (XX^T) (XX^T)^\\dagger Xy = 0\\] With this equation we basically prove that the solution \\(\\alpha^*\\) we were looking for is the same as \\(w^*\\) Gradient Descent We know that \\(w^*\\) has a closed form solution which is \\(w^* = (XX^T)^\\dagger Xy\\) , but it is computationally expensive to compute \\(w^*\\) as it takes \\(O(d^3)\\) iterations. Also, solving for \\(w^*\\) is an unconstrained optimization problem , which can be solved using the method of Gradient Descent. Gradient Descent is an iterative way to find minimizers of functions using just first order information , which is gradient of the function (vector of partial derivatives). \\[ w^{t+1} = w^t - \\eta_t \\nabla f(w^t) \\] The gradient tells you the direction where function will increase , instead , when doing gradient descent we move opposite to the direction of the gradient along the function , where it gradually decreases. \\(\\eta_t\\) is a scalar value and it is the step-size we take to move along the function. \\(f(w^t)\\) gives us the direction of the gradient. After some iterations , we eventually reach the global minima of the function. Our original Mean Squared Error function was, \\[\\begin{equation*} \\begin{split} f(w) &= ||Xw - y||^2 = \\sum_{i=1}^{n} (w^T x_i - y_i)^2 \\\\ \\nabla f(w) &= 2 (XX^T)w - 2Xy \\\\ \\end{split} \\end{equation*}\\] Now we can use this gradient of \\(f(w)\\) in the gradient descent equation, \\[ w^{t+1} = w^t - \\eta_t [2(XX^T)w - 2Xy] \\] This solves the problem of not having to compute the inverse of \\(XX^T\\) , which takes \\(O(d^3)\\) iterations. Using gradient descent to calculate \\(w^*\\) makes it less computationally expensive. Now what to do if \\(n\\) is too large, we know that \\(XX^T\\) is a \\(d \\times n * n * \\times d\\) matrix , just to calculate \\(XX^T\\) there is an inner dependency of \\(n\\) , hence it becomes computationally expensive to solve for \\(XX^T\\) . Is there any way we can avoid computing \\(XX^T\\) ? Stochastic Gradient Descent for \\(t = 1,2,3 ...... T\\) At each step sample a bunch ( \\(k\\) ) of datapoints uniformly at random from the set of all datapoints. Pretend as if this sample ( \\(k\\) datapoints) is the entire dataset and take a gradient step with respect to it, \\[2(\\tilde{X}\\tilde{X}^T w^t - \\tilde{X} \\tilde{y})\\] where \\(\\tilde{X}\\) is the sampled ( \\(k\\) ) datapoints and \\(\\tilde{y}\\) are the labels corresponding to the datapoints. This makes calculating \\(XX^T\\) managable as we only take \\(k\\) points at a time. After \\(t\\) rounds , use \\[ w^T_{\\text{SGD}} = \\frac{1}{T} \\sum_{i=1}^{t} w^t \\] At the end we basically take the average of the \\(w^t\\) obtained after several iterations , though the direction of descent may be noisy at first but in a typical case the average usually gives out the \\(w^*\\) with least possible noise. Stochastic Gradient Descent is always guaranteed to converge to optima with high probability. Kernel Regression Our goal here is to map the data points to a higher dimensional space and then learn a linear model in higher dimension (regressor) without explicitly computing the higher dimensional mappings. The solution for \\(w^*\\) in \\(w^* = (XX^T)^ \\dagger Xy\\) lies in the subspace spanned by the datapoints. It can also be seen as \\(w^*\\) lying in a \\(\\mathbb{R}^3\\) subspace spanned by the datapoints. How? Lets say that there are some data points in \\(\\mathbb{R}^3\\) , even though they are 3-dimensional vectors , lets assume that they all line in some 2-d plane (in our case its the green area above) For arguments sake , lets assume that our \\(w^*\\) lies outside the (green) plane. To minimize the error we will now take a point which is closest to \\(w^*\\) which lies on the same (green) plane. Here , \\(\\tilde{w}\\) is the projection of \\(w^*\\) / closest point to \\(w^*\\) which lies in the same (2-d) space spanned by the datapoints. Now , lets see what's the difference between error functions of \\(w^*\\) and \\(\\tilde{w}\\) \\[\\sum_{i=1}^n ({w^*}^Tx_i - y_i) \\;\\;\\;\\;\\; \\text{and} \\;\\;\\;\\;\\; \\sum_{i=1}^n ({\\tilde{w}}^Tx_i - y_i)\\] \\(w^*\\) can be written as the sum of \\(\\tilde{w}\\) and the vector perpendicular to \\(\\tilde{w}\\) , which is \\(\\tilde{w}_{\\perp}\\) Note that \\(\\tilde{w}_{\\perp}\\) is perpendicular to the (2-d) plane itself , which means it is perpendicular / orthogonal to all the points which lie in the plane (datapoints). \\[\\begin{equation*} \\begin{split} w^* &= \\tilde{w} + \\tilde{w}_{\\perp} \\\\ {w^*}^Tx_i &= {(\\tilde{w} + \\tilde{w}_{\\perp} )}^T x_i \\\\ &= \\tilde{w} x_i + \\tilde{w}_{\\perp}^T x_i \\\\ &= \\tilde{w} x_i \\;\\;\\;\\;\\;\\; \\forall i \\\\ \\end{split} \\end{equation*}\\] \\(\\tilde{w}_{\\perp}^T x_i\\) will become 0 as explained above. Now we see that the error for both \\(w^*\\) and \\(\\tilde{w}\\) is exactly the same. It can also be seen that \\(w^*\\) is some combination of the datapoints, which can be written as \\[w^* = X \\alpha^*\\] for some \\(\\alpha^* \\in \\mathbb{R}^n\\) We also know that , \\[\\begin{equation*} \\begin{split} w^* &= (XX^T)^\\dagger Xy \\\\ X \\alpha^* &= (XX^T)^\\dagger Xy \\\\ (XX^T) X \\alpha^* &= (XX^T)(XX^T)^\\dagger Xy \\\\ (XX^T) X \\alpha^* &= Xy \\\\ X^T(XX^T) X \\alpha^* &= X^TXy \\\\ (X^T X)^2 \\alpha^* &= X^TXy \\\\ K^2 \\alpha^* &= Ky \\\\ \\alpha^* &= K^{-1}y \\\\ \\end{split} \\end{equation*}\\] \\(K = X^T X\\) , is the kernel matrix. Prediction We know that, \\[ w = \\sum_{i=1}^n \\alpha_i x_i \\] Also Prediction \\(= w^T x_{\\text{test}}\\) , \\[\\begin{equation*} \\begin{split} w^T x_\\text{test} &= (\\sum_{i=1}^n \\alpha_i x_i )^T x_\\text{test} \\\\ &= \\sum_{i=1}^n \\alpha_i (x_i^T x_\\text{test}) \\\\ &= \\sum_{i=1}^n ( \\phi(x_i)^T \\phi(x_\\text{test}) ) \\\\ &= \\sum_{i=1}^n \\alpha_i K (x_i , x_\\text{test}) \\\\ \\end{split} \\end{equation*}\\] where \\(\\alpha_i\\) shows how important is \\(i^\\text{th}\\) point towards \\(w^*\\) and \\(K(x_i , x_\\text{test})\\) shows how similar is \\(x_\\text{test}\\) to \\(x_i\\) . Probabilistic view of Linear Regression In a linear regression problem we know that , \\(x \\in \\mathbb{R}^d\\) , \\(y \\in \\mathbb{R}\\) for a set of datapoints \\(\\{ (x_1 , y_1) , (x_2 , y_2) , ..... (x_n , y_n) \\}\\) . The probabilistic we are going to assume is as follows, \\[ y|x \\sim w^T x + \\epsilon \\] For a given feature theres is an unknown but fixed \\(w \\in \\mathbb{R}^d\\) and \\(\\epsilon\\) is a zero-mean gaussian ( \\(\\mathcal{N}(0 , \\sigma^2)\\) )noise. Now we can view this as an estimation problem and solve it using the maximum likelihood approach. The likelihood function will be , \\[\\begin{equation*} \\begin{split} \\mathcal{L}(w ; \\substack{x_1 , x_2 , ... x_n \\\\ y_1 , y_2 , ... y_n} ) &= \\prod_{i=1}^n e^{- \\frac{(w^Tx_i - y_i)^2}{2 \\sigma^2}} \\times \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\\\ \\log \\mathcal{L}(w ; \\substack{x_1 , x_2 , ... x_n \\\\ y_1 , y_2 , ... y_n} ) &= \\sum_{i=1}^n \\frac{-(w^Tx_i - y_i)^2}{2 \\sigma^2} \\times \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\\\ \\\\ \\text{equivalently} \\\\ \\\\ &= \\underset{w}{\\max} \\sum_{i=1}^{n} - (w^T x_i - y_i)^2 \\\\ &= \\underset{w}{\\min} \\sum_{i=1}^n (w^Tx_i - y_i)^2 \\\\ \\end{split} \\end{equation*}\\] Note that the mean of the distribution becomes \\(w^Tx + 0\\) as \\(\\epsilon\\) is a zero-mean gaussian distribution , which makes the final distribution to be \\(\\mathcal{N}(w^Tx , \\sigma^2)\\) Also, we ignored the constants in the later half of the derivation because they are, you guessed it , constants. :p Finally from this we can conclude that \\(w^* = w_\\text{ML} = (XX^T)^\\dagger Xy\\)","title":"Supervised Learning"},{"location":"supervised_learning/#supervised-learning","text":"For a bunch of datapoints \\(\\{x_1 , x_2 , .... x_n \\} \\;\\;\\;\\; x_i \\in \\mathbb{R}^d\\) are called features/attributes and \\(\\{y_1 , y_2 , .... y_n \\}\\) corresponding to the datapoints are called the labels. These labels provide \"supervision\" for our algorithms. These labels can take different types of values Binary Classification : Where the labels take only two values and they come from \\(\\{+1 , -1 \\}\\) . Multiclass Classification : Where the labels take multiples values/classes from a set like \\(\\{0,1,2,.....9 \\}\\) .","title":"Supervised Learning"},{"location":"supervised_learning/#linear-regression","text":"For input/training data \\(\\{x_1 , x_2 , .... x_n \\} \\;\\;\\;\\; x_i \\in \\mathbb{R}^d\\) our goal is to learn a function \\(h : \\mathbb{R}^d \\to \\mathbb{R}\\) which converts a feature to a label. There are many functions which map \\(\\mathbb{R}^d \\to \\mathbb{R}\\) , so how do we measure the \"goodness\" of a function? To measure the error of a function \\[\\text{error}(h) = \\sum_{i=1}^n (h(x_i) - y_i)^2 \\] In the best case scenario , how small can this error be? 0 is the least value the error function can take and it only happens when \\(h(x_i) = y_i \\forall i\\) However this \\(h(x)\\) may not always be the best function for the mapping Some of the problems with \\(h(x)\\) are To achieve 0 error , we always output the same label for each feature , this \\(h(x)\\) \"memorizes\" the mapping from \\(\\mathbb{R}^d\\) to \\(\\mathbb{R}\\) and this function may not always be useful. Functions like this tend to overfit the training data and produce considerable errors on testing data. How to prevent overfitting of training data? Our goal now is to use the same squared error function , but impose a certain structure to reduce our search space. One of the simplest structures we could impose is a linear structure. Now our modified goal is \\[\\underset{h_w \\in H_{\\text{linear}}}{\\min} \\sum_{i=1}^{n} (h_w(x_i) - y_i)^2\\] or equivalently \\[\\underset{w \\in \\mathbb{R}^d}{\\min} \\sum_{i=1}^{n} (w^Tx_i - y_i)^2\\]","title":"Linear Regression"},{"location":"supervised_learning/#optimizing-the-error-function","text":"Now that we have identified a function for our algorithm , we should think of a way to optimize this function The above function can be rewritten as follows \\[\\begin{equation*} \\begin{split} \\underset{w \\in \\mathbb{R}^d}{\\min} & {||X^Tw - y||}^2 \\\\ \\underset{w \\in \\mathbb{R}^d}{\\min} & (X^Tw - y)^T(X^Tw - y) \\\\ \\end{split} \\end{equation*}\\] The above equation is an unconstrained optimization problem , to minimize the equation we will now take the derivative and equate it to zero. \\[\\begin{equation*} \\begin{split} f(w) &= (X^Tw - y)^T(X^Tw - y) \\\\ \\nabla f(w) &= 2 (XX^T)w - 2(Xy) \\\\ (XX^T)w^* &= Xy \\\\ w^* &= (XX^T)^\\dagger(Xy) \\end{split} \\end{equation*}\\]","title":"Optimizing the Error Function"},{"location":"supervised_learning/#geometric-interpretation-of-linear-regression","text":"Lets say for a dataset with number of features to be 2 ( \\(d=2\\) ) and number of points be 3 ( \\(n=3\\) ). How can we interpret \\(w^* = (XX^T)^\\dagger(Xy)\\) geometrically? Now if we draw an \\(n\\) dimensional space , in our case its \\(n =3\\) , the first vector that we will have will be in \\(\\mathbb{R}^3\\) and the other vector also in \\(\\mathbb{R}^3\\) . Note that we are not plotting the the datapoints but the features themselves. Now we plot the label in the same \\(\\mathbb{R}^3\\) subspace. We can see that the linear combinations of the features (green vectors) will lie in the same plane as the features themselves. Now if we also plot the label vector ( \\(y\\) ) onto the same \\(\\mathbb{R}^3\\) subspace , it may or may not lie in the same plane as of the features themselves. In the case it does not lie in the plane spanned by the features , we will find the closest projection of \\(y\\) onto the plane. We also know that the red point will also be a linear combination of the features as it lies in the same plane as of the features themselves. So for some real numbers like \\(\\alpha^*_1 , \\alpha^*_2\\) , the red point can be expressed as the linear combination of features ( \\(\\alpha^*_1f_1 + \\alpha^*_2f_2\\) ) \\[\\implies \\alpha^*_1f_1 + \\alpha^*_2f_2 = X^T \\alpha^*\\] We also know that \\(X^T\\alpha^*\\) and \\(y - X^T \\alpha^*\\) are orthogonal to each other. \\[\\begin{equation*} \\begin{split} \\implies (X^T \\alpha^*)^T(y - X^T \\alpha^*) &= 0 \\\\ y^TX^T \\alpha^* - {\\alpha^*}^T(XX^T) \\alpha^* \\\\ \\end{split} \\end{equation*}\\] Now what happens if we put \\(w^* = \\alpha^*\\) , where \\(w^* = {(XX^T)}^\\dagger Xy\\) The new equation will be, \\[y^T X^T ((XX^T)^\\dagger Xy) - ((XX^T)^\\dagger Xy)^T (XX^T) (XX^T)^\\dagger Xy = 0\\] With this equation we basically prove that the solution \\(\\alpha^*\\) we were looking for is the same as \\(w^*\\)","title":"Geometric Interpretation of Linear Regression"},{"location":"supervised_learning/#gradient-descent","text":"We know that \\(w^*\\) has a closed form solution which is \\(w^* = (XX^T)^\\dagger Xy\\) , but it is computationally expensive to compute \\(w^*\\) as it takes \\(O(d^3)\\) iterations. Also, solving for \\(w^*\\) is an unconstrained optimization problem , which can be solved using the method of Gradient Descent. Gradient Descent is an iterative way to find minimizers of functions using just first order information , which is gradient of the function (vector of partial derivatives). \\[ w^{t+1} = w^t - \\eta_t \\nabla f(w^t) \\] The gradient tells you the direction where function will increase , instead , when doing gradient descent we move opposite to the direction of the gradient along the function , where it gradually decreases. \\(\\eta_t\\) is a scalar value and it is the step-size we take to move along the function. \\(f(w^t)\\) gives us the direction of the gradient. After some iterations , we eventually reach the global minima of the function. Our original Mean Squared Error function was, \\[\\begin{equation*} \\begin{split} f(w) &= ||Xw - y||^2 = \\sum_{i=1}^{n} (w^T x_i - y_i)^2 \\\\ \\nabla f(w) &= 2 (XX^T)w - 2Xy \\\\ \\end{split} \\end{equation*}\\] Now we can use this gradient of \\(f(w)\\) in the gradient descent equation, \\[ w^{t+1} = w^t - \\eta_t [2(XX^T)w - 2Xy] \\] This solves the problem of not having to compute the inverse of \\(XX^T\\) , which takes \\(O(d^3)\\) iterations. Using gradient descent to calculate \\(w^*\\) makes it less computationally expensive. Now what to do if \\(n\\) is too large, we know that \\(XX^T\\) is a \\(d \\times n * n * \\times d\\) matrix , just to calculate \\(XX^T\\) there is an inner dependency of \\(n\\) , hence it becomes computationally expensive to solve for \\(XX^T\\) . Is there any way we can avoid computing \\(XX^T\\) ?","title":"Gradient Descent"},{"location":"supervised_learning/#stochastic-gradient-descent","text":"for \\(t = 1,2,3 ...... T\\) At each step sample a bunch ( \\(k\\) ) of datapoints uniformly at random from the set of all datapoints. Pretend as if this sample ( \\(k\\) datapoints) is the entire dataset and take a gradient step with respect to it, \\[2(\\tilde{X}\\tilde{X}^T w^t - \\tilde{X} \\tilde{y})\\] where \\(\\tilde{X}\\) is the sampled ( \\(k\\) ) datapoints and \\(\\tilde{y}\\) are the labels corresponding to the datapoints. This makes calculating \\(XX^T\\) managable as we only take \\(k\\) points at a time. After \\(t\\) rounds , use \\[ w^T_{\\text{SGD}} = \\frac{1}{T} \\sum_{i=1}^{t} w^t \\] At the end we basically take the average of the \\(w^t\\) obtained after several iterations , though the direction of descent may be noisy at first but in a typical case the average usually gives out the \\(w^*\\) with least possible noise. Stochastic Gradient Descent is always guaranteed to converge to optima with high probability.","title":"Stochastic Gradient Descent"},{"location":"supervised_learning/#kernel-regression","text":"Our goal here is to map the data points to a higher dimensional space and then learn a linear model in higher dimension (regressor) without explicitly computing the higher dimensional mappings. The solution for \\(w^*\\) in \\(w^* = (XX^T)^ \\dagger Xy\\) lies in the subspace spanned by the datapoints. It can also be seen as \\(w^*\\) lying in a \\(\\mathbb{R}^3\\) subspace spanned by the datapoints. How? Lets say that there are some data points in \\(\\mathbb{R}^3\\) , even though they are 3-dimensional vectors , lets assume that they all line in some 2-d plane (in our case its the green area above) For arguments sake , lets assume that our \\(w^*\\) lies outside the (green) plane. To minimize the error we will now take a point which is closest to \\(w^*\\) which lies on the same (green) plane. Here , \\(\\tilde{w}\\) is the projection of \\(w^*\\) / closest point to \\(w^*\\) which lies in the same (2-d) space spanned by the datapoints. Now , lets see what's the difference between error functions of \\(w^*\\) and \\(\\tilde{w}\\) \\[\\sum_{i=1}^n ({w^*}^Tx_i - y_i) \\;\\;\\;\\;\\; \\text{and} \\;\\;\\;\\;\\; \\sum_{i=1}^n ({\\tilde{w}}^Tx_i - y_i)\\] \\(w^*\\) can be written as the sum of \\(\\tilde{w}\\) and the vector perpendicular to \\(\\tilde{w}\\) , which is \\(\\tilde{w}_{\\perp}\\) Note that \\(\\tilde{w}_{\\perp}\\) is perpendicular to the (2-d) plane itself , which means it is perpendicular / orthogonal to all the points which lie in the plane (datapoints). \\[\\begin{equation*} \\begin{split} w^* &= \\tilde{w} + \\tilde{w}_{\\perp} \\\\ {w^*}^Tx_i &= {(\\tilde{w} + \\tilde{w}_{\\perp} )}^T x_i \\\\ &= \\tilde{w} x_i + \\tilde{w}_{\\perp}^T x_i \\\\ &= \\tilde{w} x_i \\;\\;\\;\\;\\;\\; \\forall i \\\\ \\end{split} \\end{equation*}\\] \\(\\tilde{w}_{\\perp}^T x_i\\) will become 0 as explained above. Now we see that the error for both \\(w^*\\) and \\(\\tilde{w}\\) is exactly the same. It can also be seen that \\(w^*\\) is some combination of the datapoints, which can be written as \\[w^* = X \\alpha^*\\] for some \\(\\alpha^* \\in \\mathbb{R}^n\\) We also know that , \\[\\begin{equation*} \\begin{split} w^* &= (XX^T)^\\dagger Xy \\\\ X \\alpha^* &= (XX^T)^\\dagger Xy \\\\ (XX^T) X \\alpha^* &= (XX^T)(XX^T)^\\dagger Xy \\\\ (XX^T) X \\alpha^* &= Xy \\\\ X^T(XX^T) X \\alpha^* &= X^TXy \\\\ (X^T X)^2 \\alpha^* &= X^TXy \\\\ K^2 \\alpha^* &= Ky \\\\ \\alpha^* &= K^{-1}y \\\\ \\end{split} \\end{equation*}\\] \\(K = X^T X\\) , is the kernel matrix.","title":"Kernel Regression"},{"location":"supervised_learning/#prediction","text":"We know that, \\[ w = \\sum_{i=1}^n \\alpha_i x_i \\] Also Prediction \\(= w^T x_{\\text{test}}\\) , \\[\\begin{equation*} \\begin{split} w^T x_\\text{test} &= (\\sum_{i=1}^n \\alpha_i x_i )^T x_\\text{test} \\\\ &= \\sum_{i=1}^n \\alpha_i (x_i^T x_\\text{test}) \\\\ &= \\sum_{i=1}^n ( \\phi(x_i)^T \\phi(x_\\text{test}) ) \\\\ &= \\sum_{i=1}^n \\alpha_i K (x_i , x_\\text{test}) \\\\ \\end{split} \\end{equation*}\\] where \\(\\alpha_i\\) shows how important is \\(i^\\text{th}\\) point towards \\(w^*\\) and \\(K(x_i , x_\\text{test})\\) shows how similar is \\(x_\\text{test}\\) to \\(x_i\\) .","title":"Prediction"},{"location":"supervised_learning/#probabilistic-view-of-linear-regression","text":"In a linear regression problem we know that , \\(x \\in \\mathbb{R}^d\\) , \\(y \\in \\mathbb{R}\\) for a set of datapoints \\(\\{ (x_1 , y_1) , (x_2 , y_2) , ..... (x_n , y_n) \\}\\) . The probabilistic we are going to assume is as follows, \\[ y|x \\sim w^T x + \\epsilon \\] For a given feature theres is an unknown but fixed \\(w \\in \\mathbb{R}^d\\) and \\(\\epsilon\\) is a zero-mean gaussian ( \\(\\mathcal{N}(0 , \\sigma^2)\\) )noise. Now we can view this as an estimation problem and solve it using the maximum likelihood approach. The likelihood function will be , \\[\\begin{equation*} \\begin{split} \\mathcal{L}(w ; \\substack{x_1 , x_2 , ... x_n \\\\ y_1 , y_2 , ... y_n} ) &= \\prod_{i=1}^n e^{- \\frac{(w^Tx_i - y_i)^2}{2 \\sigma^2}} \\times \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\\\ \\log \\mathcal{L}(w ; \\substack{x_1 , x_2 , ... x_n \\\\ y_1 , y_2 , ... y_n} ) &= \\sum_{i=1}^n \\frac{-(w^Tx_i - y_i)^2}{2 \\sigma^2} \\times \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\\\ \\\\ \\text{equivalently} \\\\ \\\\ &= \\underset{w}{\\max} \\sum_{i=1}^{n} - (w^T x_i - y_i)^2 \\\\ &= \\underset{w}{\\min} \\sum_{i=1}^n (w^Tx_i - y_i)^2 \\\\ \\end{split} \\end{equation*}\\] Note that the mean of the distribution becomes \\(w^Tx + 0\\) as \\(\\epsilon\\) is a zero-mean gaussian distribution , which makes the final distribution to be \\(\\mathcal{N}(w^Tx , \\sigma^2)\\) Also, we ignored the constants in the later half of the derivation because they are, you guessed it , constants. :p Finally from this we can conclude that \\(w^* = w_\\text{ML} = (XX^T)^\\dagger Xy\\)","title":"Probabilistic view of Linear Regression"},{"location":"img/Representation%20Learning/","text":"Principal Component Analysis (PCA) Introduction Unsupervised learning, specifically \"representation learning,\" is a subset of machine learning where the goal is to automatically discover meaningful representations or features from raw data without explicit supervision or labeled examples. In representation learning, the algorithms aim to capture the underlying structure, patterns, or features within the data itself. This can be highly valuable for various tasks like data compression, feature extraction, data visualization, and even for improving the performance of other machine learning models. Representation Learning (1) The main objective of representation learning is to transform the input data into a more meaningful and compact representation. This representation should capture the essential characteristics of the data, making it easier to analyze or use in downstream tasks. What is the need for compression of data points? Compressing a dataset, especially in the context of unsupervised learning or data preprocessing, can serve several important purposes and provide various benefits , mainly: Reduced Storage Space : Large datasets can require significant storage space. Compressing the dataset reduces storage requirements, which can be cost-effective, especially when dealing with massive datasets in cloud storage or on limited storage devices. Faster Data Transfer : Smaller datasets transfer more quickly over networks, which is crucial when moving data between systems or uploading/downloading data from the internet. Faster Training : When working with machine learning models, smaller datasets can lead to faster training times. This is particularly important when experimenting with different models, hyperparameters, or architectures. How to Compress Data Points? Lets say there are 4 data points \\(\\left\\{ \\stackrel{x_1}{\\begin{bmatrix} -7 \\\\ -14 \\end{bmatrix}} , \\stackrel{x_2}{\\begin{bmatrix} 2.5 \\\\ 5 \\end{bmatrix}} , \\stackrel{x_3}{\\begin{bmatrix} 0.5 \\\\ 1 \\end{bmatrix}} , \\stackrel{x_4}{\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}} \\right\\}\\) Now one might ask , \"how many data points are needed to store this dataset?\" The naive answer would be to say , \"as there are 8 data points , hence 8 real numbers are required to store this dataset.\" A better way to represent this dataset would be to find a \"function\" which takes 1 real number and outputs a matrix of 2 real numbers. If we look at the dataset we can see that the first coordinate is half of the second coordinate. We can exploit this feature of the dataset to reduce the number of data points to be stored. One way to form this \"function\" would be to get a \"representative\" \\(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) and \"coefficients\" \\(\\{ -7 , 2.5 , 0.5 ,1 \\}\\) . Now we can get the back our dataset by multiplying them with the coefficients. Using this way we only need to store 6 real numbers (4 for coefficients + 2 for representative). Similarly, on a dataset of \\(2n\\) points , we can store them as \\(n +2\\) real numbers ( \\(n\\) coefficients + 2 for representative). Working with a Realistic Dataset In the above example we were able construct a function which reduced the number of real numbers required to represent the dataset , but in a realistic dataset all the points do not lie on the same line , they are scattered and its very hard to derive meaningful conclusions from them. \"What to do if all the points dont lie in the dataset?\" A simple answer would be, \"We can get more representatives to accommodate the points which do not lie on the same line.\" This approach does accommodate the outliers but it also increases the number of real numbers required to represent the dataset. \"Whats the solution then?\" There has to be a tradeoff between the \"accuracy\" or the \"size\" of the dataset. If we want to reduce the size of the \"size\" then approximating the outlier to the \"line\" with least lost would be the best bet. To represent outliers and reduce the size of the dataset at the same time , we must project the outlier onto the line. Let there be a vector \\(\\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}\\) which represents the \"line\". The projection of the vector \\(\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\) onto the line would be \\[ \\left[ \\frac{x_1w_1 + x_2w_2}{w_1^2 + w_2^2} \\right] \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\] This will give us a point on the \"line\" with the least loss/distance. Now if we pick vector \\((w_1 , w_2)\\) which lies on the \"line\" such that \\(w_1^2 + w_2^2 = 1\\) , the above equation becomes \\[ \\left[ {x_1w_1 + x_2w_2} \\right] \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\] Representation Learning (2) Our original objective was to find a \"compressed\" representation of the data when all the data-points not necessarily fall on the same line. \"How do we know that the line we picked is the best line?\" One could argue that the same line doesnt fit all the data-points with the \" least loss \" , where \" loss \" is the average of all the \" errors \" when projecting outliers onto the given \" line \". How to find the Best Line? To find the Best Line we should first have a certain way to compare 2 different lines. In our case it would be the line with the least \" reconstruction error \". Lets say for a dataset \\(\\{ x_1 , x_2 , .... x_n \\}\\) , where \\(x_i \\in \\mathbb{R}^d\\) \\[\\begin{equation*} \\begin{split} \\text{Error}(\\text{line} , \\text{dataset}) & = \\sum_{i=1}^n \\text{error}(\\text{line} , x_i) \\\\ & = \\sum_{i=1}^n \\text{length}^2(x - (x^Tw)w) \\\\ & = \\sum_{i=1}^n || x - (x^Tw).w||^2 \\\\ \\end{split} \\end{equation*}\\] To minimize the above equation we can think of it as a function \\(f(W)\\) \\[\\begin{equation*} \\begin{split} f(W) &= \\frac{1}{n}\\sum_{i=1}^{n} || x - (x^Tw).w ||^2 \\\\ &= \\frac{1}{n}\\sum_{i=1}^{n} (x - (x^Tw).w)^T(x - (x^Tw).w) \\\\ &= \\frac{1}{n}\\sum_{i=1}^{n} \\left[ x_i^Tx_i - (x_i^Tw)^2 - (x_i^Tw)^2 + (x_i^Tw)^2(1) \\right] \\\\ &= \\frac{1}{n}\\sum_{i=1}^{n} \\left( x_i^Tx_i - (x_i^Tw)^2 \\right) \\end{split} \\end{equation*}\\] Here we are minimzing \\(f(W)\\) with respect to \\(w\\) , the first term of the above equation \\(x_i^Tx_i\\) can be ignored as its a constant. So we can write the new function as \\(g(W)\\) \\[\\begin{equation*} \\begin{split} \\min_{W_{||w||^2 = 1}} g(W) &= \\frac{1}{n}\\sum_{i=1}^n - (x_i^Tw)^2 \\end{split} \\end{equation*}\\] Alternatively this same function can be written as \\[\\begin{equation*} \\begin{split} \\max_{W_{||w||^2 = 1}} g(W) = \\frac{1}{n}\\sum_{i=1}^n (x_i^Tw)^2 &= \\frac{1}{n} \\sum_{i=1}^{n} (w^Tx_i)(x_i^Tw) \\\\ &= \\frac{1}{n} \\sum_{i=1}^{n} w^T(x_ix_i^T)w \\\\ &= \\frac{1}{n} w^T(\\sum_{i=1}^{n} x_ix_i^T)w \\\\ \\end{split} \\end{equation*}\\] The above equation can also be written as \\[ \\max_{w_{||w||^2 = 1}} w^TCw \\] where \\(C = \\frac{1}{n}\\sum_{i=1}^{n}x_ix_i^T\\) Note that \\(C\\) is also the covariance matrix and the solution for the above maximization equation would be the eigenvector corresponding to the largest eignevalue of \\(C\\) . Error Vector has Information Our hypothesis was that there is a line which best represents the data but , if all the data points lied along a plane then this part (dotted orange lines) which we are imagining to be error may not be the error but rather useful information because the necessary information , the structure is in a plane but not on a line , so the bits we lose while selecting the best line will also contain some information. How do we extract this information? One possible algorithm could be , Input : \\(\\{ x_1 , x_2 , x_3 ...... x_n \\} x_i \\in \\mathbb{R}^d\\) Find the \" best \" line \\(w_1 \\in \\mathbb{R}^d\\) Replace \\(x_i\\) with \\(x_i - (x_i^Tw)w\\) Repeat to obtain \\(w_2\\) Note Sometimes the data might not be centered around the origin. To counter that we can subtract the average of the dataset from the points. Principal Component Analysis (1) From the algorithm above we can find a \\(w_2\\) vector which represents the line which passes through the \"error/residues\" generated while finding out \\(w_1\\) . Observations made while finding out \\(\\mathbf{w_2}\\) All \"residues/errors\" are orthogonal to \\(w_1\\) . Any line which minimizes sum of errors w.r.t. residues must also be orthogonal to \\(w_1\\) A question which comes to mind is , \"is there a relationship between \\(w_1\\) and \\(w_2\\) ?\" The answer is \"Yes , there is a relationship.\" \\(w_1\\) and \\(w_2\\) are orthognal to each other. \\[\\implies w_1^Tw_2 = 0\\] By continuing this procedure for dataset of \\(d\\) dimension, we get a set of vectors \\(\\{ w_1 , w_2 , w_3 .... w_d \\}\\) with the following properties. \\(||w_k||^2 = 1 \\; \\; \\; \\forall k\\) \\(w_iw_j = 0 \\;\\;\\;\\;\\; \\forall i \\neq j\\) Hence , we get a set of Orthonormal Vectors. What is the use of Set \\(D\\) of Orthonormal Vectors? Residue after round 1 is \\(\\left\\{ x_1 - (x_1^Tw_1)w_1 , ..... , x_n - (x_n^Tw_1)w_1 \\right\\} \\;\\;\\;\\; \\forall \\;\\; \\text{vectors} \\in \\mathbb{R}^d\\) Residue after round 2 is \\(\\{ x_1 - (x_1^Tw_1)w_1 - (x_1 - (x_1^Tw_1)w_1)^Tw_2 , ..... \\}\\) \\(\\{ x_1 - (x_1^Tw_1)w_1 - (x_1^Tw_2 - (x_1^Tw_1).w_1^Tw_2)w_2 , ...... \\}\\) \\(\\{ x_1 - (x_1^Tw_1)w_1 - (x_1^Tw_2)w_2 , ...... \\}\\) as \\([w_1^Tw_2 = 0]\\) Residue after \\(d\\) rounds is \\(\\forall i \\;\\;\\;\\;\\;\\; x_i - ((x_i^Tw_1)w_1 + (x_i^Tw_2)w_2 + ...... (x_i^Tw_d)w_d) = 0\\) After \\(d\\) rounds all the error vectors become zero vectors. \\(\\forall i \\;\\;\\;\\;\\;\\; x_i = ((x_i^Tw_1)w_1 + (x_i^Tw_2)w_2 + ...... (x_i^Tw_d)w_d)\\) This leads to the conclusion that if data lives in a \"low\" dimensional linear sub-space, then residue becomes 0 much earlier than \\(d\\) rounds. What does the above statement actually mean? Lets say for a dataset \\(\\{x_1 , x_2 \\cdots , x_n \\}\\) , where \\(x_i \\in \\mathbb{R}^d\\) , all the residues/error vectors become 0 after 3 rounds. This means that every datapoint can be expressed as the sum of projections itself onto the residues/error vectors. \\[ \\forall i \\;\\; x_i = (x_i^Tw_1)w_1 + (x_i^Tw_2)w_2 + (x_i^Tw_3)w_3 \\] where \\(\\{w_1 ,w_2 ,w_3 \\} \\in \\mathbb{R}^d\\) Note that the \\(\\{w_1 ,w_2 ,w_3 \\}\\) are the representatives and \\(\\{x_i^T w_1 , x_i^T w_2 , x_i^T w_3 \\}\\) are the coefficients for a datapoint \\(x_i\\) Example Lets assume for a dataset of dimension \\(d\\) and \\(n\\) points , after \\(k\\) rounds the error vectors become zero vectors. This means that now the dataset can be represented with \\(\\mathbf{d \\times k + k \\times n}\\) points instead of \\(d \\times n\\) points. In the case shown above where the residues/error vectors become 0 after \\(k = 3\\) rounds, we can represent the whole dataset as, \\(d \\times 3\\) + \\(3 \\times n\\) Where , \\(d \\times 3\\) = Total numbers required to store the representatives \\(3 \\times n\\) = Total numbers required to store datapoints Principal Component Analysis (2) Our original problem was \\[ \\max_{w_{||w||^2 = 1}} w^TCw \\] where \\(C = \\frac{1}{n}\\sum_{i=1}^{n}x_ix_i^T\\) , and the solution for this problem was the eigenvector corresponding to the maximum eigenvalue . We also observed that the set of eigenvectors \\((\\{w_1 , w_2 , .... w_d\\})\\) coresspnding to the eigenvalues of \\(C\\) form an orthonormal basis. Now lets look at this problem in a more \"linear algebraic\" way, We know, \\[\\begin{equation*} \\begin{split} Cw_1 &= \\lambda_1 w_1 \\\\ w_1^TCw_1 &= w_1^T(\\lambda_1w_1) = \\lambda_1 \\\\ \\lambda_1 &= w_1^TCw_1 = w_1^T(\\frac{1}{n}\\sum_{i=1}^{n}x_ix_i^T)w_1 \\\\ \\lambda_1 &= \\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw_1)^2 \\end{split} \\end{equation*}\\] Usually we take highest \\(L\\) lambdas such that 95% of the variance in the dataset is captured, \\[\\frac{\\sum_{i=1}^L \\lambda_i }{ \\sum_{i=1}^d \\lambda_i } \\geq 0.95\\] where , \\(\\lambda_i\\) are the eigenvalues of the covariance matrix . Relation between Variance and \\(\\mathbf{\\lambda}\\) For an arbitrary set of points \\((\\{(x_1^Tw) , (x_2^Tw) ...... (x_n^T)w \\})\\) projected onto line represented by vector \\(w\\) . The average \\(\\mu\\) of the projected points will be \\(\\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw)\\) . If the data is centered then, \\[\\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw) = (\\frac{1}{n}\\sum_{i=1}^{n}x_i)w = 0w = 0\\] This implies that average for a centered dataset is \\(\\mu = 0\\) . The variance of this same dataset will be , \\[\\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw - \\mu)^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw)^2\\] We can see that the variance is same as \\(\\lambda\\) required to solve the maximization problem. Hence we can say that , variance maximization is the same as error minimization on centered dataset","title":"Principal Component Analysis (PCA)"},{"location":"img/Representation%20Learning/#principal-component-analysis-pca","text":"","title":"Principal Component Analysis (PCA)"},{"location":"img/Representation%20Learning/#introduction","text":"Unsupervised learning, specifically \"representation learning,\" is a subset of machine learning where the goal is to automatically discover meaningful representations or features from raw data without explicit supervision or labeled examples. In representation learning, the algorithms aim to capture the underlying structure, patterns, or features within the data itself. This can be highly valuable for various tasks like data compression, feature extraction, data visualization, and even for improving the performance of other machine learning models.","title":"Introduction"},{"location":"img/Representation%20Learning/#representation-learning-1","text":"The main objective of representation learning is to transform the input data into a more meaningful and compact representation. This representation should capture the essential characteristics of the data, making it easier to analyze or use in downstream tasks. What is the need for compression of data points? Compressing a dataset, especially in the context of unsupervised learning or data preprocessing, can serve several important purposes and provide various benefits , mainly: Reduced Storage Space : Large datasets can require significant storage space. Compressing the dataset reduces storage requirements, which can be cost-effective, especially when dealing with massive datasets in cloud storage or on limited storage devices. Faster Data Transfer : Smaller datasets transfer more quickly over networks, which is crucial when moving data between systems or uploading/downloading data from the internet. Faster Training : When working with machine learning models, smaller datasets can lead to faster training times. This is particularly important when experimenting with different models, hyperparameters, or architectures. How to Compress Data Points? Lets say there are 4 data points \\(\\left\\{ \\stackrel{x_1}{\\begin{bmatrix} -7 \\\\ -14 \\end{bmatrix}} , \\stackrel{x_2}{\\begin{bmatrix} 2.5 \\\\ 5 \\end{bmatrix}} , \\stackrel{x_3}{\\begin{bmatrix} 0.5 \\\\ 1 \\end{bmatrix}} , \\stackrel{x_4}{\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}} \\right\\}\\) Now one might ask , \"how many data points are needed to store this dataset?\" The naive answer would be to say , \"as there are 8 data points , hence 8 real numbers are required to store this dataset.\" A better way to represent this dataset would be to find a \"function\" which takes 1 real number and outputs a matrix of 2 real numbers. If we look at the dataset we can see that the first coordinate is half of the second coordinate. We can exploit this feature of the dataset to reduce the number of data points to be stored. One way to form this \"function\" would be to get a \"representative\" \\(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) and \"coefficients\" \\(\\{ -7 , 2.5 , 0.5 ,1 \\}\\) . Now we can get the back our dataset by multiplying them with the coefficients. Using this way we only need to store 6 real numbers (4 for coefficients + 2 for representative). Similarly, on a dataset of \\(2n\\) points , we can store them as \\(n +2\\) real numbers ( \\(n\\) coefficients + 2 for representative).","title":"Representation Learning (1)"},{"location":"img/Representation%20Learning/#working-with-a-realistic-dataset","text":"In the above example we were able construct a function which reduced the number of real numbers required to represent the dataset , but in a realistic dataset all the points do not lie on the same line , they are scattered and its very hard to derive meaningful conclusions from them. \"What to do if all the points dont lie in the dataset?\" A simple answer would be, \"We can get more representatives to accommodate the points which do not lie on the same line.\" This approach does accommodate the outliers but it also increases the number of real numbers required to represent the dataset. \"Whats the solution then?\" There has to be a tradeoff between the \"accuracy\" or the \"size\" of the dataset. If we want to reduce the size of the \"size\" then approximating the outlier to the \"line\" with least lost would be the best bet. To represent outliers and reduce the size of the dataset at the same time , we must project the outlier onto the line. Let there be a vector \\(\\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}\\) which represents the \"line\". The projection of the vector \\(\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\) onto the line would be \\[ \\left[ \\frac{x_1w_1 + x_2w_2}{w_1^2 + w_2^2} \\right] \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\] This will give us a point on the \"line\" with the least loss/distance. Now if we pick vector \\((w_1 , w_2)\\) which lies on the \"line\" such that \\(w_1^2 + w_2^2 = 1\\) , the above equation becomes \\[ \\left[ {x_1w_1 + x_2w_2} \\right] \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\]","title":"Working with a Realistic Dataset"},{"location":"img/Representation%20Learning/#representation-learning-2","text":"Our original objective was to find a \"compressed\" representation of the data when all the data-points not necessarily fall on the same line. \"How do we know that the line we picked is the best line?\" One could argue that the same line doesnt fit all the data-points with the \" least loss \" , where \" loss \" is the average of all the \" errors \" when projecting outliers onto the given \" line \". How to find the Best Line? To find the Best Line we should first have a certain way to compare 2 different lines. In our case it would be the line with the least \" reconstruction error \". Lets say for a dataset \\(\\{ x_1 , x_2 , .... x_n \\}\\) , where \\(x_i \\in \\mathbb{R}^d\\) \\[\\begin{equation*} \\begin{split} \\text{Error}(\\text{line} , \\text{dataset}) & = \\sum_{i=1}^n \\text{error}(\\text{line} , x_i) \\\\ & = \\sum_{i=1}^n \\text{length}^2(x - (x^Tw)w) \\\\ & = \\sum_{i=1}^n || x - (x^Tw).w||^2 \\\\ \\end{split} \\end{equation*}\\] To minimize the above equation we can think of it as a function \\(f(W)\\) \\[\\begin{equation*} \\begin{split} f(W) &= \\frac{1}{n}\\sum_{i=1}^{n} || x - (x^Tw).w ||^2 \\\\ &= \\frac{1}{n}\\sum_{i=1}^{n} (x - (x^Tw).w)^T(x - (x^Tw).w) \\\\ &= \\frac{1}{n}\\sum_{i=1}^{n} \\left[ x_i^Tx_i - (x_i^Tw)^2 - (x_i^Tw)^2 + (x_i^Tw)^2(1) \\right] \\\\ &= \\frac{1}{n}\\sum_{i=1}^{n} \\left( x_i^Tx_i - (x_i^Tw)^2 \\right) \\end{split} \\end{equation*}\\] Here we are minimzing \\(f(W)\\) with respect to \\(w\\) , the first term of the above equation \\(x_i^Tx_i\\) can be ignored as its a constant. So we can write the new function as \\(g(W)\\) \\[\\begin{equation*} \\begin{split} \\min_{W_{||w||^2 = 1}} g(W) &= \\frac{1}{n}\\sum_{i=1}^n - (x_i^Tw)^2 \\end{split} \\end{equation*}\\] Alternatively this same function can be written as \\[\\begin{equation*} \\begin{split} \\max_{W_{||w||^2 = 1}} g(W) = \\frac{1}{n}\\sum_{i=1}^n (x_i^Tw)^2 &= \\frac{1}{n} \\sum_{i=1}^{n} (w^Tx_i)(x_i^Tw) \\\\ &= \\frac{1}{n} \\sum_{i=1}^{n} w^T(x_ix_i^T)w \\\\ &= \\frac{1}{n} w^T(\\sum_{i=1}^{n} x_ix_i^T)w \\\\ \\end{split} \\end{equation*}\\] The above equation can also be written as \\[ \\max_{w_{||w||^2 = 1}} w^TCw \\] where \\(C = \\frac{1}{n}\\sum_{i=1}^{n}x_ix_i^T\\) Note that \\(C\\) is also the covariance matrix and the solution for the above maximization equation would be the eigenvector corresponding to the largest eignevalue of \\(C\\) .","title":"Representation Learning (2)"},{"location":"img/Representation%20Learning/#error-vector-has-information","text":"Our hypothesis was that there is a line which best represents the data but , if all the data points lied along a plane then this part (dotted orange lines) which we are imagining to be error may not be the error but rather useful information because the necessary information , the structure is in a plane but not on a line , so the bits we lose while selecting the best line will also contain some information. How do we extract this information? One possible algorithm could be , Input : \\(\\{ x_1 , x_2 , x_3 ...... x_n \\} x_i \\in \\mathbb{R}^d\\) Find the \" best \" line \\(w_1 \\in \\mathbb{R}^d\\) Replace \\(x_i\\) with \\(x_i - (x_i^Tw)w\\) Repeat to obtain \\(w_2\\) Note Sometimes the data might not be centered around the origin. To counter that we can subtract the average of the dataset from the points.","title":"Error Vector has Information"},{"location":"img/Representation%20Learning/#principal-component-analysis-1","text":"From the algorithm above we can find a \\(w_2\\) vector which represents the line which passes through the \"error/residues\" generated while finding out \\(w_1\\) . Observations made while finding out \\(\\mathbf{w_2}\\) All \"residues/errors\" are orthogonal to \\(w_1\\) . Any line which minimizes sum of errors w.r.t. residues must also be orthogonal to \\(w_1\\) A question which comes to mind is , \"is there a relationship between \\(w_1\\) and \\(w_2\\) ?\" The answer is \"Yes , there is a relationship.\" \\(w_1\\) and \\(w_2\\) are orthognal to each other. \\[\\implies w_1^Tw_2 = 0\\] By continuing this procedure for dataset of \\(d\\) dimension, we get a set of vectors \\(\\{ w_1 , w_2 , w_3 .... w_d \\}\\) with the following properties. \\(||w_k||^2 = 1 \\; \\; \\; \\forall k\\) \\(w_iw_j = 0 \\;\\;\\;\\;\\; \\forall i \\neq j\\) Hence , we get a set of Orthonormal Vectors. What is the use of Set \\(D\\) of Orthonormal Vectors? Residue after round 1 is \\(\\left\\{ x_1 - (x_1^Tw_1)w_1 , ..... , x_n - (x_n^Tw_1)w_1 \\right\\} \\;\\;\\;\\; \\forall \\;\\; \\text{vectors} \\in \\mathbb{R}^d\\) Residue after round 2 is \\(\\{ x_1 - (x_1^Tw_1)w_1 - (x_1 - (x_1^Tw_1)w_1)^Tw_2 , ..... \\}\\) \\(\\{ x_1 - (x_1^Tw_1)w_1 - (x_1^Tw_2 - (x_1^Tw_1).w_1^Tw_2)w_2 , ...... \\}\\) \\(\\{ x_1 - (x_1^Tw_1)w_1 - (x_1^Tw_2)w_2 , ...... \\}\\) as \\([w_1^Tw_2 = 0]\\) Residue after \\(d\\) rounds is \\(\\forall i \\;\\;\\;\\;\\;\\; x_i - ((x_i^Tw_1)w_1 + (x_i^Tw_2)w_2 + ...... (x_i^Tw_d)w_d) = 0\\) After \\(d\\) rounds all the error vectors become zero vectors. \\(\\forall i \\;\\;\\;\\;\\;\\; x_i = ((x_i^Tw_1)w_1 + (x_i^Tw_2)w_2 + ...... (x_i^Tw_d)w_d)\\) This leads to the conclusion that if data lives in a \"low\" dimensional linear sub-space, then residue becomes 0 much earlier than \\(d\\) rounds. What does the above statement actually mean? Lets say for a dataset \\(\\{x_1 , x_2 \\cdots , x_n \\}\\) , where \\(x_i \\in \\mathbb{R}^d\\) , all the residues/error vectors become 0 after 3 rounds. This means that every datapoint can be expressed as the sum of projections itself onto the residues/error vectors. \\[ \\forall i \\;\\; x_i = (x_i^Tw_1)w_1 + (x_i^Tw_2)w_2 + (x_i^Tw_3)w_3 \\] where \\(\\{w_1 ,w_2 ,w_3 \\} \\in \\mathbb{R}^d\\) Note that the \\(\\{w_1 ,w_2 ,w_3 \\}\\) are the representatives and \\(\\{x_i^T w_1 , x_i^T w_2 , x_i^T w_3 \\}\\) are the coefficients for a datapoint \\(x_i\\) Example Lets assume for a dataset of dimension \\(d\\) and \\(n\\) points , after \\(k\\) rounds the error vectors become zero vectors. This means that now the dataset can be represented with \\(\\mathbf{d \\times k + k \\times n}\\) points instead of \\(d \\times n\\) points. In the case shown above where the residues/error vectors become 0 after \\(k = 3\\) rounds, we can represent the whole dataset as, \\(d \\times 3\\) + \\(3 \\times n\\) Where , \\(d \\times 3\\) = Total numbers required to store the representatives \\(3 \\times n\\) = Total numbers required to store datapoints","title":"Principal Component Analysis (1)"},{"location":"img/Representation%20Learning/#principal-component-analysis-2","text":"Our original problem was \\[ \\max_{w_{||w||^2 = 1}} w^TCw \\] where \\(C = \\frac{1}{n}\\sum_{i=1}^{n}x_ix_i^T\\) , and the solution for this problem was the eigenvector corresponding to the maximum eigenvalue . We also observed that the set of eigenvectors \\((\\{w_1 , w_2 , .... w_d\\})\\) coresspnding to the eigenvalues of \\(C\\) form an orthonormal basis. Now lets look at this problem in a more \"linear algebraic\" way, We know, \\[\\begin{equation*} \\begin{split} Cw_1 &= \\lambda_1 w_1 \\\\ w_1^TCw_1 &= w_1^T(\\lambda_1w_1) = \\lambda_1 \\\\ \\lambda_1 &= w_1^TCw_1 = w_1^T(\\frac{1}{n}\\sum_{i=1}^{n}x_ix_i^T)w_1 \\\\ \\lambda_1 &= \\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw_1)^2 \\end{split} \\end{equation*}\\] Usually we take highest \\(L\\) lambdas such that 95% of the variance in the dataset is captured, \\[\\frac{\\sum_{i=1}^L \\lambda_i }{ \\sum_{i=1}^d \\lambda_i } \\geq 0.95\\] where , \\(\\lambda_i\\) are the eigenvalues of the covariance matrix .","title":"Principal Component Analysis (2)"},{"location":"img/Representation%20Learning/#relation-between-variance-and-mathbflambda","text":"For an arbitrary set of points \\((\\{(x_1^Tw) , (x_2^Tw) ...... (x_n^T)w \\})\\) projected onto line represented by vector \\(w\\) . The average \\(\\mu\\) of the projected points will be \\(\\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw)\\) . If the data is centered then, \\[\\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw) = (\\frac{1}{n}\\sum_{i=1}^{n}x_i)w = 0w = 0\\] This implies that average for a centered dataset is \\(\\mu = 0\\) . The variance of this same dataset will be , \\[\\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw - \\mu)^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i^Tw)^2\\] We can see that the variance is same as \\(\\lambda\\) required to solve the maximization problem. Hence we can say that , variance maximization is the same as error minimization on centered dataset","title":"Relation between Variance and \\(\\mathbf{\\lambda}\\)"}]}