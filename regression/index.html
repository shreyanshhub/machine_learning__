<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Regression - Machine Learning Theory</title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css">
        <link href="../stylesheets/extra.css" rel="stylesheet">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="..">Machine Learning Theory</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href=".." class="nav-link">Machine Learning Theory</a>
                            </li>
                            <li class="navitem">
                                <a href="../SVM/" class="nav-link">Support Vector Machine</a>
                            </li>
                            <li class="navitem">
                                <a href="../SVM2/" class="nav-link">SVM2</a>
                            </li>
                            <li class="navitem">
                                <a href="../Types%20of%20Models/" class="nav-link">Types of Models</a>
                            </li>
                            <li class="navitem">
                                <a href="../classification/" class="nav-link">Classification</a>
                            </li>
                            <li class="navitem">
                                <a href="../clustering/" class="nav-link">Clustering</a>
                            </li>
                            <li class="navitem">
                                <a href="../estimation/" class="nav-link">Estimation</a>
                            </li>
                            <li class="navitem">
                                <a href="../kernel_pca/" class="nav-link">Kernel PCA</a>
                            </li>
                            <li class="navitem">
                                <a href="../pca/" class="nav-link">Principal Component Analysis (PCA)</a>
                            </li>
                            <li class="navitem">
                                <a href="../perceptron_learning/" class="nav-link">Logistic Regression</a>
                            </li>
                            <li class="navitem active">
                                <a href="./" class="nav-link">Regression</a>
                            </li>
                            <li class="navitem">
                                <a href="../supervised_learning/" class="nav-link">Supervised Learning</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Img <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../img/Representation%20Learning/" class="dropdown-item">Principal Component Analysis (PCA)</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../perceptron_learning/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../supervised_learning/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#regression" class="nav-link">Regression</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#goodness-of-maximum-likelihood-estimator-for-linear-regression" class="nav-link">Goodness of Maximum Likelihood Estimator for linear Regression</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#cross-validation-for-minimizing-mse" class="nav-link">Cross-validation for minimizing MSE</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#bayesian-modeling-for-linear-regression" class="nav-link">Bayesian Modeling for Linear Regression</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#ridge-regression" class="nav-link">Ridge Regression</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#relation-between-solution-of-linear-regression-and-ridge-regression" class="nav-link">Relation Between Solution of Linear Regression and Ridge Regression</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#relation-betwen-solution-of-linear-regression-and-lasso-regression" class="nav-link">Relation betwen solution of Linear Regression and Lasso Regression</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#characteristics-of-lasso-regression" class="nav-link">Characteristics of Lasso regression</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="regression">Regression</h1>
<h2 id="goodness-of-maximum-likelihood-estimator-for-linear-regression">Goodness of Maximum Likelihood Estimator for linear Regression</h2>
<p>In the previous week we observed that <span class="arithmatex">\(w^*\)</span> which comes from the 
squared error equation is the same as <span class="arithmatex">\(\hat{w}_{\text{ML}}\)</span> which 
came from the maximal Likelihood equation.</p>
<p>Now we are going to look at <span class="arithmatex">\(\hat{w}_{\text{ML}}\)</span> and its properties ,
which will gives us a better idea of <span class="arithmatex">\(w\)</span>.
For <span class="arithmatex">\(\hat{w}_{\text{ML}}\)</span> our assumption was that,</p>
<div class="arithmatex">\[y|x = w^T x + \epsilon \]</div>
<p>where <span class="arithmatex">\(\epsilon\)</span> (gaussian noise) belongs to a gaussian distribution with mean 0 and variance <span class="arithmatex">\(\sigma^2\)</span> 
(<span class="arithmatex">\(\epsilon \sim \mathcal{N}(0 , \sigma^2)\)</span>).
For every <span class="arithmatex">\(x\)</span> in our data <span class="arithmatex">\(y\)</span> was generated using some <span class="arithmatex">\(w^T x\)</span> , using some unknown 
but fixed <span class="arithmatex">\(w\)</span> and then adding 0 mean gasussian noise to it.</p>
<p>Hence , <span class="arithmatex">\(y\)</span> given <span class="arithmatex">\(x\)</span> can be thought of as gaussian distribution with mean <span class="arithmatex">\(w^T x\)</span> and 
variance <span class="arithmatex">\(\sigma^2\)</span> (<span class="arithmatex">\(y|x \sim \mathcal{N}(w^T x , \sigma^2)\)</span>)</p>
<blockquote>
<p>For <span class="arithmatex">\(\hat{w}_{\text{ML}}\)</span> we should now look for a way to test that ,
how good the function is as a guess from true <span class="arithmatex">\(w\)</span>.</p>
</blockquote>
<p>A good function to compare <span class="arithmatex">\(\hat{w}_{\text{ML}}\)</span>  to <span class="arithmatex">\(w\)</span> is</p>
<div class="arithmatex">\[{||\hat{w}_{\text{ML}} - w ||}^2\]</div>
<p>and to see what happens to this value on an average we will 
look at its expected value over the randomness in <span class="arithmatex">\(y\)</span>, which can be written as </p>
<div class="arithmatex">\[E[{||\hat{w}_{\text{ML}} - w ||}^2]\]</div>
<p>If we solve for the expected value further , we get</p>
<div class="arithmatex">\[E[{||\hat{w}_{\text{ML}} - w ||}^2] = \sigma^2 \text{trace}((XX^T)^ \dagger)\]</div>
<p>where <span class="arithmatex">\(\sigma^2\)</span> is the variance from the gaussian noise</p>
<blockquote>
<p><span class="arithmatex">\(\sigma^2\)</span> in the expected value of the above function cant be reduced , 
it is the variance / loss which will always happen.
Only thing we can reduce is the trace of the inverse of covariance matrix</p>
</blockquote>
<h2 id="cross-validation-for-minimizing-mse">Cross-validation for minimizing MSE</h2>
<blockquote>
<p>We know that trace of a matrix is the sum of the diagonal entries of matrix ,
but in previous courses we have also seen that trace of a matrix is also equal to 
the sum of the eigenvalues of that matrix.</p>
</blockquote>
<div class="arithmatex">\[\text{tr}(A) = \sum_{i=1}^{d} \lambda_i\]</div>
<p>where <span class="arithmatex">\(A\)</span> is any matrix , <span class="arithmatex">\(d\)</span> is the dimension of the matrix and 
<span class="arithmatex">\(\lambda_i\)</span> is the <span class="arithmatex">\(i^{\text{th}}\)</span> eigenvalue.</p>
<ul>
<li>Let the eigenvalues of <span class="arithmatex">\((XX^T)\)</span> be <span class="arithmatex">\(\{\lambda_1 , \lambda_2 , .... \lambda_d \}\)</span>.</li>
<li>The eigenvalues of <span class="arithmatex">\((XX^T)^{-1}\)</span> are <span class="arithmatex">\(\{ \frac{1}{\lambda_1} , \frac{1}{\lambda_2} , .... \frac{1}{\lambda_d} \}\)</span></li>
</ul>
<p>The mean squared error equation <span class="arithmatex">\((\hat{w}_{\text{ML}})\)</span>,</p>
<div class="arithmatex">\[E(|| \hat{w}_{\text{ML}} - w ||^2) = \sigma^2 \left( \sum_{i=1}^d \frac{1}{\lambda_i} \right)\]</div>
<p><strong>Consider the following estimator:</strong></p>
<div class="arithmatex">\[\hat{w}_{\text{new}} = (XX^T + \lambda I )^{-1} XY \]</div>
<ul>
<li>For some matrix <span class="arithmatex">\(A\)</span> , let the eigenvalues be <span class="arithmatex">\(\{ \lambda_1 , \lambda_2 , ..... \lambda_d \}\)</span></li>
<li>Now what will be the eigenvalues of <span class="arithmatex">\(A + \lambda I\)</span>?</li>
</ul>
<div class="arithmatex">\[\begin{equation*}
\begin{split}
A v_1 &amp;= \lambda_1 v_1 \\ 
(A + \lambda I)v_1 &amp;= A v_1 + \lambda v_1 \\
&amp;= \lambda_1 v_1 + \lambda v_1 \\ 
&amp;= (\lambda_1 + \lambda) v_1 \\
\end{split}
\end{equation*}\]</div>
<p><span class="arithmatex">\(\implies\)</span> eigenvalues will be <span class="arithmatex">\(\{ \lambda_1 + \lambda , \lambda_2 + \lambda , ... \lambda_d + \lambda \}\)</span></p>
<ul>
<li>Similarly eigenvalues of <span class="arithmatex">\({(XX^T + \lambda I)}^{-1}\)</span> will be <span class="arithmatex">\(\{ \frac{1}{\lambda_1 + \lambda} , \frac{1}{\lambda_2 + \lambda} , ... \frac{1}{\lambda_d + \lambda} \}\)</span></li>
</ul>
<div class="arithmatex">\[\therefore \text{trace}((XX^T + \lambda I)^{-1}) = \left( \sum_{i=1}^d \frac{1}{\lambda_i + \lambda} \right)\]</div>
<blockquote>
<p>If the MSE is really large one of the reasons for 
it might because the trace of the matrix was really large , 
which means the eigenvalues were really small 
(smaller eigenvalues give large trace as they are inversely proportional).
To counter this we artifically introduced <span class="arithmatex">\(\lambda\)</span> 
which increases the overall eigenvalues and hence 
reducing the trace of the matrix , which in turn decreases MSE.</p>
</blockquote>
<h3 id="types-of-cross-validation">Types of Cross Validation</h3>
<p>Three commonly used techniques for cross-validation are as follows:</p>
<ul>
<li>Training-Validation Split: The training set is randomly divided into
a training set and a validation set, typically in an 80:20 ratio. Among
various <span class="arithmatex">\(\lambda\)</span> values, the one that yields the lowest error is selected.</li>
<li>K-Fold Cross Validation: The training set is partitioned into K equallysized parts. The model is trained K times, each time using K-1 parts as
the training set and the remaining part as the validation set. The <span class="arithmatex">\(\lambda\)</span> value
that leads to the lowest average error is chosen.</li>
<li>Leave-One-Out Cross Validation: The model is trained using all but
one sample in the training set, and the left-out sample is used for validation. This process is repeated for each sample in the dataset. The optimal
<span class="arithmatex">\(\lambda\)</span> is determined based on the average error across all iterations.</li>
</ul>
<h2 id="bayesian-modeling-for-linear-regression">Bayesian Modeling for Linear Regression</h2>
<details class="question">
<summary>What are conjugate priors?</summary>
<p>The key property of a conjugate prior is that, 
when combined with a likelihood function, 
the resulting posterior distribution belongs to the same 
family of distributions as the prior. 
This property simplifies the computation of the posterior distribution.</p>
</details>
<p><strong>Note:</strong> The conjugate prior for a gaussian distribution is gaussian distribution itself.</p>
<p>We know that our original likelihood function was, </p>
<div class="arithmatex">\[ y|x  \sim \mathcal{N}(w^Tx , 1) \]</div>
<p>where we are taking <span class="arithmatex">\(\sigma^2 = 1\)</span> for convenience , the following derivation will 
still be valid when variance is just <span class="arithmatex">\(\sigma^2\)</span>.</p>
<p>Now our prior will be,</p>
<div class="arithmatex">\[ w \sim \mathcal{N}(0 , \gamma^2 I) \]</div>
<p>where <span class="arithmatex">\(\mu \in \mathbb{R}^d\)</span> and <span class="arithmatex">\(\gamma^2 I \in \mathbb{R}^{d \times d}\)</span></p>
<p>Now,</p>
<div class="arithmatex">\[\begin{equation*}
\begin{split}
P(w| \{ (x_1 , y_1) , (x_2 , y_2) , .... (x_n , y_n) \}) &amp;\propto P(\{(x_1 , y_1) , (x_2 , y_2) , .... (x_n , y_n) \}|w) \times P(w) \\ 
&amp;\propto \left(\prod_{i=1}^{n} e^{-\frac{(y_i - w^Tx)^2}{2}} \right) \times \left(\prod_{i=1}^{n} e^{-\frac{(w_i - 0)^2}{2 \gamma^2}} \right) \\
&amp;\propto \left(\prod_{i=1}^{n} e^{-\frac{(y_i - w^Tx)^2}{2}} \right) \times e^{- \frac{||w||^2}{2 \gamma^2}} \\
\end{split}
\end{equation*}\]</div>
<p>Now how will maximum aposterior (MAP) estimate look like? (Here we are taking log of the above function)</p>
<div class="arithmatex">\[\begin{equation*}
\begin{split}
\hat{W}_{\text{MAP}} &amp;= \overset{\arg}{\underset{w}{\max}} \sum_{i=1}^{n}- \frac{(y_i - w^Tx_i)^2}{2} - \frac{||w||^2}{2 \gamma^2} \\ 
\hat{W}_{\text{MAP}} &amp;= \overset{\arg}{\underset{w}{\min}} \sum_{i=1}^{n} \frac{(y_i - w^Tx_i)^2}{2} + \frac{||w||^2}{2 \gamma^2} \\ 
\end{split}
\end{equation*}\]</div>
<p>Taking gradient of this function,</p>
<div class="arithmatex">\[\begin{equation*}
\begin{split}
\nabla f(w) &amp;= (XX^T)w - Xy + \frac{w}{\gamma^2} \\ 
\hat{W}_{\text{MAP}} &amp;= (XX^T + \frac{1}{\gamma^2}I )^{-1}Xy
\end{split}
\end{equation*}\]</div>
<p>We can see that the same estimator can be derived when 
find out MAP for a gaussian distribution prior.</p>
<h2 id="ridge-regression">Ridge Regression</h2>
<blockquote>
<p>Previously our linear regression equation was, </p>
<div class="arithmatex">\[ \hat{w}_{\text{ML}} = \underset{w}{\arg \min} \sum_{i=1}^{n} (w^T x_i - y_i)^2 \]</div>
</blockquote>
<p>The Ridge Regression equation is given as,</p>
<div class="arithmatex">\[ \hat{w}_{\text{R}} = \underset{w}{\arg \min} \sum_{i=1}^{n} (w^T x_i - y_i)^2 + \lambda ||w||^2 \]</div>
<p>where <span class="arithmatex">\(\sum_{i=1}^{n} (w^T x_i - y_i)^2\)</span> is the loss and <span class="arithmatex">\(\lambda ||w||^2\)</span> is the regularizer.</p>
<ul>
<li>The regularizer has bayesian viewpoint to it as shown in the above derivation,
also it can be thought of as adding a penalty on the overall function for 
prefering a certain type of <span class="arithmatex">\(w\)</span>.</li>
<li>The higher the weights of the <span class="arithmatex">\(w\)</span> the larger the penalty , it can also 
be thought of as prefering <span class="arithmatex">\(w\)</span> which have their features reduced to zero or almost zero.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul>
<li>Ridge Regression has more error than linear regression.</li>
<li>Ridge Regresison increases the training error so that the model does not overfit.</li>
</ul>
</div>
<h2 id="relation-between-solution-of-linear-regression-and-ridge-regression">Relation Between Solution of Linear Regression and Ridge Regression</h2>
<p>Lets say for a dataset we solved the linear regression problem and got <span class="arithmatex">\(\hat{w}_{\text{ML}}\)</span> on 
a 2-d subspace</p>
<p><img alt="" src="../img/w_ml.png" /></p>
<blockquote>
<p>Now is there any way to find the ridge regression solution for the same dataset?</p>
</blockquote>
<p>We know that equation of ridge regression is, </p>
<div class="arithmatex">\[ \hat{w}_{\text{R}} = \underset{w}{\arg \min} \sum_{i=1}^{n} (w^T x_i - y_i)^2 + \lambda ||w||^2 \]</div>
<p>It can be argued that this problem/equation is the same as 
constrained optimization problem,</p>
<div class="arithmatex">\[\begin{equation*}
\begin{split}
\underset{w \in \mathbb{R}^d}{\min} &amp; \sum_{i=1}^n (w^T x_i - y_i)^2 \\
\\
\text{such that}\\
&amp; ||w||^2 \leq \theta \\
\end{split}
\end{equation*}\]</div>
<p>For every choice of <span class="arithmatex">\(\lambda &gt; 0\)</span> , there exists a <span class="arithmatex">\(\theta\)</span> such that the
optimal solutions of Ridge Regression and Constrained Ridge Regression coincide.</p>
<p>In a 2-d space , when <span class="arithmatex">\(||w||^2 \leq \theta\)</span> , it can be written as,</p>
<div class="arithmatex">\[\begin{equation*}
\begin{split}
||w||^2 \leq \theta \\
\implies w_1^2 + w_2^2 \leq \theta \\
\end{split}
\end{equation*}\]</div>
<p>This is very similar to the equation of a circle whose radius (here)
is <span class="arithmatex">\(\theta\)</span> and it is centered at origin.</p>
<p><img alt="" src="../img/constrained_ridge.png" /></p>
<blockquote>
<p>From the above image we can see that we have found out a constrained 
area for <span class="arithmatex">\(\hat{w}_{\text{Ridge}}\)</span> , but where it is exactly?</p>
</blockquote>
<p>What is the loss/error value of linear regression at 
<span class="arithmatex">\(\hat{w}_{\text{ML}}\)</span>?</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The loss calculated at <span class="arithmatex">\(\hat{w}_{\text{ML}}\)</span> is the least when 
compared to any other <span class="arithmatex">\(w\)</span>.</p>
</div>
<div class="arithmatex">\[\begin{equation*}
\begin{split}
\sum_{i=1}^n (\hat{w}_{\text{ML}} x_i - y_i)^2 &amp;= f(\hat{w}_{\text{ML}}) \\ 
\end{split}
\end{equation*}\]</div>
<p>Consider the set of <span class="arithmatex">\(w\)</span> such that </p>
<div class="arithmatex">\[\begin{equation*}
\begin{split}
f(w) &amp;= f( \hat{w}_{\text{ML}} ) + c \;\;\;\;\;\; c&gt;0 \\
S_c &amp;= \{ w : f(w) = f( \hat{w}_{\text{ML}}) + c \} \\
\end{split}
\end{equation*}\]</div>
<p>Every vector in <span class="arithmatex">\(S_c\)</span> satisfies,</p>
<div class="arithmatex">\[\begin{equation*}
\begin{split}
\underset{f(w)}{||X^Tw - y||^2} &amp;= \underset{f(\hat{w}_{\text{ML}})}{|| X^T \hat{w}_{\text{ML}} - y ||^2 } + c \\ 
\\
\text{on simplification,}\\
(w - \hat{w}_{\text{ML}})^T (XX^T) (w - \hat{w}_{\text{ML}}) &amp;= c' \\
\end{split}
\end{equation*}\]</div>
<blockquote>
<p>If we have <span class="arithmatex">\(\hat{w}_{\text{ML}}\)</span> , <span class="arithmatex">\(XX^T\)</span> and <span class="arithmatex">\(c'\)</span> we can get a set 
of all the <span class="arithmatex">\(w\)</span> which satisfy the above equation such that they 
are <span class="arithmatex">\(c'\)</span> distance away in terms of error when compared to <span class="arithmatex">\(\hat{w}_{\text{ML}}\)</span>.</p>
</blockquote>
<p>If <span class="arithmatex">\(XX^T = I\)</span> (<span class="arithmatex">\(I\)</span> = Identity Matrix),</p>
<div class="arithmatex">\[\begin{equation*}
\begin{split}
(w - \hat{w}_{\text{ML}})^T I (w - \hat{w}_{\text{ML}}) &amp;= c' \\
|| w - \hat{w}_{\text{ML}} ||^2 = c' \\
\end{split}
\end{equation*}\]</div>
<p>This again corresponds to the equation of a circle (with c' radius) in a 2-d space , 
but thats only the case when <span class="arithmatex">\(XX^T = I\)</span>. If <span class="arithmatex">\(XX^T \neq I\)</span> then instead of a circle , 
an ellipse is formed around <span class="arithmatex">\(\hat{w}_{\text{ML}}\)</span> . </p>
<p><img alt="" src="../img/ridge_error.png" /> </p>
<p>If we keep increasing the <span class="arithmatex">\(c'\)</span> while using the same values for <span class="arithmatex">\(\hat{w}_{\text{ML}}\)</span>
and <span class="arithmatex">\(XX^T\)</span> , ellipses with increasing size are formed around <span class="arithmatex">\(\hat{w}_{\text{ML}}\)</span> 
which eventually touch the circle formed by the constrained ridge regression equation.</p>
<p><img alt="" src="../img/ridge_solution.png" /></p>
<p>The point where it touches (yellow dot) is the ridge regression solution with the 
least loss possible when compared to any other <span class="arithmatex">\(\hat{w}_{\text{Ridge}}\)</span> in the 
green circle. In other words that point (yellow dot) is closest to <span class="arithmatex">\(\hat{w}_{\text{ML}}\)</span> and yet 
satisfy the constrained ridge regression equation.</p>
<div class="admonition abstract">
<p class="admonition-title">Conclusion</p>
<p>We can see that Ridge Regression pushes the values in the wieght vector (<span class="arithmatex">\(w\)</span>)
to 0 , but does not necessarily make them 0.</p>
</div>
<h2 id="relation-betwen-solution-of-linear-regression-and-lasso-regression">Relation betwen solution of Linear Regression and Lasso Regression</h2>
<blockquote>
<p>Our goal here is to change the regularizer in ridge regression 
equation in such a way that the elliptical contours around <span class="arithmatex">\(\hat{w}_{\text{ML}}\)</span> 
hit at a point where some of the features become zero.</p>
</blockquote>
<p>We know that ridge regression pushes feature values towards 0. But does not 
necessarily make it 0.</p>
<p>An alternate way to regularize would be to use <span class="arithmatex">\(||\cdot||_1\)</span> norm instead 
of <span class="arithmatex">\(||\cdot||^2_2\)</span> norm.</p>
<div class="arithmatex">\[ ||w||_1 = \sum_{i=1}^d | w_i | \]</div>
<p>For L1 Regularization , </p>
<div class="arithmatex">\[\begin{equation*}
\begin{split}
&amp; \underset{w \in \mathbb{R}^d}{\min} \sum_{i=1}^{n} (w^T x_i - y_i)^2 + \lambda ||w||_1\\
\\
&amp; \text{which is similar to} \\ 
\\ 
&amp; \underset{w \in \mathbb{R}^d}{\min} \sum_{i=1}^{n} (w^T x_i - y_i)^2 \\
&amp; \text{such that}\\
\\
||w||_1 \leq \theta \\
\end{split}
\end{equation*}\]</div>
<p>When compared to L2 Constraint on the regularizer , 
this is how L1 constraint would look like </p>
<p><img alt="" src="../img/l2_constraint.png" /></p>
<p>Now when we keep increasing the size of the elliptical contours 
around <span class="arithmatex">\(\hat{w}_{\text{ML}}\)</span> our hope is that it touches the point 
where some of the features of weight vector become zero.</p>
<p><img alt="" src="../img/lasso_regression.png" /></p>
<blockquote>
<p>When looking at this , one can argue that the elliptical contours 
will not always touch red area in such a way that one of the weight
vectors becomes 0; Which is true when looking at it in a 2d subspace ,
but in higher dimensions the typical case is when some of weight vectors 
become 0.</p>
</blockquote>
<p>This way of using L1 Regularizer is called LASSO (Least Absolute Shrinkage and
Selection Operator)</p>
<h2 id="characteristics-of-lasso-regression">Characteristics of Lasso regression</h2>
<blockquote>
<p>We know now that lasso regression makes certain weight vectors zero , 
so why not always use lasso?</p>
</blockquote>
<p>Advantages of using Ridge Regression vs Lasso Regression,</p>
<ul>
<li>Lasso Regression does not have a closed form solution.</li>
<li>Sub-gradient methods are usually used to solve for Lasso Regression.</li>
</ul>
<h3 id="what-are-sub-gradients">What are Sub-Gradients?</h3>
<p>For a piecewise linear function at the point <span class="arithmatex">\(x\)</span> (purple) point the function 
is not differentiable , sub-gradient provide a lower bound for this function at 
<span class="arithmatex">\(x\)</span> (purple point).</p>
<p>At the blue point the function is differentiable and hence only 1 sub-gradient exists
which is the gradient/slope itself.
<img alt="" src="../img/sub_gradient.png" /></p>
<blockquote>
<p>Now , what is the use of sub-gradients in lasso regression?
We know that the regularizer in lasso regression takes the absolute values of 
weight vectors, </p>
</blockquote>
<p>At the origin (green point) finding the differential is not 
possible , hence we bound the function using sub-gradients.</p>
<p><img alt="" src="../img/lass_sub_gradients.png" /></p>
<p>Any sub-gradient between <span class="arithmatex">\([-1,1]\)</span> lower bounds the function (<span class="arithmatex">\(|x|\)</span>).</p>
<div class="admonition note">
<p class="admonition-title">Definition of Sub-Gradients</p>
<p>A vector <span class="arithmatex">\(g \in \mathbb{R}^d\)</span> is a sub-gradient of <span class="arithmatex">\(f:\mathbb{R}^D \to \mathbb{R}\)</span> at 
a point <span class="arithmatex">\(x \in \mathbb{R}^d\)</span> , if </p>
<div class="arithmatex">\[\forall z \;\;\;\;\;\;\;\;\; f(z) \geq f(x) + g^T(z - x)\]</div>
<p><img alt="" src="../img/sub_grad_definition.png" /> </p>
</div>
<div class="admonition question">
<p class="admonition-title">Whats the use of Sub-Gradients?</p>
<p>If function <span class="arithmatex">\(f\)</span> to minimize is a convex function , then 
sub-gradient descent converges.</p>
</div></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../js/jquery-3.6.0.min.js"></script>
        <script src="../js/bootstrap.min.js"></script>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js"></script>
        <script src="../javascripts/mathjax.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
